%************************************************
\chapter{Computational models of artificial curiosity}\label{CH3}
%************************************************

As discussed in \cref{CH2}, the motivation for seeking non-instrumental information is a crucial developmental element that allows humans to acquire useful knowledge. Specifically, we seem to have evolved a certain "liking" for certain experiences for the inherent value of learning from them, not because they are rewarding in a more tangible way. Researchers have proposed that the cognitive mechanism that primes learners to seek potential learning experiences is there to prepare them for future challenges that are as yet unknown \parencite{oudeyer_computational_2018,gopnik_childhood_2020}.

Precisely how the curiosity-inducing mechanism operates in biological organisms is poorly understood, but the undeniable significance of curiosity for the eventual acquisition of useful knowledge continues to spur \ac{AI} researchers to develop "curious" learning algorithms. These efforts have generated a great diversity of mechanisms enabling curiosity-driven learning in artificial agents. This chapter identifies common structural dimensions of these mechanisms and relates variation along these dimensions to possible evolutionary functions. Before we discuss these ideas, let us briefly recount how machines learn.

Although defining learning can be controversial \parencite{barron_embracing_2015}, in this chapter, I will adopt a straightforward formulation from \ac{ML} \parencite{jordan_machine_2015}, where learning is defined as improvement on a task (or a set of tasks) with experience. An artificial agent is said to have improved on a task if -- according to some well-defined criterion -- it is able to perform the task better than it did prior to receiving learning experience. Thus, what drives improvement is a combination of the experience that comes in the form of data that the agent can represent and the learning algorithm (also called learning, or update rule) that specifies how the agent’s innards change by processing the incoming data.

For a long time, major \ac{ML} paradigms have been elaborating increasingly efficient and powerful learning algorithms that optimize pre-specified task criteria. For example, all traditional algorithms of \ac{SL} and \ac{UL} depend on a formally defined objective function which evaluates the agent’s responses to stimuli and thus drives structural changes that result in better responses in the future. While these algorithms can learn many different tasks (e.g. image classification, natural language processing, visual scene parsing etc.), they are typically trained on datasets and objective functions assigned by the engineer. On the other hand, active learning agents \parencite{thrun_lifelong_1994,cohn_active_1995} feature algorithms that actively sample the data they learn from. This is particularly the case in \acf{RL} \parencite{sutton2018reinforcement} where agents have control over the sampled data by virtue of causal interactions with their environments. Here, learning is driven by an evaluative objective criterion which comes in the form of a reward function. Like in \ac{SL} and \ac{UL}, what the agent ends up learning is determined by a predefined criterion, but additional complications arise because of the need to sample experiences that help the agent improve. In realistic settings, only a tiny fraction of all possible experiences are relevant, which can be further complicated by the sparsity or deceptiveness of rewards \parencite[see ][]{oudeyer_computational_2018}. Sampling relevant experiences while avoiding noise is an important problem which will receive much attention in this chapter.

In contrast to \ac{ML} agents that learn externally assigned tasks, biological agents, particularly humans, often have autonomy not only in how they choose experiences to learn tasks, but also in choosing what tasks (i.e., goals) to learn. Moreover, we often seek information when we become curious/interested without being told what to be curious/interested about. Seeking information out of curiosity, rather than to achieve a separable outcome like food or money, is characterized in psychology as intrinsically motivated \parencite{harlow_learning_1950,ryan_self-determination_2000}. Intrinsically motivated behavior -- including information-seeking -- also includes pursuing self-selected goals that do not have proximal benefits for biological fitness (e.g., learning tango or climbing Everest). In addition to information-seeking phenomena like curiosity and interest, intrinsic motivation is also linked to other hallmarks of human behavior, such as creativity \parencite{gross_cultivating_2020} and play \parencite{chu_play_2020}.

Similarly to humans but unlike the traditional \ac{ML} systems mentioned above, intrinsically motivated artificial agents control what they learn through autonomous and non-instrumental sampling of learning experiences. This sampling is achieved by considering various features of what we call \emph{learning situations}. To understand what we mean by "learning situations" better and to illustrate how they relate to various mechanisms and functions of intrinsic motivation, consider the following scenarios:

\begin{itemize}
    \item A robot trying random actions in its environment
    \item A rat exploring a maze to get familiar with its environment
    \item A toddler trying to build the tallest possible tower with toy blocks
    \item A curious student raising his hand to pose a question to her teacher
    \item A infant looking at where her mother is pointing
    \item A philosopher considering what book to read
\end{itemize}

All of these scenarios describe an agent interacting with its environment and thereby engaging in a learning situation\footnote{Of course, the agent may not learn upon every single interaction with the environment, but any interaction creates a situation where learning could happen.}. What differentiates these learning situations is the mechanism by which information happens to be sampled. We can identify two core components of computational mechanisms of curiosity-driven exploration. The first component is the interface through which agents actively sample learning situations, i.e. the space in which they can make choices. The second component is the principle by which agents rank learning situations within this space. We describe these components more thoroughly in \cref{CH3_S_mechanisms} (\textsc{Mechanisms}), but the examples above already illustrate that agents can sample learning situations completely at random (like the robot), by considering what actions to take (like the rat), what goals to pursue (like the toddler), or whom to ask (the student). Moreover, decisions between alternative actions, goals, or social peers, may be based on prior knowledge (in case of the philosopher), driven by competence (in case of the toddler), or influenced externally, e.g., by others (in case of the infant). Note different mechanisms illustrated by the examples imply distinct consequences, suggesting  that they may serve distinct functions. In \cref{CH3_S_functions} (\textsc{Functions}), we organize these functions along the axis of causal proximity to evolutionary fitness: from the rat exploring a maze to the philosopher pondering the most obscure questions. Finally, in \cref{CH3_S_usages} (\textsc{Usages}) we discuss the relationships between \ac{AI} and psychology and briefly survey some of the use cases for various intrinsically motivated algorithms outside the realm of \ac{AI} research.

\section{Mechanisms}\label{CH3_S_mechanisms}
As mentioned in the introduction, researchers in \ac{AI} have proposed a wealth of algorithms for intrinsically-motivated exploration. These algorithms differ by how they address two related subproblems:

\begin{enumerate}
    \item How to \textit{parameterize} learning situations?
    \item How to \textit{choose} learning situations?
\end{enumerate}

The first subproblem is addressed by defining the choice space to drive exploration \parencite{moulin-frier_exploration_2013}. Intuitively, a choice space serves as a basis for assessing different learning situations. Actualizing choices in a choice space results in agent-environment interactions from which the agent can learn. This chapter reviews three kinds of choice spaces that consist of either actions, goal states, or social partners. These choice spaces correspond to different ways in which learning situations can be parameterized. Parameterizing learning situations on the basis of \emph{actions} lets the agent consider what can be learned by performing these actions; parameterizing learning situations on the basis of \emph{goals} lets the agent consider what can be learned by pursuing goals; parameterizing learning situations on the basis of \emph{social partners} lets the agent consider what can be learned by interacting with others. We discuss the main ways in which choice spaces are specified and how they differ in \cref{CH3_SS_exploration_bases} (\textsc{Mechanisms}/\textit{Exploration Bases}).

Given a fully specified choice space, the agent can make decisions within that space. Specifying this decision-making process addresses the second subproblem. Agents choose what to learn by following a certain strategy by which they assign “interestingness” to the available choices, thereby determining which learning situations are more likely to be approached. For example, choices can be driven by features of learning situations, such as the amount of knowledge a situation might bring or how novel it is; they can also be made completely at random \cite[e.g., ][]{colas_gep-pg_2018,colas_language_2020}. We provide a brief survey of these methods in \autoref{CH3_SS_exploration_strategies} (\textsc{Mechanisms}/\textit{Exploration Strategies}).

\subsection{Exploration Bases}\label{CH3_SS_exploration_bases}
We stated before that a learning situation arises whenever an agent interacts with its environment. Active interactions entail that the agent has to decide how to act in a given context, and upon deciding and acting, gets to observe the effects of its actions. How the agent explores, therefore, depends primarily on how the agent contextualizes its interactions. Specifically, the agent can explore by considering what actions to take, what goals to pursue, or what social partners to engage. Sets of actions, goals, or social partners provide a basis for comparing potential interactions. The rest of this section reviews different ways in which such bases are defined, used, and represented.

\subsubsection{Exploring by Choosing Actions}\label{CH3_SSS_exploring_by_choosing_actions}
One family of approaches considers agents which observe the current state of the environment as a context, choose actions to execute, and observe the resulting following state. Here, the objective of exploration is to select the actions which generate informative data for learning an internal model of causal dynamics through SL, UL or RL. For example, several SL-based robotic agents \parencite{oudeyer_intrinsic_2007,caligiore_using_2008,baranes_r-iac_2009,saegusa_active_2009,lefort_active_2015} maintain world models representing their knowledge about either forward dynamics (inferring future states from specific actions), or inverse dynamics (inferring the right actions to bring about specific states), or both. These systems learn from observations borne out of the actions they choose. Therefore, exploration of learning situations in these approaches corresponds to making choices in the action space. Examples of action-space exploration can also be found in RL settings \parencite{singh_intrinsically_2010,bellemare_unifying_2016,jaderberg_reinforcement_2016,pathak_curiosity-driven_2017,tang_exploration_2017,burda_large-scale_2018,haber_learning_2018,bougie_fast_2020}. In intrinsically motivated RL, agents learn behavioral policies by maximizing intrinsic rewards (e.g., rewards based on state novelty as in \parencite{tang_exploration_2017}; on model prediction error as in \parencite{pathak_curiosity-driven_2017}; or on surprise as in \parencite{berseth_smirl_2021}). In these systems, actions that bring about rewarding states get reinforced. Thus, the agent collects learning data (state transitions) by ranking actions according to their capacity to yield intrinsic rewards. 

\subsubsection{Exploring by Choosing Goals}\label{CH3_SSS_exploring_by_choosing_goals}
Another family of approaches considers agents making choices in a goal space instead of an action space. In the general case, such agents learn to represent and sample their own goals, i.e., they are autotelic \parencite{colas_intrinsically_2021}. This family of approaches is referred to as Intrinsically Motivated Goal-Exploration Processes (or IMGEP for short; \parencite{forestier_intrinsically_2020,colas_intrinsically_2021}). Here, the notion of a goal is generalized: it refers to any set of constraints on any set of future sensorimotor representations \parencite{colas_intrinsically_2021}. This abstract conceptualization enables researchers to express all kinds of goals, ranging from particular world states (e.g., coordinates of the agent’s hand must be equal to a specific x, y, and z), to constraints on entire behavioral trajectories and their linguistic descriptions (e.g., "water the plant and then feed the dog"; \parencite{colas_language_2020}). In all cases, goals are specified by two essential components: (1) goal representation that specifies the criteria and (2) goal-achievement function that signals whether the criteria are met. Goals are usually represented as numerical vectors (sometimes called goal embeddings) that comprise abstract goal spaces from which specific goals can be sampled, while goal achievement is evaluated using logical operations.

Examples of IMGEPs can be found in SL and UL contexts \parencite{jordan_forward_1992,rolf_goal_2010,baranes_active_2013,moulin-frier_exploration_2013,forestier_intrinsically_2020,takahashi_dynamic_2017,reinke_intrinsically_2020,laversanne-finot_curiosity_2018}. In these frameworks, agents autonomously engage in learning situations by attempting to reach self-selected goals. Note that this process is markedly different from the one described above, where the agent accesses learning situations by choosing among the available actions. In IMGEPs, the agent can consider any goal it may imagine, not just the states that its actions may bring about.

The domain of goal-conditioned RL works with agents that learn action policies conditioned on goals. These agents base their actions not only on the current state (as in traditional RL) but also the goal encoding, which means they can act differently in the same situation depending on what they are after \parencite{schaul_prioritized_2016}. Intrinsically motivated goal-conditioned RL builds upon that framework and allows agents to generate their own goals \parencite[see][for a recent review]{colas_intrinsically_2021}. Some notable examples include \parencite{colas_gep-pg_2018,nair_visual_2018,colas_curious_2019,pong_skew-fit_2020,colas_language_2020}. While there is a great deal of variability among strategies for sampling interesting goals (see \cref{CH3_SS_exploration_strategies}, \textsc{Mechanisms}/\textit{Exploration Strategies}), all of these goal-oriented agents make decisions in a goal space rather than in an action space. Goal-oriented intrinsically motivated learning has a number of advantages compared to action-oriented intrinsically-motivated learning: it improves the performance and convergence time when learning inverse models in high-dimensional spaces with highly non-linear mappings \parencite{baranes_active_2013}, it automatically generates learning curricula from easy to more complex skills \parencite{moulin-frier_self-organization_2014}; finally, it enables hindsight learning \parencite{forestier_intrinsically_2020,andrychowicz_hindsight_2018,colas_curious_2019}. We discuss these positive practical implications in more detail in \cref{CH3_S_functions}, \textsc{Functions}.

\subsubsection{Exploring by Choosing Social Partners}\label{CH3_SSS_exploring_by_choosing_social_partners}
A few contributions have explored how IM can be coupled with social interaction. The SGIM-ACTS architecture \parencite{nguyen_active_2012} considers an IM agent that is able to choose whether and when to learn from a social peer or by autonomous goal generation, from which social peer to learn, and what to ask the chosen social peer. Interacting with the social peer becomes part of the choice space where the agent makes decisions hierarchically: it first decides to interact or to self-explore its own goals, then which social peer or self-generated goal to focus on. The SGIM-ACTS framework was also applied to agents equipped with a realistic computer model of the human vocal tract and was able to reproduce the main developmental stages of infant vocal development \parencite{moulin-frier_self-organization_2014}.

More recent contributions consider social influence as intrinsic motivation for achieving coordination and communication in multi-agent RL \parencite{jaques_social_2019}. Other work in multi-agent RL have theorized and demonstrated how competition and cooperation display intrinsic dynamics, resulting in a naturally emergent curriculum \parencite{leibo_autocurricula_2019,baker_emergent_2020}.

In humans, not only does the choice of a social partner can be intrinsically motivated, but the social partner can also influence intrinsic motivation. As humans often model the behavior of social peers, curiosity of a social partner can "spread" onto the learner, making them more likely to seek for information \parencite{gordon_can_2015}. This interplay between the curiosity of the agent and the behavior of a curious social partner has not yet been modeled in artificial systems. However, it can be a part of the mechanism which enables agents to modulate their own exploratory behavior, not by sampling instructions or directions from their peers, but by inferring mimicking their motivations.

\subsubsection{Representing Choice Spaces}\label{CH3_SSS_representing_choice_spaces}
Precisely what do choices in choice spaces represent? In the case of action-space exploration, actions can represent micro-actions responsible for transitions between temporally adjacent momentary states, like pixel images \parencite[e.g.,][]{bellemare_unifying_2016,pathak_curiosity-driven_2017}. In this case, the agent can only learn from short-term transitions which limits their ability to efficiently learn regularities spanning larger time scales\footnote{Model-free RL agents \parencite[e.g.,][]{bellemare_unifying_2016,pathak_curiosity-driven_2017} do not learn from the transitions per se -- they learn from rewards. Specifically, the agent usually learns a value function V, mapping states to their expected cumulative reward. Even if the transitions are momentary, the agent can still maximize their long-term cumulative reward using techniques like bootstrapping \parencite{sutton2018reinforcement}. Another exception is when the world model is represented by a recurrent neural network (RNN; \cite{takahashi_dynamic_2017}) as RNNs can encode time series.}. However, the agent can sample learning situations by making decisions in the space of macro-actions (e.g., action-policies), rather than micro-actions, which enables exploration of more temporally extended effects of its actions \parencite[see][]{baranes_active_2013}. 

In the case of goal sampling, the choice space can correspond entirely to the state space, so that a given goal represents a particular state. This can be problematic for very high-dimensional spaces. For example, if the state space is a space of pixel images, because sampling directly from this space yields white noise most of the time, just like blindly picking alphabet letters would produce mostly gibberish \parencite{nair_visual_2018}. One solution to this problem is to define the goal space as some high-level feature space (also called latent or embedding space) of the raw-image space \parencite[e.g.,][]{laversanne-finot_intrinsically_2021}. Following the letter-picking analogy, this would correspond to composing and sampling from a higher-level space of syllables or words which is more likely to produce meaningful strings. A related issue, in cases when the goal space is defined over some abstraction over the raw sensory experience (e.g., 2D positions of objects vs raw pixel images), is whether to assume that this abstract space is given to the agent like in \citeauthor{forestier_intrinsically_2020} \parencite{forestier_intrinsically_2020}, or whether the agent needs to learn a latent goal space from scratch \parencite[e.g.,][]{nair_visual_2018,laversanne-finot_intrinsically_2021}.

\subsection{Exploration Strategies}\label{CH3_SS_exploration_strategies}
Given a well-defined choice space for exploration, what strategies can an artificial agent follow to decide which learning situations are more or less interesting? Comprehensive reviews of different approaches can be found elsewhere \parencite{oudeyer_what_2007,mirolli_functions_2013,aubret_survey_2019,linke_adapting_2020}, so we only provide a short survey of different approaches with a focus on their diversity rather than precise implementations. 

We group existing approaches into three main categories: undirected, knowledge-based, and competence-based exploration. While all learning situations are equally interesting in undirected exploration, directed exploration strategies scale interestingness with the agent’s abilities. Directed exploration can be divided into two broad classes: knowledge-based and competence-based strategies \parencite{oudeyer_what_2007}. Sometimes the distinction between the two can be subtle because knowledgeable systems can also be competent and competent systems can be knowledgeable \parencite{mirolli_functions_2013}. The point of divergence for these families of mechanisms is that to a knowledge-based system, the interestingness of a learning situation is determined by its relation to the system’s knowledge. On the other hand, to a competence-based system, a given learning situation may be more or less interesting because it relates to the system’s ability to reach a specific self-generated goal.

\subsubsection{Undirected Exploration}\label{subsub:3-undirected}
Undirected exploration (sometimes random or uniform exploration) refers to a strategy that assigns interest uniformly across the choice space (making all learning situations equally interesting). The effectiveness of this simple strategy is inconsistent across different settings. When applied to action-space exploration, for example, undirected exploration is only effective for simplistic problems \parencite{baranes_active_2013,benureau_behavioral_2016}, for example, when the environment provides dense rewards or when actions have simple and consistent effects. In more challenging settings, where the mapping between actions and their effects exhibits a combination of non-linearity, stochasticity, and redundancy, motor exploration is not sufficient for effective learning, but goal exploration could be \parencite{moulin-frier_exploration_2013}. This is largely because learning how to reach goals contributes to the agent’s competence and thus its ability to control the environment, while learning about various outcomes of all of one’s actions in all possible contexts may be worthless in practice \parencite{mirolli_functions_2013}. Besides, trying out random actions from a particular state may be futile for reaching certain hard-to-reach regions of the state space. Think of how hard it would be to learn how to drive from home to work by performing random actions (you would end up crashing your car most of the time). If instead you were to learn how to drive to various places from home (your driveway, a corner shop, a nearby gas station) your chances of finding your way to your office eventually would be much higher. 

Despite its simplicity, random goal exploration has proven to be surprisingly efficient, leading to some forms of novelty search as an emergent feature and surpassing directed approaches operating in the action space in the learning of redundant inverse mappings \parencite{benureau_behavioral_2016,colas_gep-pg_2018}. Because it is simple and computationally cheap, random goal exploration is often combined with other strategies, either to jumpstart the primary mode of directed exploration by collecting initial data, or sometimes as a complementary strategy at a certain level of hierarchical sampling decisions in modular spaces \parencite[e.g.,][]{forestier_intrinsically_2020}, and sometimes as an epsilon-greedy strategy \parencite[see][]{sutton2018reinforcement} to balance between random and directed exploration \parencite[e.g.,][]{colas_curious_2019}. Still, in many situations, undirected exploration may not be as efficient or effective as more sophisticated guided exploration approaches. For example, random (goal) exploration performs poorly when the space of effects has a hierarchical structure, so that certain states are only accessible through reaching some prerequisite states \parencite[e.g.,][]{forestier_intrinsically_2020}. An agent exploring goals randomly is unlikely to ever get to practice these “out-of-reach” goals, unless there is a mechanism for imagining them that leverages structured representations of goals, such as natural language encodings \parencite[e.g.,][]{colas_language_2020}.

\subsubsection{Knowledge-Based Exploration}\label{CH3_SSS_knowledge_based_exploration}
Knowledge-based exploration is perhaps the most diverse family of intrinsically motivated strategies. Not only are there many ways in which one can characterize a relation between learning situations and the agent’s knowledge, but there are many kinds of knowledge that agents represent. For example, an agent can maintain a predictive causal model of the effects of its actions and have a metacognitive monitoring system track errors that this predictive model commits. Equipped with such a system, the agent can measure interestingness of actions based on, for example, outcome prediction error of the forward model \parencite[e.g.,][]{saegusa_active_2009}. Specifically, the agent can be more (or less) interested in taking actions for which the forward model does accurately predict the consequences (known as prediction-error strategy). Alternatively, the agent can track changes in prediction accuracy (a strategy known as learning progress; e.g. \cite{schmidhuber_curious_1991,oudeyer_intrinsic_2007,kim_active_2020}). Here the agent would be more (or less) interested in actions, predictions for which get more accurate with time. While the measure of interestingness in these approaches is based on the predictions from a forward model, exploring agents can also monitor the behavior of an inverse dynamics model \parencite{pathak_curiosity-driven_2017,haber_learning_2018}.

A distinct subclass of knowledge-based strategies relies on knowledge about frequencies of observed states. In so-called count-based approaches \parencite{bellemare_unifying_2016,tang_exploration_2017}, this knowledge is represented explicitly, allowing the agent to selectively explore over- or under-visited states. Other systems in this subclass of approaches incorporate different variants of autoencoder networks to learn latent spaces \parencite{twomey_curiosity-based_2018,bougie_fast_2020} or generative models \parencite{nair_visual_2018,pong_skew-fit_2020} of states observed by the agent. Specifically, Twomey and Westerman defined several interestingness measures based on backpropagation computation of their category-learning neural network, including measures of weight update, prediction error, and activation-function derivative (which model, respectively, the system’s curiosity, novelty, and plasticity; \cite{twomey_curiosity-based_2018}). Bougie and Ichise introduced auxiliary tasks of image reconstruction with context-based autoencoders and defined an intrinsic reward measure derived from reconstruction errors from these tasks \parencite{bougie_fast_2020}. Others \parencite{nair_visual_2018,pong_skew-fit_2020} employed variational autoencoders to estimate valid-state distributions in order to guide exploration. Although these models do not explicitly encode state visitation counts, the interestingness measures defined on their basis are related to frequencies of the observed states. Autoencoder prediction error, for instance, should decrease with repeated exposure to a given state. Variational autoencoders additionally represent the statistics of the latent space -- a feature that can be used to estimate the likelihood of any state (including completely novel states) given what has been observed in the past. 

\subsubsection{Competence-Based Exploration}\label{CH3_SSS_competence_based_exploration}
Another family of strategies assigns interestingness based on competence. These approaches do not need to assume any explicit world-dynamics knowledge model (although they can), so they can be readily incorporated in model-free learning systems without the need to introduce any auxiliary tasks. Competence-based exploration strategies are especially well-suited for intrinsically motivated agents that explore self-imposed goals, since the notion of competence corresponds naturally to goal achievement. A simple competence-based heuristic in a goal-oriented context is to sample goals according to the agent’s ability to achieve them. For example, Bougie and Ichise reward the agent in a given state based on how incompetent the agent perceives itself to be in that state \parencite{bougie_skill-based_2019}. Here the agent generates its data set by taking interest in actions that lead it to states at which it deems itself incompetent, i.e. by choosing actions that maximize incompetence. In a different approach, Florensa and colleagues leveraged the generative power of adversarial networks for generating goals, which the agent evaluates based on the probability of reaching them \parencite{florensa_automatic_2018}. Thanks to this evaluation, their agent can prioritize goals of intermediate difficulty, thereby avoiding goals that it already knows how to achieve and goals that it knows it cannot achieve. Santucci et al. compared the efficacy of several interestingness measures for autonomous mastering a set of tasks that included unreachable distractor tasks. The best-performing measure that allowed their agent to learn all learnable tasks in the least amount of time was based on competence prediction-error \parencite{santucci_which_2013}. Several other teams have explored modular goal spaces using measures of competence progress \parencite{stout_competence_2010,forestier_intrinsically_2020,colas_curious_2019}. Agents in these studies were incentivized to sample goals from the predefined regions (modules) of the goal space where competence was either improving or deteriorating. Oudeyer and colleagues used a similar competence progress-based strategy but in settings where the singular goal-space was progressively modularized \parencite{baranes_active_2013, moulin-frier_self-organization_2014}.

\section{Functions}\label{CH3_S_functions}
The previous section reviewed the diversity of approaches to specifying intrinsically-motivated mechanisms in AI. We have mentioned in the introduction that these mechanisms are useful for autonomous learning in the absence of externally assigned objectives, but what specific functional consequences do different intrinsic motivation mechanisms bear? This section focuses on the functional aspects of intrinsically motivated systems addressing the question of how different intrinsic-motivational systems are useful for both artificial and biological agents.  

Singh et al. provide a useful connection between the concepts of extrinsic and intrinsic motivation and the concepts of primary and secondary rewards \parencite{singh_intrinsically_2010}. While the reception of primary rewards (e.g., related to nutrients, sex, or pain) contributes to an agent’s survival and, thus, its reproductive success, secondary reinforcers signal anticipation of primary rewards, but are neutral a priori and have to be learned. In RL, outputs of the reward function can be analogous to primary reward signals, because the reward function is given to the agent. However, value functions evaluate states based on their learned expected cumulative reward, and therefore outputs of the value function are analogous to secondary reinforcers. In this formulation, intrinsic motivators (e.g., novelty, uncertainty, learning progress etc.) are primary reinforcers because their rewardingness is a given. On the other hand, predictors of intrinsic primary reinforcers can acquire rewarding qualities through the learning of secondary reinforcers, in the same way as non-rewarding states gain value due to extrinsic reinforcers. Therefore, intrinsic primary rewards differ from extrinsic primary rewards, mostly due to their causal proximity to evolutionary success. As it is often the case in biology \parencite{dobzhansky_nothing_1973}, it is sensible to analyze the functional aspect of artificial intrinsic-motivational mechanisms in light of evolution -- an exercise that allows us to consider the possible bio-ecological roles of engineered curiosity-driven systems.

In what follows, we build upon Singh et al.’s framework as well as the taxonomy of mechanisms we proposed in the previous section in order to extract key functional aspects of intrinsic motivation in both biological and artificial agents. We start from the more evolutionarily proximal functions (direct procurement of primary rewards) and gradually consider increasingly distal ones (learning of internal models, goal discovery, and cultural innovation).

\subsection{Procurement of Extrinsic Primary Rewards}\label{CH3_SS_procurement_of_extrinsic_primary_rewards}
In relatively dense primary-reward environments, unstructured random exploration in the action space is sufficient to efficiently elicit extrinsic primary rewards (as it is the case in some standard AI benchmarks based on video games, e.g., \cite{mnih_human-level_2015}). This points to the most proximal function of intrinsic motivation: the generation of diverse sensorimotor experiences. The direct benefit of generating diverse experiences is to increase the probability of eliciting primary extrinsic rewards. However, the diversity generated by random exploration is usually not sufficient in sparser reward environments \parencite[e.g.,][]{pathak_curiosity-driven_2017}. Knowledge-based exploration can increase the diversity of learning experiences by guiding exploration based on different measures of interestingness (see section Exploration principles). Discovering extrinsic primary rewards such as food, water, or shelter through exploration is clearly linked to the agent’s well-being.

\subsection{Learning Internal Models}\label{CH3_SS_learning_internal_models}
A more distal function of intrinsic motivation is the learning of internal models of the agent-environment interaction (e.g. forward or inverse models) that can enhance the agent’s decision-making abilities. Learning such a model can be formalized as an UL or an SL problem and strongly relies on the information contained in the training dataset. Autonomous agents generate this dataset by interacting with the environment, and the key role of intrinsic motivation is to generate informative training data. Research in intrinsically motivated SL extensively studied two main cases. On the one hand, knowledge-based approaches have proven to be efficient in generating informative data for learning forward models \parencite{oudeyer_intrinsic_2007}. On the other hand, competence-based approaches have proven to be more efficient than knowledge-based approaches for learning inverse models \parencite{baranes_active_2013}. RL agents can also benefit from internal models, be it in the form of a value function and an action policy in model-free RL \parencite{pathak_curiosity-driven_2017}, or in the form of a world-dynamics model (forward or inverse) in model-based RL \parencite{haber_learning_2018}. Understanding how the world works per se does not put proverbial food in the agent’s mouth, but it allows the agent to act more intelligently in novel situations and plan ahead in order to obtain what it needs more reliably.

\subsection{Goal Discovery}\label{CH3_SS_goal_discovery}
The third level of functions we propose is related to the discovery and learning of novel goals and the associated skills to achieve them. This is the main function of competence-based approaches, where exploration is guided by the pursuit of self-imposed goals. These approaches can automatically organize exploration from simple to more complex skills \parencite{forestier_intrinsically_2020,pong_skew-fit_2020}, as well as discover the full range of the achievable behavioral repertoire, possibly in an open-ended manner \parencite{colas_curious_2019}. There are multiple functional advantages to discover and master novel goals that are not extrinsically rewarding. First, in environments where eliciting extrinsic primary rewards require the acquisition of complex skills (e.g., hunting), it is crucial to structure learning in a curriculum from simple (e.g., locomotion) to more complex skills. Complex skill sets often display a hierarchical structure, where mastering easier ones is prerequisite for acquiring more complex ones. Second, the ability to autonomously explore and discover new goals and skills provides a crucial advantage in changing environments. For example, paleo-climatological data provides evidence for strongly varying climate conditions in the Rift Valley at East Africa approximately 7 million years ago, and it is hypothesized that the ability to rapidly and flexibly reorganize a diverse behavioral repertoire was a key requirement to adapt to such unprecedented conditions \parencite{potts_hominin_2013}. Thus, the ability to autonomously generate and master novel goals and thereby acquire a diverse repertoire of complex skills provides a crucial advantage for a species' success in such settings of strong environmental variability (see \cite{nisioti_grounding_2020}, for a recent proposition to apply this principle in AI).

\subsection{Cultural Innovation}\label{CH3_SS_cultural_innovation}
Finally, the fourth and most distal level of functions we propose concerns cultural innovation. Several theoretical contributions proposed a potential role of curiosity-driven exploration in both language acquisition \parencite{oller_emergence_2000} and evolution \parencite{oudeyer_how_2016}. From a sensorimotor perspective, active exploration can spontaneously generate diverse behaviors from modality-independent and task-independent internal drives. Such spontaneous behavior can result in vocal activity that may have bootstrapped the emergence of communication. This hypothesis is supported by computational simulations showing a role of curiosity-driven exploration in vocal development \parencite{moulin-frier_self-organization_2014}, social affordance discovery \parencite{oudeyer_discovering_2006}, and active control of the emerging conventions in social lexicon \parencite{schueller2018complexity}. From a cognitive perspective, compositional language itself is a powerful cognitive tool to imagine novel out-of-distribution goals in competence-based intrinsic motivation \parencite{colas_language_2020}. Moreover, recent contributions in multiagent RL have shown how an auto-curriculum of increasingly complex behaviors displaying features of open-ended innovation can emerge from agents’ co-adaptation in mixed cooperative-competitive environments \parencite{baker_emergent_2020}. Such mechanisms are potential precursors of cultural evolution in the human species. Cultural evolution has triggered increasingly complex technological innovation across generations \parencite{fogarty_niche_2017}. A prime example of this is the industrial revolution, which has resulted in a rapid acceleration of global population growth in the 19th century \parencite{lucas_industrial_2004}.

\section{Usages}\label{CH3_S_usages}
Other than helping artificial agents explore learning situations in abstract task-independent contexts, how can intrinsically-motivated learning algorithms be used? We identify two main directions in which such algorithms can have high impact. On the one hand, they can be a great tool for advancing research in cognitive psychology and neuroscience. Outside cognitive research, these algorithms can be applied directly to problems that require intelligent automated exploration. We review both of these domains of application in the rest of this section.

\subsection{Cognitive Modeling}\label{CH3_SS_cognitive_modeling}
The notion of intrinsically-motivated exploration in psychology has been developing -- for the most part -- independently of AI \parencite{kaplan_search_2007}. Psychological investigations can be traced back to psycho-physiological perspectives on exploration in the early 1940s \parencite{hull_principles_1943}. Since then, psychological views on curiosity and information-seeking have undergone multiple changes \parencite{loewenstein_psychology_1994,bazhydai_curiosity_2020} and are now becoming more integrated with the computational perspective \parencite{kaplan_search_2007,gottlieb_information-seeking_2013}. On the other hand, early formulations of intrinsic motivation in AI were either “discovered by accident” \parencite[p. 5]{andreae_thinking_1977} or influenced by relatively distant research areas, like biological autopoiesis \parencite{maturana_autopoiesis_1980} and aesthetic information theory (\cite{nake_asthetik_1974}, as cited in \cite{schmidhuber_possibility_1991}), yet not the aforementioned psychological literature \parencite[see][for a historical overview]{kaplan_search_2007}. Over the course of history, psychology and AI have actually been converging on similar ideas for why certain behaviors could be intrinsically rewarding: due to some kind of mismatch between bottom-up observations and top-down predictions \parencite{kaplan_search_2007}.

Evolutionary implications of artificial intrinsic-motivational systems discussed earlier (\cref{CH3_S_functions}, \textsc{Functions}) reinforce the need to seriously consider them as good candidate models for human curiosity-driven learning. A major advantage that comes naturally with these systems is their precise formulation. Such a formal description unambiguously discloses crucial structural and functional properties of the system in question, and thus enables to advance the related theory more efficiently \parencite{mcclelland_place_2009}. 

Cognitive models based on exploring artificial agents are becoming increasingly frequent. For instance, \citeauthor{moulin-frier_self-organization_2014} \citeyearpar{moulin-frier_self-organization_2014} explained the progression of human vocal behavior through distinct developmental stages as an intrinsically motivated, competence-progress based goal-exploration process with an emergent curriculum. Such computational accounts of curiosity-driven learning have led to novel hypotheses on the role of curiosity in the evolution of language \parencite[see][]{oudeyer_how_2016}. Gordon and colleagues implemented an intrinsically motivated \ac{RL} agent to account for multiple features of the development of exploratory whisking behavior in rats \parencite{gordon_hierarchical_2012,gordon_emergent_2014}. More recently, \citeauthor{twomey_curiosity-based_2018} \citeyearpar{twomey_curiosity-based_2018} used an actively exploring autoencoder network to hypothesize an algorithmic-level description of visual exploration in infants. \citeauthor{poli_infants_2020} \citeyearpar{poli_infants_2020} compared several knowledge-based sampling strategies to predict visual-attention control in infants. Moreover, computational models of intrinsic motivation are invoked to explain self-determined instrumental \parencite{gershman_uncertainty_2018} and non-instrumental \parencite{ten_humans_2021} choices in human adults.

This chapter relates several mechanistic implementations of curiosity-driven exploratory systems to a common underlying structure. This can be beneficial for revealing potential theoretical and empirical gaps in the scientific understanding of human information-seeking. For instance, while information-seeking literature is brimming with work investigating episodic curiosity-driven sampling over short time scales \parencite[see][for a recent review]{bazhydai_curiosity_2020}, fewer studies have looked at active time-extended curiosity-driven exploration \parencite[e.g.,][]{holm_episodic_2019,ten_humans_2021}. Similarly, while most behavioral paradigms study mechanisms with action-based choice spaces \parencite[e.g., ][]{twomey_curiosity-based_2018}, hardly any studies investigate intrinsically motivated exploration of goals in humans, specifically of how humans generate and choose new goals in unfamiliar sensorimotor environments.

Of course, successful curiosity-driven systems developed in AI are not guaranteed to be good cognitive models of human exploration. Data from carefully designed behavioral studies should bear more weight on the truthfulness of propositions about the mechanisms implemented in biological organisms than the success of these mechanisms in virtual settings. Behavioral studies employing model comparison techniques \parencite[e.g.,][]{poli_infants_2020,ten_humans_2021} can evaluate different AI-inspired proposals by their ability to explain human or animal data. Such studies could also fuel novel ideas to improve existing AI systems, as it was done in instrumental problem settings \parencite{lin_story_2020}. These are exciting areas of future research that can promote synergy between the two fields, allowing us to hone in on a more comprehensive mechanistic understanding of intrinsically-motivated information-seeking in biological organisms.
    
\subsection{Practical Applications}\label{CH3_SS_practical_applications}
Functional diversity of intrinsic-motivational mechanisms makes them useful for practical applications, such as automated knowledge discovery and education. Mechanisms of intrinsic motivation help autonomous agents learn in settings with complex sensorimotor spaces and sparse or non-existent rewards. They are crucial for building autonomous control systems that can learn efficiently in open-ended environments. The practical effectiveness of these mechanisms has been recently demonstrated in studies of automated discovery in complex systems, where curious agents learn to control diverse effects in complex non-linear settings, such as smartphone applications \parencite{pan_reinforcement_2020}, continuous cellular automata \parencite{etcheverry_hierarchically_2021}, and real-world chemical systems \parencite{grizou_curious_2020}. Automated discovery can have a high societal impact by assisting both scientific research and artistic creation. In another line of work, artificial intrinsic motivation has been used to foster generalization in deep \ac{RL} agents by guiding them through automatic-curriculum learning \parencite{portelas_automatic_2020}. Moreover, since one of the functions of curiosity-driven systems is knowledge acquisition, directed exploration strategies of such systems can be used to assist learners when they behave suboptimally. Importantly, such intelligent tutoring systems can be tailored to the current levels of knowledge or competence of individual learners \parencite{clement_comparison_2016} and have shown promising results in pedagogical settings \parencite{clement_multi-armed_2015,delmas_fostering_2018}, where they assist learners in selecting topics and exercises that maximize their individual learning progress.

\section{Conclusion}\label{CH3_S_conclusion}
The goal of this chapter was to familiarize the reader with the diversity of computational mechanisms and possible evolutionary functions of curiosity-driven exploration. We identified an important problem facing autonomous agents that have control over their learning experiences. Specifically, such agents must decide how to sample learning data in the absence of externally imposed tasks. We briefly reviewed several ways in which artificial agents choose actions, goals, or social peers in order to engage in learning situations. We presented distinct families of exploration strategies, including undirected, knowledge-based, competence-based and socially-influenced approaches. We then discussed how these mechanisms can contribute to evolutionary success at different levels: by helping agents to approach primary rewards, acquire world models, discover goals, and bootstrap a cultural repertoire. Finally, we provided some contemporary examples of how intrinsic motivation algorithms are used in practical applications as well as in cognitive research.

We hope that this concise bird’s-eye perspective -- organized along the proposed mechanistic, functional, and pragmatic dimensions of curiosity-driven exploration -- can serve as a stepping stone towards a unified taxonomy of this fascinating and important field. Recently, \citeauthor{gordon_infant-inspired_2020} \citeyearpar{gordon_infant-inspired_2020} proposed a related framework that organizes different curiosity-driven artificial systems along hierarchical levels of cognitive development. In this framework, curious artificial agents can be understood as instantiations of a so-called "curiosity loop" \cite{gordon_hierarchical_2012} consisting of an embodied learner, an action/goal selection mechanism, and an intrinsic reward. Thus, specific curiosity loops operate at different levels of an autonomous-learning hierarchy, where each level is associated with a specific function, including exploration of the self, exploration of the environment, object interaction, and social interaction.

We believe that our present organization and the "curiosity loops" framework are highly complementary. For instance, our taxonomy highlights the parameterization of choice spaces for decision-making (the action selection component in a curiosity loop), pointing out not only the diversity of mechanisms, but also the common underlying structure. At the same time, the consideration of the learner component's learning problem in \citeauthor{gordon_infant-inspired_2020}'s \citeyearpar{gordon_infant-inspired_2020} framework enables to draw more fine-grained functional distinctions between different agents. Specifically, at different points of the developmental trajectory, the learner might prioritize learning different aspects of the world structure, which leads to the acquisition of increasingly complex world models, from self-models, to models of objects, to models of other sentient beings.