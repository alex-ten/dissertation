%************************************************
\chapter{Computational models of artificial curiosity}\label{ch:ai}
%************************************************

As discussed in \autoref{ch:psychology}, the motivation for seeking non-instrumental information is a crucial developmental element that allows humans to acquire useful knowledge. Specifically, we seem to have evolved a certain "liking" for certain experiences for the inherent value of learning from them, not because they are rewarding in a more tangible way. Researchers have proposed that the cognitive mechanism that primes learners to seek potential learning experiences is there to prepare them for future challenges that are as yet unknown \cite{oudeyer_computational_2018,gopnik_childhood_2020}.

Precisely how the curiosity-inducing mechanism operates in biological organisms is poorly understood, but the undeniable significance of curiosity for the eventual acquisition of useful knowledge continues to spur \ac{AI} researchers to develop "curious" learning algorithms. These efforts have generated a great diversity of mechanisms enabling curiosity-driven learning in artificial agents. This chapter identifies common structural dimensions of these mechanisms and relates variation along these dimensions to possible evolutionary functions. Before we discuss these ideas, let us briefly recount how machines learn.

Although defining learning can be controversial \cite{barron_embracing_2015}, in this chapter, I will adopt a straightforward formulation from \ac{ML} \cite{jordan_machine_2015}, where learning is defined as improvement on a task (or a set of tasks) with experience. An artificial agent is said to have improved on a task if -- according to some well-defined criterion -- it is able to perform the task better than it did prior to receiving learning experience. Thus, what drives improvement is a combination of the experience that comes in the form of data that the agent can represent and the learning algorithm (also called learning, or update rule) that specifies how the agent’s innards change by processing the incoming data.

For a long time, major \ac{ML} paradigms have been elaborating increasingly efficient and powerful learning algorithms that optimize pre-specified task criteria. For example, all traditional algorithms of \ac{SL} and \ac{UL} depend on a formally defined objective function which evaluates the agent’s responses to stimuli and thus drives structural changes that result in better responses in the future. While these algorithms can learn many different tasks (e.g. image classification, natural language processing, visual scene parsing etc.), they are typically trained on datasets and objective functions assigned by the engineer. On the other hand, active learning agents \cite{thrun_lifelong_1994,cohn_active_1995} feature algorithms that actively sample the data they learn from. This is particularly the case in \acf{RL} \cite{sutton2018reinforcement} where agents have control over the sampled data by virtue of causal interactions with their environments. Here, learning is driven by an evaluative objective criterion which comes in the form of a reward function. Like in \ac{SL} and \ac{UL}, what the agent ends up learning is determined by a predefined criterion, but additional complications arise because of the need to sample experiences that help the agent improve. In realistic settings, only a tiny fraction of all possible experiences are relevant, which can be further complicated by the sparsity or deceptiveness of rewards \cite[see ][]{oudeyer_computational_2018}. Sampling relevant experiences while avoiding noise is an important problem which will receive much attention in this chapter.

In contrast to \ac{ML} agents that learn externally assigned tasks, biological agents, particularly humans, often have autonomy not only in how they choose experiences to learn tasks, but also in choosing what tasks (i.e., goals) to learn. Moreover, we often seek information when we become curious/interested without being told what to be curious/interested about. Seeking information out of curiosity, rather than to achieve a separable outcome like food or money, is characterized in psychology as intrinsically motivated \cite{harlow_learning_1950,ryan_self-determination_2000}. Intrinsically motivated behavior -- including information-seeking -- also includes pursuing self-selected goals that do not have proximal benefits for biological fitness (e.g., learning tango or climbing Everest). In addition to information-seeking phenomena like curiosity and interest, intrinsic motivation is also linked to other hallmarks of human behavior such as creativity \cite{gross_cultivating_2020} and play \cite{chu_play_2020}.

Similarly to humans but unlike the traditional \ac{ML} systems mentioned above, intrinsically motivated artificial agents control what they learn through autonomous and non-instrumental sampling of learning experiences. This sampling is achieved by considering various features of what we call \emph{learning situations}. To understand what we mean by "learning situations" better and to illustrate how they relate to various mechanisms and functions of intrinsic motivation, consider the following scenarios:

\begin{itemize}
    \item A robot trying random actions in its environment
    \item A rat exploring a maze to get familiar with its environment
    \item A toddler trying to build the tallest possible tower with toy blocks
    \item A curious student raising his hand to pose a question to her teacher
    \item A infant looking at where her mother is pointing
    \item A philosopher considering what book to read
\end{itemize}

All of these scenarios describe an agent interacting with its environment and thereby engaging in a learning situation\footnote{Of course, the agent may not learn upon every single interaction with the environment, but any interaction creates a situation where learning could happen.}. What differentiates these learning situations is the mechanism by which information happens to be sampled. We can identify two core components of computational mechanisms of curiosity-driven exploration. The first component is the interface through which agents actively sample learning situations, i.e. the space in which they can make choices. The second component is the principle by which agents rank learning situations within this space. We describe these components more thoroughly in \autoref{sec:3-mechanisms} (\textsc{mechanisms}), but the examples above already illustrate that agents can sample learning situations completely at random (like the robot), by considering what actions to take (like the rat), what goals to pursue (like the toddler), or whom to ask (the student). Moreover, decisions between alternative actions, goals, or social peers, may be based on prior knowledge (in case of the philosopher), driven by competence (in case of the toddler), or influenced externally, e.g., by others (in case of the infant). Note different mechanisms illustrated by the examples imply distinct consequences, suggesting  that they may serve distinct functions. In \autoref{sec:3-functions} (\textsc{functions}), we organize these functions along the axis of causal proximity to evolutionary fitness: from the rat exploring a maze to the philosopher pondering the most obscure questions. Finally, in \autoref{sec:3-usages} (\textsc{usages}) we discuss the relationships between \ac{AI} and psychology and briefly survey some of the use cases for various intrinsically motivated algorithms outside the realm of \ac{AI} research.

\section{Mechanisms}\label{sec:3-mechanisms}
As mentioned in the introduction, researchers in \ac{AI} have proposed a wealth of algorithms for intrinsically-motivated exploration. These algorithms differ by how they address two related subproblems:

\begin{enumerate}
    \item How to parameterize learning situations?
    \item How to choose learning situations?
\end{enumerate}

The first subproblem is addressed by defining the choice space to drive exploration \cite{moulin-frier_exploration_2013}. Intuitively, a choice space serves as a basis for assessing different learning situations. Actualizing choices in a choice space results in agent-environment interactions from which the agent can learn. This chapter reviews three kinds of choice spaces that consist of either actions, goal states, or social partners. These choice spaces correspond to different ways in which learning situations can be parameterized. Parameterizing learning situations on the basis of \emph{actions} lets the agent consider what can be learned by performing these actions; parameterizing learning situations on the basis of \emph{goals} lets the agent consider what can be learned by pursuing goals; parameterizing learning situations on the basis of \emph{social partners} lets the agent consider what can be learned by interacting with others. We discuss the main ways in which choice spaces are specified and how they differ in \autoref{sec:exploration-bases} (\textit{Exploration bases}).

Given a fully specified choice space, the agent can make decisions within that space. Specifying this decision-making process addresses the second subproblem. Agents choose what to learn by following a certain strategy by which they assign “interestingness” to the available choices, thereby determining which learning situations are more likely to be approached. For example, choices can be driven by features of learning situations, such as the amount of knowledge a situation might bring or how novel it is; they can also be made completely at random \cite[e.g., ][]{colas_gep-pg_2018,colas_language_2020}. We provide a brief survey of these methods in \autoref{sec:exploration-strategies} (\textit{Exploration strategies}).

\subsection{Exploration bases}\label{sec:exploration-bases}
We stated before that a learning situation arises whenever an agent interacts with its environment. Active interactions entail that the agent has to decide how to act in a given context, and upon deciding and acting, gets to observe the effects of its actions. How the agent explores, therefore, depends primarily on how the agent contextualizes its interactions. Specifically, the agent can explore by considering what actions to take, what goals to pursue, or what social partners to engage. Sets of actions, goals, or social partners provide a basis for comparing potential interactions. The rest of this section reviews different ways in which such bases are defined, used, and represented.

\paragraph{Exploring by choosing actions} 
One family of approaches considers agents which observe the current state of the environment as a context, choose actions to execute, and observe the resulting following state. Here, the objective of exploration is to select the actions which generate informative data for learning an internal model of causal dynamics through \ac{SL}, \ac{UL} or \ac{RL}. For example, several \ac{SL}-based robotic agents (Oudeyer et al., 2007; Caligiore et al., 2008; Baranes \& Oudeyer, 2009; Saegusa et al., 2009; Lefort \& Gepperth, 2015) maintain world models representing their knowledge about either forward dynamics (inferring future states from specific actions), or inverse dynamics (inferring the right actions to bring about specific states), or both. These systems learn from observations borne out of the actions they choose. Therefore, exploration of learning situations in these approaches corresponds to making choices in the action space. Examples of action-space exploration can also be found in \ac{RL} settings (Singh et al., 2005, Bellemare et al., 2016; Jaderberg et al., 2016; Pathak et al. 2017; Tang et al., 2017; Burda et al. 2018; Haber et al., 2018, Bougie \& Ichise, 2020a). In intrinsically motivated \ac{RL}, agents learn behavioral policies by maximizing intrinsic rewards (e.g., rewards based on state novelty as in Tang et al., 2017; on model prediction error as in Pathak et al., 2017; or on surprise as in Berseth et al. 2019). In these systems, actions that bring about rewarding states get reinforced. Thus, the agent collects learning data (state transitions) by ranking actions according to their capacity to yield intrinsic rewards. 

\paragraph{Exploring by choosing goals}
Another family of approaches considers agents making choices in a goal space instead of an action space. In the general case, such agents learn to represent and sample their own goals, i.e., they are autotelic (Colas et al., 2020b). This family of approaches is referred to as Intrinsically Motivated Goal-Exploration Processes (or IMGEP for short; Forestier et al., 2016; Colas et al., 2020b). Here, the notion of a goal is generalized: it refers to any set of constraints on any set of future sensorimotor representations (Colas et al., 2020b). This abstract conceptualization enables researchers to express all kinds of goals, ranging from particular world states (e.g., coordinates of the agent’s hand must be equal to a specific x, y, and z), to constraints on entire behavioural trajectories and their linguistic descriptions (e.g., "water the plant and then feed the dog"; Colas et al., 2020a). In all cases, goals are specified by two essential components: (1) goal representation that specifies the criteria and (2) goal-achievement function that signals whether the criteria are met. Goals are usually represented as numerical vectors (sometimes called goal embeddings) that comprise abstract goal spaces from which specific goals can be sampled, while goal achievement is evaluated using logical operations.

Examples of IMGEPs can be found in \ac{SL} and \ac{UL} contexts (Jordan \& Rumelhart, 1992; Rolf et al., 2010; Baranes \& Oudeyer, 2013, Moulin-Frier \& Oudeyer, 2013; Forestier et al., 2016; Takahashi et al., 2017; Reinke et al., 2019; Laversanne-Finot et al., 2020). In these frameworks, agents autonomously engage in learning situations by attempting to reach self-selected goals. Note that this process is markedly different from the one described above, where the agent accesses learning situations by choosing among the available actions. In IMGEPs, the agent can consider any goal it may imagine, not just the states that its actions may bring about.

The domain of goal-conditioned \ac{RL} works with agents that learn goal-conditioned action policies. These agents base their actions not only on the current state (as in traditional \ac{RL}) but also the goal encoding, which means they can act differently in the same situation depending on what they are after (Schaul et al., 2015). Intrinsically motivated goal-conditioned \ac{RL} builds upon that framework and allows agents to generate their own goals (see Colas et al., 2020b for a recent review). Some notable examples include (Colas et al. 2018; Nair et al. 2018; Colas et al. 2019; Pong et al. 2019, Colas et al. 2020a). While there is a great deal of variability among strategies for sampling interesting goals (see Exploration strategies), all of these goal-oriented agents make decisions in a goal space rather than in an action space. Goal-oriented intrinsically motivated learning has a number of advantages compared to action-oriented intrinsically-motivated learning: it improves the performance and convergence time when learning inverse models in high-dimensional spaces with highly non-linear mappings (Baranes \& Oudeyer 2013), it automatically generates learning curricula from easy to more complex skills (Moulin-Frier et al., 2014); finally, it enables hindsight learning (Forestier et al., 2017; Andrychowicz et al. 2017; Colas et al., 2019). We discuss these positive practical implications in more detail in Functions.

\paragraph{Exploring by choosing social partners}
A few contributions have explored how IM can be coupled with social interaction. The SGIM-ACTS architecture (Nguyen \& Oudeyer, 2012) considers an IM agent that is able to choose whether and when to learn from a social peer or by autonomous goal generation, from which social peer to learn, and what to ask to the chosen social peer. Interacting with the social peer becomes part of the choice space and the agent makes hierarchical decisions: it first decides to interact or to self-explore its own goals, then which social peer or self-generated goal to focus on. The SGIM-ACTS framework was also applied to agents equipped with a realistic computer model of the human vocal tract and was able to reproduce the main developmental stages of infant vocal development (Moulin-Frier et al., 2014). More recent contributions consider social influence as intrinsic motivation for achieving coordination and communication in multi-agent \ac{RL} (Jaques et al., 2019). Other work in multi-agent \ac{RL} have theorized and demonstrated how competition and cooperation display intrinsic dynamics resulting in a naturally emergent curriculum (Leibo et al., 2019, Baker et al., 2020).

\subsubsection{Representing choice spaces}
Precisely what do choices in choice spaces represent? In the case of action-space exploration, actions can represent micro-actions responsible for transitions between temporally adjacent momentary states like pixel images (e.g., Bellemare et al., 2016; Pathak et al. 2017). In this case, the agent can only learn from short-term transitions which limits their ability to efficiently learn regularities spanning larger time scales\footnote{Model-free \ac{RL} agents (e.g. Bellemare et al. 2016; Pathak et al., 2017) do not learn from the transitions per se -- they learn from rewards. Specifically, the agent usually learns a value function V, mapping states to their expected cumulative reward. Even if the transitions are momentary, the agent can still maximize their long-term cumulative reward using techniques like bootstrapping (Sutton \& Barto, 2018). Another exception is when the world model is represented by a recurrent neural network (RNN, Takahashi et al., 2017) as RNNs can encode time series.}. However, the agent can sample learning situations by making decisions in the space of macro-actions (e.g., action-policies), rather than micro-actions, which enables exploration of more temporally extended effects of its actions (see Baranes \& Oudeyer, 2013). 

In the case of goal sampling, the choice space can correspond entirely to the state space, so that a given goal represents a particular state. This can be problematic for very high-dimensional spaces. For example, if the state space is a space of pixel images, because sampling directly from this space yields white noise most of the time, just like blindly picking alphabet letters would produce mostly gibberish (Nair et al., 2018). One solution to this problem is to define the goal space as some high-level feature space (also called latent or embedding space) of the raw-image space (e.g., Laversanne-Finot et al., 2020). Following the letter-picking analogy this would correspond to composing and sampling from a higher-level space of syllables or words which is more likely to produce meaningful strings. A related issue in cases when the goal space is defined over some abstraction over the raw sensory experience (e.g., 2D positions of objects vs raw pixel images), is whether to assume that this abstract space is given to the agent like in Forestier et al. (2017) or whether the agent needs to learn a latent goal space from scratch (e.g., Nair et al., 2018; Laversanne-Finot et al., 2020). Whether learned or given, latent goal representations help agents target valid achievable goals (Pathak et al, 2017).

\subsection{Exploration strategies}\label{sec:exploration-strategies}
Given a well-defined choice space for exploration, what strategies can an artificial agent follow to decide which learning situations are more or less interesting? Comprehensive reviews of different approaches can be found elsewhere (Oudeyer \& Kaplan, 2009; Mirolli \& Baldassarre, 2013; Aubret et al., 2019; Linke et al., 2020), so we only provide a short survey of different approaches with a focus on their diversity rather than precise implementations. 

We group existing approaches into three main categories: undirected, knowledge-based, and competence-based exploration. While all learning situations are equally interesting in undirected exploration, directed exploration strategies scale interestingness with the agent’s abilities. Directed exploration can be divided into two broad classes: knowledge-based and competence-based strategies (Oudeyer \& Kaplan, 2009). Sometimes the distinction between the two can be subtle because knowledgeable systems can also be competent and competent systems can be knowledgeable (Mirolli \& Baldassarre, 2013). The point of divergence for these families of mechanisms is that to a knowledge-based system, the interestingness of a learning situation is determined by its relation to the system’s knowledge. On the other hand, to a competence-based system, a given learning situation may be more or less interesting because it relates to the system’s ability to reach a specific self-generated goal.

\subsubsection{Undirected exploration}
Undirected exploration (sometimes random or uniform exploration) refers to a strategy that assigns interest uniformly across the choice space (making all learning situations equally interesting). The effectiveness of this simple strategy is inconsistent across different settings. When applied to action-space exploration, for example, undirected exploration is only effective for simplistic problems (Baranes \& Oudeyer, 2013; Benureau \& Oudeyer, 2016), for example, when the environment provides dense rewards or when actions have simple and consistent effects. In more challenging settings, where the mapping between actions and their effects exhibits a combination of non-linearity, stochasticity, and redundancy, motor exploration is not sufficient for effective learning, but goal exploration could be (Moulin-Frier \& Oudeyer, 2013). This is largely because learning how to reach goals contributes to the agent’s competence and thus its ability to control the environment, while learning about various outcomes of all of one’s actions in all possible contexts may be worthless in practice (Mirolli \& Baldassarre, 2013). Besides, trying out random actions from a particular state may be futile for reaching certain hard-to-reach regions of the state space. Think of how hard it would be to learn how to drive from home to work by performing random actions (you would end up crashing your car most of the time). If instead you were to learn how to drive to various places from home (your driveway, a cornershop, a nearby gas station) your chances of finding your way to your office eventually would be much higher. 

Despite its simplicity, random goal exploration has proven to be surprisingly efficient, leading to some forms of novelty search as an emergent feature and surpassing directed approaches operating in the action space in the learning of redundant inverse mappings (Baranes and Oudeyer, 2013; Benureau \& Oudeyer, 2016; Colas et al., 2018). Because it is simple and computationally cheap, random goal exploration is often combined with other strategies, either to jumpstart the primary mode of directed exploration by collecting initial data, or sometimes as a complementary strategy at a certain level of hierarchical sampling decisions in modular spaces (e.g. Forestier \& Oudeyer, 2016), and sometimes as a epsilon-greedy strategy (see Sutton \& Barto, 2018) to balance between random and directed exploration (e.g. Colas et al., 2019). Still in many situations, undirected exploration may not be as efficient or effective as more sophisticated guided exploration approaches. For example, random (goal) exploration performs poorly when the space of effects has a hierarchical structure, so that certain states are only accessible through reaching some prerequisite states (e.g. Forestier et al., 2017). An agent exploring goals randomly is unlikely to ever get to practice these “out-of-reach” goals, unless there is a mechanism for imagining them that leverages structured representations of goals such as natural language encodings (e.g. Colas et al., 2020a).

\subsubsection{Knowledge-based exploration}
Knowledge-based exploration is perhaps the most diverse family of intrinsically motivated strategies. Not only are there many ways in which one can characterize a relation between learning situations and the agent’s knowledge, but there are many kinds of knowledge that agents represent. For example, an agent can maintain a predictive causal model of the effects of its actions and have a metacognitive monitoring system track errors that this predictive model commits. Equipped with such a system, the agent can measure interestingness of actions based on, for example, outcome prediction error of the forward model (e.g. Saegusa et al., 2009). Specifically, the agent can be more (or less) interested in taking actions for which the forward model does accurately predict the consequences (known as prediction-error strategy). Alternatively, the agent can track changes in prediction accuracy (a strategy known as learning progress; e.g. Schmidhuber, 1991b; Oudeyer et al., 2007; Kim et al., 2020). Here the agent would be more (or less) interested in actions, predictions for which get more accurate with time. While the measure of interestingness in these approaches is based on the predictions from a forward model, exploring agents can also monitor the behavior of an inverse dynamics model (Pathak et al., 2017; Haber et al., 2018).

A distinct subclass of knowledge-based strategies relies on knowledge about frequencies of observed states. In so-called count-based approaches (Bellemare et al., 2016; Tang et al., 2018), this knowledge is represented explicitly allowing the agent to selectively explore over- or under-visited states. Other systems in this subclass of approaches incorporate different variants of autoencoder networks to learn latent spaces (Twomey \& Westermann, 2018; Bougie \& Ichise, 2020a) or generative models (Nair et al., 2018; Pong et al., 2019) of states observed by the agent. Specifically, Twomey \& Westermann (2018) defined several interestingness measures based on backpropagation computation of their category-learning neural network, including measures of weight update, prediction error, and activation-function derivative (which model, respectively, the system’s curiosity, novelty, and plasticity). Bougie \& Ichise (2020a) introduced auxiliary tasks of image reconstruction with context-based autoencoders and defined an intrinsic reward measure derived from reconstruction errors from these tasks. Others (Nair et al., 2018; Pong et al., 2019) employed variational autoencoders to estimate valid-state distributions in order to guide exploration. Although these models do not explicitly encode state visitation counts, the interestingness measures defined on their basis are related to frequencies of the observed states. Autoencoder prediction error, for instance, should decrease with repeated exposure to a given state. Variational autoencoders additionally represent the statistics of the latent space -- a feature that can be used to estimate the likelihood of any state (including completely novel states) given what has been observed in the past. 

\subsubsection{Competence-based exploration}
Another family of strategies assigns interestingness based on competence. These approaches do not need to assume any explicit world-dynamics knowledge model (although they can), so they can be readily incorporated in model-free learning systems without the need to introduce any auxiliary tasks. Competence-based exploration strategies are especially well-suited for intrinsically motivated agents that explore self-imposed goals, since the notion of competence corresponds naturally to goal achievement. A simple competence-based heuristic in a goal-oriented context is to sample goals according to the agent’s ability to achieve them. For example, Bougie \& Ichise (2020b) reward the agent in a given state based on how incompetent the agent perceives itself to be in that state. Here the agent generates its data set by taking interest in actions that lead it to states at which it deems itself incompetent, i.e. by choosing actions that maximize incompetence. In a different approach, Florensa et al. (2018) leveraged the generative power of adversarial networks for generating goals, which the agent evaluates based on the probability of reaching them. Thanks to this evaluation, their agent can prioritize goals of intermediate difficulty, thereby avoiding goals that it already knows how to achieve and goals that it knows it cannot achieve. Santucci et al. (2013) compared the efficacy of several interestingness measures for autonomous mastering a set of tasks that included unreachable distractor tasks. The best-performing measure that allowed their agent to learn all learnable tasks in the least amount of time was based on competence prediction-error. Stout \& Barto (2010), Forestier et al. (2017) and Colas et al. (2019) explored modular goal spaces using measures of competence progress. Agents in these studies were incentivised to sample goals from the predefined regions (modules) of the goal space where competence was either improving or deteriorating. Baranes \& Oudeyer (2013), Moulin-Frier et al. (2014) and Laversanne-Finot et al. (2018) used a similar competence progress-based strategy but in settings where the singular goal-space was progressively modularized.

\section{Functions}\label{sec:3-functions}
The previous section reviewed the diversity of approaches to specifying intrinsically-motivated mechanisms in \ac{AI}.  We have mentioned in the introduction that these mechanisms are useful for autonomous learning in the absence of externally assigned objectives, but what specific functional consequences do different intrinsic motivation mechanisms bear?  This section focuses on the functional aspects of intrinsically motivated systems addressing the question of how different intrinsic-motivational systems are useful for both artificial and biological agents.  

Singh et al. (2010) provide a useful connection between the concepts of extrinsic and intrinsic motivation and the concepts of primary and secondary rewards. While the reception of primary primary (e.g., related to nutrients, sex, or pain) contributes to an agent’s survival and, thus, its reproductive success, secondary reinforcers signal anticipation of primary rewards, but are neutral a priori and have to be learned. In \ac{RL}, outputs of the reward function can be analogous to primary reward signals, because the reward function is given to the agent. However, value functions evaluate states based on their learned expected cumulative reward, and therefore outputs of the value function are analogous to secondary reinforcers. In this formulation, intrinsic motivators (e.g., novelty, uncertainty, learning progress etc.) are primary reinforcers because their rewardingness is a given. On the other hand, predictors of intrinsic primary reinforcers can acquire rewarding qualities through the learning of secondary reinforcers, in the same way as non-rewarding states gain value due to extrinsic reinforcers. Therefore, intrinsic primary rewards differ from extrinsic primary rewards mostly due to their causal proximity to evolutionary success. As it is often the case in biology (Dobzhansky, 1973), it is sensible to analyze the functional aspect of artificial intrinsic-motivational mechanisms in light of evolution -- an exercise that allows us to consider the possible bio-ecological roles of engineered curiosity-driven systems.
In what follows, we build upon Singh et al.’s (2010) framework as well as the taxonomy of mechanisms we proposed in the previous section in order to extract key functional aspects of intrinsic motivation in both biological and artificial agents. We start from the more evolutionarily proximal functions (direct procurement of primary rewards) and gradually consider increasingly distal ones (learning of internal models, goal discovery, and cultural innovation).

\subsection{Procurement of extrinsic primary rewards}
In relatively dense primary-reward environments, unstructured random exploration in the action space is sufficient to efficiently elicit extrinsic primary rewards (as it is the case in some standard \ac{AI} benchmarks based on video games, e.g., Mnih et al., 2015). This points to the most proximal function of intrinsic motivation: the generation of diverse sensorimotor experiences. The direct benefit of generating diverse experiences is to increase the probability of eliciting primary extrinsic rewards. However, the diversity generated by random exploration is usually not sufficient in sparser reward environments (e.g. Pathak et al., 2017). Knowledge-based exploration can increase the diversity of learning experiences by guiding exploration based on different measures of interestingness (see section Exploration principles). Discovering extrinsic primary rewards such as food, water, or shelter through exploration is clearly linked to the agent’s well-being.

\subsection{Learning internal models} A more distal function of intrinsic motivation is the learning of internal models of the agent-environment interaction (e.g. forward or inverse models) that can enhance the agent’s decision-making abilities. Learning such a model can be formalized as an \ac{UL} or an \ac{SL} problem and strongly relies on the information contained in the training dataset. Autonomous agents generate this dataset by interacting with the environment and the key role of intrinsic motivation is to generate informative training data. Research in intrinsically motivated \ac{SL} extensively studied two main cases. On the one hand, knowledge-based approaches have proven to be efficient in generating informative data for learning forward models (Oudeyer et al., 2007). On the other hand, competence-based approaches have proven to be more efficient than knowledge-based approaches for learning inverse models (Baranes \& Oudeyer, 2013). \ac{RL} agents can also benefit from internal models, be it in the form of a value function and an action policy in model-free \ac{RL} (Pathak et al., 2017), or in the form of a world-dynamics model (forward or inverse) in model-based \ac{RL} (Haber et al., 2018). Understanding how the world works per se does not put proverbial food in the agent’s mouth, but it allows the agent to act more intelligently in novel situations and plan ahead in order to obtain what it needs more reliably.

\subsection{Goal discovery} The third level of functions we propose is related to the discovery and learning of novel goals and the associated skills to achieve them. This is the main function of competence-based approaches, where exploration is guided by the pursual of self-imposed goals. These approaches can automatically organize exploration from simple to more complex skills (Forestier et al., 2017; Pong et al., 2019), as well to discover the full range of the achievable behavioral repertoire, possibly in an open-ended manner (Colas et al., 2019). There are multiple functional advantages to discover and master novel goals that are not extrinsically rewarding. First, in environments where eliciting extrinsic primary rewards require the acquisition of complex skills (e.g., hunting), it is crucial to structure learning in a curriculum from simple (e.g., locomotion) to more complex skills. Complex skill sets often display a hierarchical structure, where mastering easier ones is prerequisite for acquiring more complex ones. Second, the ability to autonomously explore and discover new goals and skills provides a crucial advantage in changing environments. For example, paleoclimatological data provides evidence for strongly varying climate conditions in the Rift Valley at East Africa approximately 7 million years ago and it is hypothesized that the ability to rapidly and flexibly reorganize a diverse behavioral repertoire was a key requirement to adapt to such unprecedented conditions (Potts, 2013). Thus, the ability to autonomously generate and master novel goals and thereby acquire a diverse repertoire of complex skills provides a crucial advantage for a species's success in such settings of strong environmental variability (see Nisioti \& Moulin-Frier, 2020, for a recent proposition to apply this principle in \ac{AI}).

\subsection{Cultural innovation} Finally, the fourth and most distal level of functions we propose concerns cultural innovation. Several theoretical contributions proposed a potential role of curiosity-driven exploration in both language acquisition (Oller, 2000) and evolution (Oudeyer \& Smith, 2016). From a sensorimotor perspective, active exploration can spontaneously generate diverse behaviors from modality-independent and task-independent internal drives. Such spontaneous behavior can result in vocal activity that may have bootstrapped the emergence of communication. This hypothesis is supported by computational simulations showing a role of curiosity-driven exploration in vocal development (Moulin-Frier et al., 2014), social affordance discovery (Oudeyer \& Kaplan, 2006), and the active control of complexity growth in naming games (Schueller \& Oudeyer, 2015). From a cognitive perspective, compositional language itself is a powerful cognitive tool to imagine novel out-of-distribution goals in competence-based intrinsic motivation (Colas et al., 2020a).  Moreover, recent contributions in multi-agent \ac{RL} have shown how an auto-curriculum of increasingly complex behaviors displaying features of open-ended innovation can emerge from agents’ co-adaptation in mixed cooperative-competitive environments (Baker et al., 2020). Such mechanisms are potential precursors of cultural evolution in the human species. Cultural evolution has triggered increasingly complex technological innovation across generations (Fogarty \& Creanza, 2017). A prime example of this is the industrial revolution which has resulted in a rapid acceleration of global population growth in the 19th century (Lucas, 2002).

\section{Usages}\label{sec:3-usages}
Other than helping artificial agents explore learning situations in abstract task-independent contexts, how can intrinsically-motivated learning algorithms be used? We identify two main directions in which such algorithms can have high impact. On the one hand, they can be a great tool for advancing research in cognitive psychology and neuroscience. Outside cognitive research, these algorithms can be applied directly to problems that require intelligent automated exploration. We review both of these applicational domains in the rest of this section.
    
\subsection{Cognitive modeling} The notion of intrinsically-motivated exploration in psychology has been developing -- for the most part -- independently from \ac{AI} (Kaplan \& Oudeyer, 2007). Psychological investigations can be traced back to psycho-physiological perspectives on exploration in the early 1940s (Hull, 1943). Since then, psychological views on curiosity and information-seeking have undergone multiple changes (Loewenstein, 1994; Bazhydai et al., 2020) and are now becoming more integrated with the computational perspective (Kaplan \& Oudeyer, 2007; Gottlieb et al., 2013). On the other hand, early formulations of intrinsic motivation in \ac{AI} were either “discovered by accident” (Andreae, 1977, p. 5) or influenced by relatively distant research areas, like biological autopoiesis (Maturana \& Varela, 2012) and aesthetic information theory (Nake, 1974, as cited in Schmidhuber, 1991a), yet not the aforementioned psychological literature (see Kaplan \& Oudeyer, 2007, for a historical overview). Over the course of history, psychology and \ac{AI} have actually been converging on similar ideas for why certain behaviors could be intrinsically rewarding: due to some kind of mismatch between bottom-up observations and top-down predictions (Kaplan \& Oudeyer, 2007).

Evolutionary implications of artificial intrinsic-motivational systems discussed earlier (see Functions) reinforce the need to seriously consider them as good candidate models for human curiosity-driven learning. A major advantage that comes naturally with these systems is their precise formulation. Such a formal description unambiguously discloses crucial structural and functional properties of the system in question and thus enables to advance the related theory more efficiently (McClelland, 2009). 

Cognitive models based on artificial exploring agents are becoming increasingly frequent. For instance, Moulin-Frier et al. (2014) explained the progression of human vocal behavior through distinct developmental stages as an intrinsically motivated, competence-progress based goal-exploration process with an emergent curriculum. Such computational accounts of curiosity-driven learning have led to novel hypotheses on the role of curiosity in the evolution of language; see Oudeyer and Smith, 2016. More recently, Twomey and Westermann (2018) used an actively exploring autoencoder network to hypothesize an algorithmic-level description of visual exploration in infants; while Poli et al. (2020) compared several knowledge-based sampling strategies to predict visual-attention control in infants. Moreover, computational models of intrinsic motivation are invoked to explain self-determined instrumental (Gershman, 2019) and non-instrumental (Ten et al., 2021) choices in human adults.

Relating specific mechanistic implementations of curiosity to a common underlying structure can be beneficial for revealing potential theoretical and empirical gaps in the scientific understanding of human information-seeking. For instance, while information-seeking literature is brimming with work investigating episodic curiosity-driven sampling over short time scales (see Bazhydai et al., 2020, for a recent review), only a handful of studies have looked at time-extended curiosity-driven exploration (Holm et al., 2019; Ten et al., 2020). Similarly, while most behavioral paradigms study mechanisms with action-based choice spaces (e.g., Twomey \& Westerman, 2018), hardly any studies investigate intrinsically motivated exploration of goals in humans, specifically of how humans generate and choose new goals in unfamiliar sensorimotor environments.

Of course, successful curiosity-driven systems developed in \ac{AI} should not be simply adopted as cognitive models of human exploration. Data from carefully designed behavioral studies should bear more weight on the truthfulness of propositions about the mechanisms implemented in biological organisms than the success of these mechanisms in virtual settings. Behavioral studies employing model comparison techniques (e.g., Poli et al., 2020; Ten et al., 2020) can evaluate different \ac{AI}-inspired proposals by their ability to explain human or animal data. Such studies could even fuel novel ideas to improve existing \ac{AI} systems, as it was done in instrumental problem settings (Lin et al., 2019). These are exciting areas of future research that can promote synergy between the two fields, allowing us to hone in on a more comprehensive mechanistic understanding of intrinsically-motivated information-seeking in biological organisms.
    
\subsection{Practical applications} 
Functional diversity of intrinsic-motivational mechanisms makes them useful for practical applications, such as automated knowledge discovery and education. Mechanisms of intrinsic motivation help autonomous agents learn in settings with complex sensorimotor spaces and sparse or non-existent rewards. They are crucial for building autonomous control systems that can learn efficiently in open-ended environments. The practical effectiveness of these mechanisms has been recently demonstrated in studies of automated discovery in complex systems, where curious agents learn to control diverse effects in complex non-linear settings, such as smartphone applications (Pan et al., 2020), continuous cellular automata (Etcheverry et al., 2020), and real-world chemical systems (Grizou et al., 2020). Automated discovery can have a high societal impact by assisting both scientific research and artistic creation. Moreover, since one of the functions of curiosity-driven systems is knowledge acquisition, directed exploration strategies of such systems can be used to assist human learners when they behave suboptimally. Importantly, such intelligent tutoring systems can be tailored to the current levels of knowledge or competence of individual learners (Clement et al., 2016) and have shown promising results in pedagogical settings (Clement et al., 2015; Delmas et al., 2018), where they assist learners in selecting topics and exercises that maximize their individual learning progress, leading both to efficient overall knowledge acquisition and to fostering learner's intrinsic motivation.
    
\section{Conclusion}
The goal of this chapter was to familiarize the reader with the diversity of computational mechanisms and possible evolutionary functions of curiosity-driven exploration. We identified an important problem facing autonomous agents that have control over their learning experiences. Specifically, such agents must decide how to sample learning data in the absence of externally imposed tasks. We briefly reviewed several ways in which artificial agents choose actions, goals, or social peers in order to engage in learning situations. We presented distinct families of exploration strategies, including undirected, knowledge-based, competence-based and socially-influenced approaches. We then discussed how these mechanisms can contribute to evolutionary success at different levels: by helping agents to approach primary rewards, acquire world models, discover goals, and bootstrap a cultural repertoire. Finally, we provided some contemporary examples of how IM algorithms are used in practical applications as well as in cognitive research. We hope that this concise bird’s-eye perspective, organized along the proposed mechanistic, functional and applicative dimensions of curiosity-driven exploration can serve as a stepping stone towards a unified taxonomy of this fascinating and important field.