%************************************************
\chapter{Metacognition}\label{ch:metacognition}
%************************************************

% Intro
\begin{flushright}{\slshape
    The world is my idea. \\
    --- A. Shopenhauer \cite{schopenhauer_world_2016}}
\end{flushright}
As many insightful people before him, Shopenhauer recognized that we all see the world through our own personal lenses of perception. His original words "Die Welt ist meine Vorstellung" are sometimes translated as "the world is my \emph{representation}", which should sound more familiar to a modern-day cognitive scientist. Indeed, behaviors we observe in humans and other animals are caused by idiosyncratic chains of neurophysiological events assumed to implement functional computations from certain inputs to certain outputs. Intermediate stages of these computations feature interactions between representations: of various external and internal variables, and of current goals and needs. In this chapter, I explore how the representation of \acf{LP} might form and manifest inside the human mind. 

% Quick recap of evidence for learning progress
Previously, I discussed the idea that information-seeking is motivated by expected improvement of knowledge or \ac{LP}. A few studies lend empirical support for this theory by measuring various task features and examining their relationships with task engagement. For example, Gerken et al. \cite{gerken_infants_2011} measured objective complexity of experimental stimuli and tested its relationship with attention. Poli et al. \cite{poli_infants_2020} made a step further by proposing several candidate mechanisms processed stimuli of different complexity and examining how their outputs related to looking time. Leonard et al. \cite{leonard_young_2021} manipulated performance feedback to control how children perceived their task progression and observed whether they were willing to carry on. Son and Metcalfe \cite{son_metacognitive_2000} asked university students to report how well they thought they would remember information from different texts and looked at how well these reports predicted time allocation and prioritization of these texts. In our own study \cite{ten_humans_2021}, we manipulated the complexity of categorization rules of different stimulus families and measured people's ongoing performance to understand its relation to self-directed learning. While these studies suggest different ways to measure \ac{LP}, they do not make explicit claims about the algorithmic implementation of the underlying computation.

% Prospective vs retrospective judgments
It is important to note that the studies reviewed above are concerned with how \emph{prospective} \ac{LP} judgments influence motivation. How such \ac{LP} expectations arise is beyond the scope of this chapter, because there are probably many variables and processes that jointly determine people's expectations about their future learning and achievement. A relevant framework for exploring the mechanisms of prospective \ac{LP} expectations is Bandura's theory of \emph{self-efficacy} \cite{bandura_self-efficacy_1977}. Bandura proposed several qualitatively different sources that contribute to one's belief about being able to perform a task, including (1) performance accomplishments, (2) vicarious experiences, (3) verbal persuasion, and (4) emotional arousal. The present chapter is concerned with a single part of this belief-formation process: the subjective evaluation of one's learning/performance dynamics. 

% Motivation and goals
What information does the human brain access and represent in order to compute the \ac{LP} on a given task? Computational literature proposes several operational mechanisms \cite{oudeyer_what_2007,graves_automated_2017,twomey_curiosity-based_2018,linke_adapting_2020}. All of these mechanisms assume an interaction between two distinct functional components, or modules. One module -- let's call it the \emph{task module} -- is a mechanism that learns to perform a task at hand. It serves to convert sensory/mnemonic inputs into response outputs (e.g., motor action, categorization, etc.). The second module -- the \emph{meta-module} -- serves to evaluate the task module in order to inform decisions pertaining to active learning and/or task selection. The meta-module performs some kind of differentiation of the task module's performance in order to generate the \ac{LP} signal.  

% A taxonomy
Despite the similarities, we can identify two fairly distinct families of mechanisms. The first one -- called \emph{performance-based mechanisms} -- assumes that the meta-module computes \ac{LP} based on the task module's performance. Here, the task module is essentially a blackbox and the meta-module's job is to infer how this blackbox learns by observing its behavior. The second family -- \emph{introspective mechanisms} -- assumes that \ac{LP} is computed by observing the structural changes in the task module itself. Here, the meta-module has elevated access to the task module's inner structure, which allows it to observe and quantify changes in the task module as it learns its task.

\section{Mechanisms of progress computation}

\subsection{Performance-based mechanisms}\label{subsec:performance-based_mechanisms}

% Schmidhuber (1991)
A commonly used approach to estimating \ac{LP} in \ac{AI}, is to compute a temporal derivative of the task module's performance trajectory. Computational architectures of intrinsically-motivated exploration provide numerous examples of how such computations can be carried out and what information needs to be represented to support them. An early algorithm by Schmidhuber \cite{schmidhuber_curious_1991} estimates \ac{LP} as:
\begin{equation}
    \mathrm{LP(t)} = o_C(t) - o_C'(t)
\end{equation}
where $o_C(t)$ is an estimated reliability of the task module -- or the "confidence" at time $t$ of the meta-module in the competence of the task module; the term $o_C'(t)$ denotes the reliability estimate after the meta-module had been adjusted to predict the reliability of the task module more accurately. Here, \ac{LP} is computed by comparing point estimates of subjective confidence before and after updating the meta-module with relevant information. As per Schmidhuber's original proposition, this information can be derived by counting how many times the task in a given context was performed well and how many times the task was attempted in this context.

% Oudeyer, Kaplan, & Hafner (2007)
In a different algorithm by Oudeyer, Kaplan, and Hafner \cite{oudeyer_intrinsic_2007} \ac{LP} is defined as:
\begin{equation}
    \mathrm{LP(t)} = e_R(t) - e_R(t-\tau)
\end{equation}
where $e_n(t)$ is the average prediction error of the task module prior to time $t$; the parameter $\tau$ controls the temporal reference point to which $e_n(t)$ is compared. The original algorithm also parameterizes the computation of the prediction-error averages to control their smoothness. Importantly, \ac{LP} is computed separately for different regions, indexed by $R$, of the sensorimotor space to prevent the agent from "maximizing" progress by alternating between attempting unrelated low- and high-error tasks in an undifferentiated space. Like in Schmidhuber's algorithm, the mean error term $e_n(t)$ can also be construed as the meta-module's confidence in the task module, as it represents the error rate in a given context. 

% Colas et al. (2019), Baranes & Oudeyer (2013)
Similar algorithms have been used to compute competence progress \cite{baranes_active_2013,santucci_which_2013,colas_curious_2019,forestier_intrinsically_2020} -- a temporal derivative of the agent's ability to reach its goals in a specific task space. For instance, Colas et al. \cite{colas_curious_2019} defined \ac{LP} as:
\begin{equation}
    \mathrm{LP(n)} = |c_R(n) - c_R(n-\tau)|
\end{equation}
where $c_R(n)$ is the subjectively estimated competence of the agent in a discrete task space, indexed by $R$; $n$ is the number of self-evaluations performed to estimate the competence score. Subjective competence is evaluated by weighting binary goal-achievement outcomes in a task space by recency and taking the average of the weighted scores\footnote{Colas et al. \cite{colas_curious_2019} used a queue-based implementation, but the effect of the computation is the same as taking a recency-weighted average of a binary vector.}. Competence is computed for all $n$ self-evaluation trials and again for a more recent $n-\tau$ portion of these trials, and the two estimates are compared. Note that here, the absolute value of the derivative is taken. While not essential to the definition of \ac{LP}, this functional form raises the question of whether improvement and deterioration in performance are equivalent for motivation and what their differences might be. In Colas et al. \cite{colas_curious_2019}, taking the absolute value of the competence differential allowed the agent to actively practice tasks on which it was getting worse over time, which ensured that the overall competence was maximized.

% The significance of uncertainty
In the approaches discussed above, the represented measure of performance can be thought of as reflecting the agent's subjective belief about its performance, suggesting that changes in subjective beliefs might underlie \ac{LP} computation. However, the above mechanisms rely on point-estimate representations that do not account for belief uncertainty. On the other hand, psychological literature suggests that declarative statements (e.g., "I can play the piano") emerge from supporting representations of varying degrees of internal consistency \cite{smith_belief_1991,koriat_self-consistency_2012}. More recently, research in neuroscience has started to unravel the neural mechanisms underlying uncertainty and confidence judgments. The so-called distributional uncertainty inherent in neural activity can potentially explain how the brain computes and represents propositional confidence \cite{meyniel_confidence_2015,pouget_confidence_2016}. If the computation of \ac{LP} involves belief comparison, then belief uncertainty should have a considerable footprint on the process. 

% Bayesian beliefs, uncertainty and confidence
A suitable tool for studying dynamic uncertain beliefs is the Bayesian framework for cognitive modeling, which assumes that humans represent beliefs probabilistically \cite{sun_bayesian_2008,perfors_tutorial_2011,coenen_asking_2019}. Sensitivity to uncertainty enables agents to intelligently switch between exploration and exploitation \cite{cohen_should_2007} and control the learning of new information \cite{meyniel_confidence_2015}. In addition to uncertainty inherent in belief distributions, individual beliefs (that a particular decision or proposition is true) are associated with a distinct kind of uncertainty that manifests in confidence judgments \cite{pouget_confidence_2016}. Confidence in beliefs is often sufficient to support decisions and might be a simplified computational substitute for overly complex belief posteriors \cite{pouget_confidence_2016}.  

% Formal description
Probabilistic beliefs (that one can accomplish a task) can be characterized by more or less confidence and change as a result of self-monitoring. The evolution of such beliefs can be expressed in terms of posterior probability. For example, suppose that an agent represents a state space, a subset of which is a state-achievement event $A$ that has a probability of occurring $P(A)$. This probability can be framed as the subjective belief that event $A$ can be reached by the agent. Since, $P(A)$ is unknown a priori, it has to be inferred from some relevant observations:
\begin{equation}
    P(A|D) = \frac{P(D|A) P(A)}{P(D|A)P(A) + P(D|A^c)P(A^c)}
\end{equation}
where $A^c$ denotes the complement of $A$, and $D$ is the observed data. This equation prescribes an optimal way to update a (binary) state-achievement belief by combining the prior expectation $P(A)$ with the normalized likelihood of that belief $P(D|A)$. Both of these components reflect the agent's uncertain knowledge about its abilities to predict the future or reach specific goal states.

% Prior knowledge
Prior confidence $P(A)$ biases how the observed data influences the posterior. For example, imagine that while estimating confidence, all that the agent observes is external binary feedback on some goal-achievement task. Now, consider the following priors and likelihood values:
\begin{center}
\begin{tabular}{c c c c}
Belief & $P(A)$ & $P(D=\mathrm{success}|A)$ & $P(D=\mathrm{fail}|A)$ \\ 
$A$    & .99    & .90                       & .10        \\  
$A^c$  & .01    & .05                       & .95         
\end{tabular}
\end{center}
The posterior from a 'success' outcome will be .9994 (an increase of .0094), while a 'fail' outcome will give us a posterior of .9124 (a decrease of .0776). Thus, high prior confidence in accomplishing the task results in asymmetric updates for different outcomes. Incidentally, the surprise from observing a failure while strongly expecting success is much higher than the surprise from observing success. The strong-expectation prior can be contrasted with the maximally uncertain prior, $P(A) = .5$: the posteriors will change by +.45 and -.40, for 'success' and 'fail' outcomes, respectively. In this case, the update is larger and relatively less asymmetric. Such dynamics are not captured by point-estimate heuristic methods. 

% When external binary feedback is not available
In the simple example above, the agent attends only to binary feedback to update its beliefs. While performance feedback affects subjective confidence judgments \cite{marti_certainty_2018,rouault_forming_2019}, other factors relating to task performance might be at play, especially when reliable external feedback is absent or very sparse \cite[e.g.,][]{rouault_forming_2019,holm_episodic_2019,locke_performance_2020}. For instance, when trying to answer a question, one might consider the utility of self-generated candidate answers \cite{coenen_asking_2019} in order to gauge how close one is to answering. In a visuomotor task of tracking the center of a dot-cloud \cite{locke_performance_2020}, participants monitored the distance between the invisible target and the cursor to make judgments about their performance. Such continuous evaluations are useful for assessing one's progress when external feedback is not available. This implies that feelings of progress may be supported not only by monitoring the success rate but also the proximity to success.

% A note on IM RL
The problem of lacking reliable feedback is at the heart of intrinsically motivated machine learning \cite{oudeyer_computational_2018,linke_adapting_2020} where the proposed solution is to provide the agent with intrinsic reward functions that support learning in the absence of primary rewards. Such intrinsic reward functions, however, are usually characterized as task-independent and are intended to enhance the agent's competence in a general way. Evaluation of the proximity to task achievement discussed here, on the other hand, is tied to the task itself.

% A sketch for a competence proxy computation
The nature of information that contributes to task-achievement proximity depends on the task and how agents represent achievement (or goals). To illustrate, consider the following example. We know that a skillful bow-and-arrow shooter is one that tends to hit the target within the bullseye region. Before ever hitting the sweet spot, a practicing shooter might consider the distance to the target's center, $d$, in order to evaluate her ability (for simplicity, suppose that $d$ represents signed horizontal distance to the center). The learner will expect some variability in the outcomes of her attempts, e.g., $d \sim \mathcal{N}(0, \sigma)$, where the value of the variance parameter $\sigma$ is unknown a priori. Suppose that the learner can (1) represent a hypothesis about what skillful performance should be in terms of the spread of outcomes, and (2) infer her own level of performance from new observations and prior knowledge. Concretely, we can assume that skillful performance is represented by a very small amount of variability around the bullseye, i.e., $\sigma=\sigma^*$. As the learner tries to accomplish the task, she observes where she hits (or misses) the target and updates her uncertain belief about how varied her attempts tend to be. To model this belief update, we can use the Bayes rule:
\begin{equation}
    p(\sigma|d) = \frac{P(d|\sigma)p(\sigma)}{\int_{0}^{+\infty}P(d|\sigma)p(\sigma)d \sigma}
\end{equation}
where $p(\sigma)$ is the prior distribution over $\sigma$ (e.g., a conjugate inverse-gamma prior) and the likelihood:
\begin{equation}
    P(d|\sigma) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{d^2}{2\sigma^2}}
\end{equation}
After any attempt, the learner can compare the target performance ($\sigma^*$) with her actual performance. This can be achieved in several ways. For example, a summary statistic derived from the posterior (e.g., the maximum a posteriori estimate or the expected value of $\sigma$) can be compared directly with $\sigma^*$. However, in order to account for belief uncertainty, we can model the estimation of goal-proximity ($\mathrm{GP}$) as Bayesian model comparison:
\begin{equation}
   \mathrm{GP} = \log \frac{P(H_1|d)}{P(H_0|d)} = \log \frac{P(d|H_1)}{P(d|H_0)} + \log \frac{P(H_0)}{P(H_1)}
\end{equation}
where $H_1$ represents a "goal-achievement hypothesis" that $\sigma = \sigma^*$ and $H_0$ represents an "alternative hypothesis" that $\sigma$ is uncertain but can be sampled from the posterior distribution $p(\sigma|d)$. Improving performance according to the $\sigma^*$-criterion should drive this posterior log odds towards zero. As with the feedback-based example above, this way of modeling proxy-competence takes into account the prior knowledge over the hypotheses: if the learner hits the bullseye while expecting a large spread of outcomes, she might discredit it as a lucky fluke. 

% Comparison with heuristic methods
To compute \ac{LP}, the learner would need to compare confidence or (proxy-)competence estimations at different points in time, e.g.:
\begin{equation}
   \mathrm{LP}_t = \mathrm{GP}_{t} - \mathrm{GP}_{t-\tau}
\end{equation}
where $\mathrm{GP}_t$ is the goal-proximity measure at time $t$ based on the data and prior knowledge at that point in time.

\subsection{Introspective approaches}\label{subsec:introspective_approaches}

Introspective approaches to computing \ac{LP} are based on the model of the learning process of the task module. In contrast to the performance-based mechanisms, where the meta-module observes consequences of the task module's behavior, the introspective accounts require specifying how the task module itself adapts to its task demands. How \ac{LP} is computed depends entirely on the learning mechanism\footnote{In this particular instance, the mechanism refers to any learning rule that determines how the task module's behavior changes by data. Bayesian models of learning typically avoid implementational claims about the underlying mechanisms, yet they describe how the system's knowledge changes (or how it should change).} specified for the task module. Note that for methods reviewed in this section, the time over which \ac{LP} is computed is implicit -- a measurement is performed each time an arbitrary amount of data is processed by the task module.

% Twomey and Westerman, 2018
One example is a connectionist model of infant stimulus categorization from Twomey and Westermann \cite{twomey_curiosity-based_2018}. The model features an autoencoder neural network for compressing full stimulus representations into a relatively small set of features. These compressed feature representations are not given, but have to be learned from observing stimuli -- here, instances of a latent structure. When an autoencoder "understands" the latent structure well, it can encode the full representation into a compact format and then decode it back into the full representation. Latent representations are learned by adjusting connection weights in the network via backward propagation of error -- a mismatch between the encoded and the decoded representation. Based on this architecture, Twomey and Westerman compared several intrinsic-motivation signals that could inform how the network should choose stimuli to learn from. One of the signals was the total amount of weight adaptation in the network. For a single weight connecting an input neuron to an output neuron, the update is given by: 
\begin{equation}
    \Delta w = (i - o)o(1-o)
\end{equation}
where $i$ is the target activation and $o$ is the actual activation of the output neuron. Total weight adaptation can be obtained by summing over the absolute values of individual weight updates. Since knowledge in connectionist systems resides in connection weights, this measure can be regarded as a version of \ac{LP} because weights are adjusted to minimize the network's error (i.e., improve the network's knowledge). Thus: 
\begin{equation}
    \mathrm{LP} = \sum_{w \in \bm{w}} |\Delta w|
\end{equation}
where $\bm{w}$ is a vector of all weights in the network.

% Graves et al., 2017 -- Change to variational complexity gain (a retrospective LP measure)
Another example of an introspective mechanism comes from a study by Graves and colleagues \cite{graves_automated_2017}. The authors investigated the effects of different \ac{LP} measures on automated curriculum learning -- a process by which a meta-module autonomously selects data to train the task module. The task module was a Bayesian neural network with probabilistic weight parameters \cite{blundell_weight_2015}. In contrast to traditional neural networks, Bayesian neural networks feature a multivariate parametric distribution for their connection weights. Instead of optimizing the weights themselves, Bayesian neural networks learn by optimizing distributional parameters. In \cite{graves_automated_2017}, network parameters were optimized with respect to the minimum description length objective \cite[see][]{graves_practical_2011}. Based on these specifications, one version of \ac{LP} -- called \emph{variational complexity gain} -- was defined as a decrease in model complexity:
\begin{equation}
    \mathrm{LP} = D_{\mathrm{KL}}(P_{\phi'}||Q_{\psi'}) - D_{\mathrm{KL}}(P_{\phi}||Q_{\psi})
\end{equation}
where $D_{\mathrm{KL}}(P_{\phi}||Q_{\psi})$ is the Kullback-Leibler divergence between the variational posterior distribution $P_{\phi}$ and the prior distribution $Q_{\psi}$. In the context of variational optimization of the minimum description length objective, the \ac{LP} definition from above is interpreted as model complexity gain, which occur only when data is compressed by a greater amount \cite{graves_automated_2017}. Note that in practice, this measure did not work as well as a less computationally expensive mechanism based on prospective approximations of variational complexity gains.
%-- called \emph{gradient variational complexity gain} -- was defined as a directional derivative of the Kullback-Leibler (KL) divergence (a.k.a. relative entropy) in the direction of gradient descent:
% \begin{equation}
%     \mathrm{LP} = [\nabla_{\phi,\psi} \mathrm{KL}(P_{\phi} || Q_{\psi})]^\top \nabla_{\phi} \displaystyle \mathop{\mathbb{E}}_{\theta \sim P_{\phi}} L(x, \theta)
% \end{equation}
% Details aside, this form of \ac{LP} can be interpreted as a Bayesian-neural-network a loss-reduction measure in a traditional neural network. It approximates how much the $\mathrm{KL}$ divergence between the (variational) posterior and the prior weight distributions would change if they were adjusted to optimize the likelihood of the data \cite[see][]{graves_automated_2017}.

% Poli et al., 2020
Poli et al. \cite{poli_infants_2020} studied a probabilistic model of infant attention in a task where a visual cue stochastically appeared in one of four locations according to a fixed probability distribution. Infants could learn this distribution by observing multiple trials of cue presentation, and looked away once the distribution was learned. The authors modeled the task module as a probabilistic predictor of the cue location that optimally changed its predictions on each bout of cue presentation via sequential Bayesian inference. They defined \ac{LP} as the KL divergence between the prior distribution before cue presentation and the posterior distribution after cue presentation:
\begin{equation}
    \mathrm{LP} = D_{\mathrm{KL}}(p^j||p^{j-1})
\end{equation}
where $p^j$ is the posterior distribution of the parameters determining the prediction on trial $j$. This form of \ac{LP} can be interpreted as the degree to which predictions of the task model change. Implicitly, because of the stationarity of environment and the optimality of Bayesian inference, these changes lead to more accurate predictions. Note, that this \ac{LP} measure seems less introspective compared to the previous two, but I include it in this section because it based solely on the expectations (at different points in time) derived from the task module -- no evaluation of these expectations by the meta-module is necessary. 

\section{Open challenges}

% Performance-based vs introspective metacognition
Given the breadth and the diversity of mechanisms computational mechanisms, which approach can we expect to be good for cognitive modeling of \ac{LP} judgments in humans? While not necessarily incompatible with introspective mechanisms, performance-based approaches seem to present a stronger case for warranting further investigation. People rely on competence metrics and external feedback when they need to evaluate their performance \cite{marti_certainty_2018,desender_subjective_2018,locke_performance_2020}. While self-evaluation without external feedback is possible and often a reality, having feedback makes us more confident in our evaluations \cite{rouault_forming_2019} and is often actively sought out despite being costly \cite{holm_episodic_2019,fitzgibbon_lure_2021}. More fundamentally, some kind of objective measure is essential for learning -- whether it is a performance metric or binary feedback. While introspective mechanisms do not rely on such measures explicitly for computing \ac{LP}, they are based on learning achieved through by optimizing an objective. Objectives are explicit in machine learning models, but even Bayesian inference can be viewed as an optimization of an objective (e.g., entropy, surprise, or free energy \cite{friston_free-energy_2009}).

% Selecting performance standards
The discussion of performance-based mechanisms in section \autoref{subsec:performance-based_mechanisms} raises an important question of how do people select task-achievement parameters and set competence standards? None of the models discussed so far specify how to select performance parameters for progress monitoring, yet it is crucial to understand this process if we want to understand how people evaluate their performance and \ac{LP}, when there is no normative feedback. The question is especially poignant in the context of complex tasks that we encounter outside psychology labs and attempt very few times (earning a Ph.D. is a perfect example). Self-evaluation and progress estimation is not necessarily easy \cite{townsend_judgments_2011,raaijmakers_effects_2019}, yet these abilities seem crucial for self-regulated learning, particularly at a young age \cite{oudeyer_computational_2018}. Understanding how representations of normative competence judgments form could help us understand the mismatch between the theorized importance and the apparent difficulty of accurate self-assessment. Considering the potentially idiosyncratic nature of self-assessment across individuals and situations \cite{boekaerts_subjective_1991} it is important to account for a variety of factors for predicting \ac{LP} judgments. For example, Townsend and Heit \cite{townsend_judgments_2011} report a stronger correlation between improvement judgments and differences in subjective judgments of learning, compared to differences in objective performance errors.

% Time extent of progress judgments (ADD BIAS VARIANCE FIGURE)
Another question concerns the temporal extent of progress judgments. We have seen examples where \ac{LP} on a given task is computed across a fixed window of time. Understanding the determinants of this time window is important for being able to model verbal reports (or other behavioral manifestations) of \ac{LP}. To illustrate the problem more precisely, consider the process of learning a complex skill. As discussed in the introduction, one might expect to improve based on many considerations, including a retrospective judgment of progress -- a comparison between the present level of performance and a reference point in the past. How far back does the reference point go? Setting the reference point too far back may bias the progress estimate: it will signal positive progress even if performance stagnates. Setting the reference point too close to the current estimate may produce a noisy and unreliable 
\ac{LP} signal. 
%This situation is similar to the bias-variance tradeoff in statistics and \ac{ML}. While humans might prefer high-bias and low-variance estimates when it come to predicting uncertain quantities \cite{gigerenzer_heuristics_2011}, a more precise specification would be useful.

% Flexible referencing
Fixed time-window computation might be too restrictive to account for the diversity of learning trajectories across different tasks. Shorter time windows are more useful for easier tasks where learning progresses rapidly, while longer time windows are more appropriate for more slowly developing skills. Fixed time-window computation also requires ad hoc assumptions to handle situations in which the reference point extends beyond what is available. For example, given a time window of size $\tau$, the learner would require at least $\tau$ performance evaluations to compute \ac{LP}, unless the parameter is allowed to vary in the beginning. A more flexible approach would be to reset the reference point to when the task is switched to. This would turn the relationship between \ac{LP} and its temporal extent upside down: instead of \ac{LP} depending on the fixed time window, the temporal extent of \ac{LP} judgments would depend on the rate of learning (assuming that low \ac{LP} signals the need to disengage from the current task). This computational approach is in line with psychological metacognition literature, namely the \emph{Region of Proximal Learning} theory \cite{metcalfe_region_2005}. This theory proposes that once a task is chosen, the amount of time spent on the task will depend on \ac{LP}, here defined as the contrast between the competence judgment at the beginning of task engagement and the dynamic judgment of competence that changes over the course of task engagement.

% Summary
In summary, I propose to assume that in commonly encountered tasks devoid of clear performance feedback, \ac{LP} computation is based on performance data that learners decide to monitor. Such data may not correspond to our preconceived objective criteria, so it needs to be validated empirically. What measures learners consider is only one piece of a puzzle. To understand \ac{LP} computation more fully, we also need to understand what they compare the monitored measures to. The next section describes a behavioral study for addressing these issues.

\section{Empirical study of improvement judgments}
To study 

% How can learning dynamics be evaluated objectively? To get started, consider an analogy with classical mechanics. In classical mechanics, objects are characterized by their position measured in some coordinate system (which can be 2D or 3D, for example). Object position provides an instantaneous description -- a snapshot -- of the object. Velocity is the first-order temporal derivative of position and it describes how position changes per an infinitesimally small unit of time. 

% Analogously, we can think of a subject's instantaneous level of competence that varies with time. Instantaneous competence provides a snapshot of one's ability while the temporal derivative of instantaneous competence describes how it evolves over an arbitrarily small unit of time. If $x = f(t)$, then $f'(t) = \frac{dx}{dt}$. In words, if competence ($x$) is a function of time ($t$), then the temporal derivative of competence is the rate of change in competence over an arbitrarily small unit of time. 

% :::info
% Note that it is not necessary to know the mathematical form of $f(t)$ in order to get $f'(t)$ if all we care about is the numeric value of the derivative at $t$. Knowing the precise functional form is only necessary if the one wants to obtain the true derivative $\frac{dx}{dt}$, where $dt$ is an infinitesimally small. It is unlikely that humans track such minute temporal changes. Numerical approximation of this quantity for an arbitrary differential $\bar{dt}$ is simple: one just needs to compare the value of $x$ at time $t$ with $x$ at $t - dt$.
% :::

% The computation of a temporal derivative is essentially symbolic. That is, to carry it out, the computer (that who/which computes) must represent the dynamic quantity (e.g., competence) symbolically as time-series. In numerical approximation of the temporal derivative, the time differential itself has a symbolic representation. At this point, we can assume that our minds indeed perform such symbolic computations in order to produce JOLDs. This would mean that objective evaluation of progress is no different to the actual cognitive process, except that the latter might be carried out implicitly (i.e., unconsciously). Let's put this possibility aside and move on to another one, which, I feel, is better connected to other cognitive theories of learning (which is desirable in science, right?).

% ### Progress as a meta-learning process
% It is possible to imagine a mechanism that computes competence changes without any explicit symbolic representations. Knowledge gains can be indexed by the very change of the underlying sensorimotor learning system itself, as opposed to explicit computation and comparison of competence levels at different points in time. Different levels of change in the sensorimotor learning system might correspond to distinct neurochemical (or backpropagational) states that might represent progress for other cognitive systems downstream, e.g., for the linguistic system to generate verbal report and for the executive systems to guide attention/task engagement.

% To illustrate, imagine how a simplified sensorimotor system might learn from prediction errors (see Figma diagram below, which is inspired by [Reinforcement Learning](http://incompleteideas.net/book/the-book.html) and [Control Theory](https://www.researchgate.net/publication/2431469_Computational_Aspects_of_Motor_Control_and_Motor_Learning)). The process begins with a selected goal, e.g., to land the lander. Upon perceiving sensory context coming from the environment at time $t$, the system equipped with prior knowledge (internal inverse model) takes a series of actions to reach the goal. An efferent copy of the action(s) enters the internal forward model that predicts outcomes of the action(s) taken. The performed action(s) creates the next environmental state (at time $t+1$) which is picked up by the system as action(s) effect. This effect can then be used to update the system's two internal models:
% 1. The forward model is updated by a contrast between the sensed state (the effect) and the predicted state. Learning results in better predictions of the next states from actions and contexts.
% 2. The inverse model is updated by a contrast between the sensed (affected) state and the goal. Learning results is choosing better the actions to achieve selected goals.

% {%figma https://www.figma.com/file/SoF0Lhp7fmSZdWxCYsWvzR/Full-learning-loop?node-id=0%3A1 %}

% This highly simplified example of a sensorimotor learning system illustrates that the learning-progress signal does not need to depend on a symbolic computation of competence derivative (although it could, as I discuss below). Instead, the amount of learning progress scales with how much the subjective internal models themselves change. Note that there might be multiple sources of progress, if the existence of separable forward and inverse models is assumed.

% ### Progress as changing beliefs
% Another possible mechanism that allows to track learning progress without relying on symbolic differentiation involves explicit beliefs about the system's own abilities (collectively called "self-efficacy"). Here, we assume that the cognitive system features a so-called self-efficacy model that, given a goal, can output the subjective likelihood of achieving it. The goal-related outcome, representation of which is derived from sensed effects, can then be used to update the prior beliefs about goal-achievement capabilities of the agent. The amount by which this belief changes can also, in principle, inform the system's competence progress.

% It should be noted that "goals" are underspecified in this discussion. It would be great to have a productive conversation with anyone willing about how goals might be represented in our task. For instance, goals can be represented as categorical criterion (*landed* vs *not landed*) or by a continuous one (average distance to platform). A goal can be even more abstract than that: e.g., *to control the lander well enough to land it at will*.

% ### Critical review of tentative mechanisms

% In summary, I've discussed 3 ways to compute learning progress:
% 1. **Meta-learning**, where learning progress corresponds to the size of the update of an internal model (or some index of the size of the update).
% 2. **Belief-updating**, where learning progress corresponds to the size of the update of a self-efficacy model.
% 3. **Symbolic differentiation**, where learning progress corresponds to the result of symbolic differentiation of an arbitrary quantity, $x$.

% Next, I provide some "armchair" justifications for why it is useful study the belief-updating approach (as we will do in the Lunar Lander experiment) first.

% #### Meta-learning vs Belief-updating

% Mechanisms based on meta-learning and belief-updating do not seem incompatible with each other. The forward and inverse "meta-learning" and the "belief-updating" processes all compute different kinds of learning progress. Good internal-models constitute knowledge of either what happens following one's actions (forward model) or what actions to take in order to achieve a goal (inverse model), while a good self-model constitutes accurate knowledge of one's task-specific aptitude. Improvement of each of these models, then, is of a different kind.

% Knowing *whether* one can achieve something is not necessarily the same as knowing *how* to achieve it[^7]. There are cases where people's explicit beliefs about their abilities and their actual abilities diverge. One prominent example comes from a neuropsychological subject, known as [HM](https://en.wikipedia.org/wiki/Henry_Molaison), who lost his ability to form new declarative memories after a bilateral medial temporal lobectomy. HM, however, was able to retain a visuomotor skill that he acquired post-surgery ([Squire, 2009](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2649674/)). While HM's internal sensorimotor model clearly improved over time, he never remembered ever performing the task in the past ([Milner, 1965](https://www.semanticscholar.org/paper/Visually-guided-maze-learning-in-man%3A-effects-of-Milner/66a3ce58895d8ddab293e2412eb7616555f867f7)). Presumably, he never knew *whether* he could perform the visuomotor task, yet he knew (implicitly) *how* to do it. 

% Follow up studies of HM's learning abilities showed that he could adapt his behavior in multiple other contexts ([Corkin, 2002](https://www.nature.com/articles/nrn726)), suggesting that human memory is comprised of multiple distinct subsystems. Although a few learning mechanisms are common to many of these subsystems, some of them involve idiosyncratic neurochemical processes ([Bisaz et al., 2014](https://www.researchgate.net/publication/266746974_The_Neurobiological_Bases_of_Memory_Formation_From_Physiological_Conditions_to_Psychopathology)). This taxonomy of learning systems is based on types of memory ([Squire & Zola, 1996](https://www.pnas.org/content/93/24/13515)) and it can be distinguished form a functional distinction between two Complementary Learning Systems (CLS, [Kumaran et al., 2016](https://www.psychologie.uzh.ch/dam/jcr:83bd242c-50a5-45df-919e-3c134ff6979e/Kumaran.TiCS.2016.pdf)). 

% In light of this complexity, it seems unlikely that the brain has evolved several specific metacognitive mechanisms to monitor each learning system within the brain. Belief updating provides a general mechanism for evaluating one's knowledge. Since the goodness of knowledge can be assessed by specific tasks (i.e., tests), beliefs about the relevant goal-achievement capabilities can serve to track the quality of knowledge in a general way. Although the possibility of one or more specific metacognitive mechanisms is not entirely zero, this reasoning provides a reason for investigating mechanism(s) based on belief-updating.

% #### Belief-updating vs symbolic differentiation

% Next, I want to contrast the computation of learning dynamics through symbolic differentiation with the belief-updating mechanism. Unlike in the symbolic differentiation approach, computation of progress in the belief-updating process is implicit (not explicit). Here, progress is a by-product of the evolving self-efficacy beliefs.

% I already discussed another distinction between these mechanisms: JOLDs borne out of belief-updating do not rely on a time-differential. Therefore, a symbolic differentiation account of progress computation makes an ontological claim regarding the existence and involvement of a temporal differential, which has to be rationally justified and empirically validated. This is another reason why I think we can narrow down the scope of our study to focus on the belief-updating approach.

% Nevertheless, some version of what we might call symbolic differentiation is arguably present in human brains, but I believe it is not an integral part of the motivational system that guides task selection and task engagement universally. Symbolic differentiation is likely to be a part of a more general quantitative reasoning system capable of comparing arbitrary quantities. People may accurately answer questions about their JOLDs concerning different time windows by reasoning about their evolving ability[^9] *even if* other special-purpose, progress-evaluating mechanisms are implemented. And even though it might in fact influence one's task engagement, I don't think young children and even adults habitually engage in such quantitative reasoning practices.

% ::: info
% In summary, let me reiterate that I think that we should focus on the belief-updating mechanism in our study. Here is why:
% - There is an established tradition to model belief updating as a computation of the posterior distribution by Bayes rule.
% - Bayesian surprise operationalized as KL divergence has been shown to attract human attention ([Itti & Baldi, 2009](https://www.sciencedirect.com/science/article/pii/S0042698908004380); [Poli et al., 2020](https://advances.sciencemag.org/content/6/39/eabb5053)).
% - An investigation of mechanisms based on internal-model learning is beyond the scope of our study, because of the following reasons:
%     - We don't have a way to measure the learning signal of internal models.
%     - We don't have a good idea of which of many learning systems in the brain the meta-learning system is likely to monitor.
% - Symbolic differentiation is likely to be a general and acquired skill of quantitative reasoning, so does not seem to be an integral part of the motivational system (although it might influence motivation).
% :::

% ## Computational bases of subjective competence

% **We do not have a definitive idea of the nature of subjective beliefs about one's competence**.

% Luckily there is helpful work that provides some clues. [Mart√≠ et al. (2018)](https://www.researchgate.net/publication/327067047_Certainty_Is_Primarily_Determined_by_Past_Performance_During_Concept_Learning) found that confidence on a categorization task depends on response accuracy. Participants learned the meaning of a novel concept by trying to classify randomly drawn stimuli as positive or negative examples of that concept. After each guess, they received binary feedback from the experimenters. The authors found that self-reported confidence in understanding the concept (and thus, in providing correct categorization responses) depends on the amount of positive feedback received in the 5 most recent guessing attempts. From the conclusions of this study, we can predict that **subjective competence depends on a binary goal-achievement signal**. The more one succeeds in accomplishing the task -- that is, in landing the lander -- the more one feels competent.

% One problem with this conceptualization is that so long as the player fails to land, he or she is not expected to experience any changes in subjective confidence. However, people can believe that they are getting closer to landing the lander even without ever landing it. What signal might people use to evaluate their competence in the absence of positive reinforcement brought by goal achievement?

% [Locke et al. (2020)](https://shannonlocke.github.io/articles/Locke2020_sensorimotorConf.pdf) studied this question using a sensorimotor task. Specifically, they were interested in what determines humans' explicit subjective confidence judgments on a cloud-tracking task that did not provide binary feedback. The authors present evidence supporting the idea that instantaneous sensorimotor confidence[^2] depends at least partially on goal-based performance monitoring, operationalized as continuous tracking error. This was evident in the association between binary subjective confidence judgments and the temporally proximal tracking error levels.

% Notably, they did not rule out the possibility that other components, such as perceptual certainty and motor awareness, might also contribute to sensorimotor confidence[^3]. The effects of perceptual certainty, motor awareness, and performance error should all be random in our study, since we did not manipulate any these variables. **However, we are specifically interested in whether temporal derivatives of ++performance errors++ are correlated with JOLD reports**. Should we find no association, we will probably need to somehow control for the perceptual and motor factors in order to see the effect of performance error rates. On the other hand, should we find an association between errors and judgments, we can affirm that either (1) the motor and perceptual effects are not present or (2) that the effect of errors is strong enough to be detected despite the noise of the other two factors.

% We can check empirically if performance errors similar to those in Locke et al. (2020) are related to competence judgments collected through one of the NASA-TLX items. We can even go a step further and try to replicate the recency effect reported in their study by which recent performance errors predict competence judgments better than older errors.

% ::: success
% **Hypothesis** : Self-reports of competence (NASA-TLX) are negatively correlated with performance errors.
% :::

% It is easier to think of positive relationships, so instead of performance error, we will use the game score which is a direct opposite of error. To calculate the score, we will first derive a measure of average weighted distance (WAD) to the platform. This is simply the average across game frames of distances between the lander and the platform, where we weight each frame linearly, so that the weight at the end of the trial is 1 and the weight at the beginning of the trial is 0. 

% When WAD is low, it means that for most of the episode the lander was close to the target. When WAD is high, it means that the lander did not spend a lot of time near the target. This measure is similar to what Locke et al. (2020) used, but we shall tweak it a little in order to account for the trial outcome. High values of WAD are good if the player lands successfully because it means that the landing took little time. However, if one fails to land, lower WAD is better than higher WAD because lower values indicate that the lander was closer to the target on average (e.g., one was close to landing but failed at the end).

% Thus, we can operationalize performance score, $s$, as follows:

% $$s = \left\{ 
%   \begin{array}{ c l }
%     \textsf{WAD} & \quad \textrm{if landed} =1 \\
%     -\textsf{WAD} & \quad \textrm{otherwise}
%   \end{array}
% \right.$$

% This score combines both the binary outcome and the outcome-related continuous variable[^10]. We can relate it to the probabilistic confidence variable using the logistic function. Thus, competence, $c$, can be expressed as:

% $$
% c = \frac{1}{1+e^{-s}}
% $$

% Each trial, the player received new data which can be used to updated the belief about his or her competence:

% $$
% c = P(\mathrm{land=1} \mid D) \propto P(D \mid \theta) * P(\theta)
% $$

% <!--  
% ==**Control**== (interesting idea, but we cannot address it in the pilot study).
% ::: spoiler
% *Presumably*, positive competence judgments can be generated even when empirical goal-achievement rate is 0. For example, one can feel more competent than someone else in landing the lander even without ever landing it. One can even feel more competent that the next person without minimizing the error. These possibilities are not captured by achievement-based and error-based competence.

% It might be possible that **subjective competence judgments track the ability to *control* the state of the system**. We can base a measure of control on the concept of *empowerment*, which originates in AI ([Salge et al., 2012](https://arxiv.org/abs/1310.1863)). If we measure entire sequences of people's actions and lander trajectories, we can define control formally as the mutual information between the distributions of actions and sensory effects. We might discover that subjective competence depends on feelings of control.
% :::



% "Reconceptualizing novelty, complexity, and ambiguity in terms of metamemory rather than stimulus features has broken new ground in the study of curiosity‚Äôs determinants (e.g., Metcalfe et al., 2017), and helped guide new work on the underlying reward mechanisms involved in curiosity based on the methods of contemporary affective neuroscience (e.g., Gruber et al., 2014)" (Litman, 2019, p. 433).



