%************************************************
\chapter{Metacognition}\label{ch:metacognition}
%************************************************

% Intro
\begin{flushright}{\slshape
    The world is my idea. \\
    --- A. Shopenhauer \cite{schopenhauer_world_2016}}
\end{flushright}
As many insightful people before him, Shopenhauer recognized that we all see the world through our own personal lenses of perception. His original words "Die Welt ist meine Vorstellung" are sometimes translated as "the world is my \emph{representation}", which should sound more familiar to a modern-day cognitive scientist. Indeed, behaviors we observe in humans and other animals are caused by idiosyncratic chains of neurophysiological events assumed to implement functional computations from certain inputs to certain outputs. Intermediate stages of these computations feature interactions between representations: of external and internal variables, of current goals and needs, and of multiple constraints. In this chapter, I explore how the representation of \acf{LP} might form and manifest inside the human mind. 

% Quick recap of evidence for learning progress
Previously, I discussed the idea that information-seeking is motivated by expected improvement of knowledge or \ac{LP}. A few studies lend empirical support for this theory by measuring various task features and examining their relationships with task engagement. For example, Gerken et al. \cite{gerken_infants_2011} measured objective complexity of experimental stimuli and tested its relationship with attention. Poli et al. \cite{poli_infants_2020} made a step further by proposing several candidate mechanisms processed stimuli of different complexity and examining how their outputs related to looking time. Leonard et al. \cite{leonard_young_2021} manipulated performance feedback to manipulate how children perceived their task progression and observed whether they were willing to carry on. Son and Metcalfe \cite{son_metacognitive_2000} asked university students to report how well they thought they would remember information from different texts and looked at how well these reports predicted time allocation and prioritization of these texts. In our own study \cite{ten_humans_2021}, we manipulated the complexity of categorization rules of different stimulus families and measured people's ongoing performance to understand its relation to self-directed learning. While these studies suggest different ways to measure \ac{LP}, they do not make explicit claims about algorithmic implementation of the underlying computation.

% Prospective vs retrospective judgments
It is important to note that the studies reviewed above are concerned with how \emph{prospective} \ac{LP} judgments influence motivation. How such \ac{LP} expectations arise is beyond the scope of this chapter, because there are probably many variables that jointly determine people's expectations about their future learning and achievement. A relevant framework for exploring the mechanisms of prospective \ac{LP} expectations is Bandura's theory of \emph{self-efficacy} \cite{bandura_self-efficacy_1977}. Bandura proposed several qualitatively different sources that contribute to one's belief about being able to perform a task, including (1) performance accomplishments, (2) vicarious experiences, (3) verbal persuasion, and (4) emotional arousal. The present chapter is concerned with a single part of this belief-formation process: the subjective evaluation of one's learning/performance dynamics. 

% Motivation and goals
What information does the human brain access and represent in order to compute the current estimate of \ac{LP} on a given task? How does it access and combine this information? The relevant work suggests several possibilities. One possibility is that the brain performs some kind of explicit differentiation of a relevant performance metric, such as numeric differentiation or regression to the mean. Another possibility is that it maintains competence beliefs and tracks the extent to which performance outcomes change these beliefs. Yet another possibility is that the brain has a private access to its internal world models which it uses to track their evolution.

% Task module and meta-module
All of these potential mechanisms assume an interaction between two modules (systems with different inputs/outputs and functions). One module -- let's call it the \emph{task module} -- is a mechanism that learns to perform a task at hand. It serves to convert sensory/mnemonic inputs into response outputs (e.g., motor action, categorization, retrieval, etc.). The second module -- the \emph{meta-module} -- serves to evaluate the task module in order to inform decisions pertaining to active learning. The next section provides a more detailed analysis of these possibilities. 

\section{Mechanisms of progress computation}

\subsection{Heuristic approaches}

% Schmidhuber (1991)
A commonly used approach to estimating \ac{LP} in \ac{AI}, is to compute a temporal derivative of a metric trajectory of the task module. Computational architectures of intrinsically-motivated exploration provide numerous examples of how such computations can be carried out and what information needs to be represented to support them. An early algorithm by Schmidhuber \cite{schmidhuber_curious_1991} estimates \ac{LP} as:
\begin{equation}
    \mathrm{LP(t)} = o_C(t) - o_C'(t)
\end{equation}
where $o_C(t)$ is an estimated reliability of the task module -- or the "confidence" at time $t$ of the meta-module in the competence of the task module; the term $o_C'(t)$ denotes the reliability estimate after the meta-module had been adjusted to predict the reliability of the task module more accurately. Here, \ac{LP} is computed by comparing point estimates of subjective confidence before and after updating the meta-module with relevant information. As per Schmidhuber's original proposition, this information can be derived by counting how many times the task in a given context was performed well and how many times the task was attempted in this context.

% Oudeyer, Kaplan, & Hafner (2007)
In a different algorithm by Oudeyer, Kaplan, and Hafner \cite{oudeyer_intrinsic_2007} \ac{LP} is defined as:
\begin{equation}
    \mathrm{LP(t)} = e_R(t) - e_R(t-\tau)
\end{equation}
where $e_n(t)$ is the average prediction error of the task module prior to time $t$; the parameter $\tau$ controls the temporal reference point to which $e_n(t)$ is compared. The original algorithm also parameterizes the computation of the prediction-error averages to control their smoothness. Importantly, \ac{LP} is computed separately for different regions, indexed by $R$, of the sensorimotor space to prevent the agent from "maximizing" progress by alternating between attempting unrelated low- and high-error tasks in an undifferentiated space. Like in Schmidhuber's algorithm, the mean error term $e_n(t)$ can also be construed as the meta-module's confidence in the task module, as it represents the error rate in a given context. 

% Colas et al. (2019), Baranes & Oudeyer (2013)
Similar algorithms have been used to compute competence progress \cite{baranes_active_2013,santucci_which_2013,colas_curious_2019,forestier_intrinsically_2020} -- a temporal derivative of the agent's ability to reach its goals in a specific task space. For instance, Colas et al. \cite{colas_curious_2019} defined \ac{LP} as:
\begin{equation}
    \mathrm{LP(n)} = |c_R(n) - c_R(n-\tau)|
\end{equation}
where $c_R(n)$ is the subjectively estimated competence of the agent in a discrete task space, indexed by $R$; $n$ is the number of self-evaluations performed to estimate the competence score. Subjective competence is evaluated by weighting binary goal-achievement outcomes in a task space by recency and taking the average of the weighted scores\footnote{Colas et al. \cite{colas_curious_2019} used a queue-based implementation, but the effect of the computation is the same as taking a recency-weighted average of a binary vector.}. Competence is computed for all $n$ self-evaluation trials and again for a more recent $n-\tau$ portion of these trials, and the two estimates are compared. Note that here, the absolute value of the derivative is taken. While not essential to the definition of \ac{LP}, this functional form raises the question of whether improvement and deterioration in performance are equivalent for motivation and what their differences might be. In Colas et al. \cite{colas_curious_2019}, taking the absolute value of the competence differential allowed the agent to actively practice tasks on which it was getting worse over time, which ensured that the overall competence was maximized.

% Summary
The approaches to computing \ac{LP} surveyed above rely on the same basic operation -- an approximation of the temporal derivative of a performance metric through finite difference. Minimally, this computation requires representing the given metric at two different time points and a mechanism for their comparison. However, as the examples above are intended to demonstrate, there are many details that have to be assumed or validated. The obvious one is what metric(s) might people use to compute progress. Another one is the extent of temporal comparison: what different time points do people consider in order to compute \ac{LP}. These degrees of freedom can be regarded as both desirable and challenging. On the one hand, they make temporal-differentiation models of \ac{LP} flexible in accounting for diverse and complex behaviors, On the other hand, they raise a need for further explanation for why particular configurations would explain behavioral data.

\subsection{Probabilistic approaches}

% The significance of uncertainty
In the heuristic approaches discussed above, the represented metric of performance can be thought of as reflecting the agent's subjective belief about its performance. Thus, changes in subjective beliefs might \ac{LP} computation. However, the approaches surveyed above rely on point-estimate representations that do not account for belief uncertainty. On the other hand, psychological literature suggests that declarative statements (e.g., "I can play the piano") emerge from supporting representations of varying degrees of internal consistency \cite{smith_belief_1991,koriat_self-consistency_2012}. More recently, research in neuroscience has started to unravel the neural mechanisms underlying uncertainty and confidence judgments. The so-called distributional uncertainty inherent in neural activity can potentially explain how the brain computes and represents propositional confidence \cite{meyniel_confidence_2015,pouget_confidence_2016}. If the computation of \ac{LP} involves belief comparison, then belief uncertainty should have a considerable footprint on the process. 

% Bayesian beliefs, uncertainty and confidence
A suitable tool for studying dynamic uncertain beliefs is the Bayesian framework for cognitive modeling, which assumes that humans represent beliefs probabilistically \cite{sun_bayesian_2008,perfors_tutorial_2011,coenen_asking_2019}. Sensitivity to uncertainty enables agents to intelligently switch between exploration and exploitation \cite{cohen_should_2007} and control the learning of new information \cite{meyniel_confidence_2015}. In addition to uncertainty inherent in belief distributions, individual beliefs (that a particular decision or proposition is true) are associated with a distinct kind of uncertainty that manifests in confidence judgments \cite{pouget_confidence_2016}. Confidence in beliefs is often sufficient to support decisions and might be a simplified computational substitute for overly complex belief posteriors \cite{pouget_confidence_2016}.  

% Formal description
Probabilistic competence beliefs can be characterized by more or less confidence and change as a result of performance monitoring. Formally, these properties can be expressed in terms of posterior probability. For example, suppose that an agent represents a state space, a subset of which is a state-achievement event $A$ that has a probability of occurring $P(A)$. This probability can be framed as the subjective belief that some target state-achievement event can be reached by the agent. Since, $P(A)$ is unknown a priori, it has to be inferred from some relevant observations:
\begin{equation}
    P(A|D) = \frac{P(D|A)}{P(D|A)P(A) + P(D|A^c)P(A^c)} \times P(A)
\end{equation}
where $A^c$ denotes the complement of $A$, and $D$ is the observed data. This equation prescribes an optimal way to update a (binary) state-achievement belief by combining the prior expectation $P(A)$ with the normalized likelihood of that belief $P(D|A)$. Both of these components reflect the agent's uncertain knowledge about its abilities to predict the future or reach specific goal states.

% Prior knowledge
The strength of prior confidence $P(A)$ biases how data influences the posterior. Imagine that while estimating confidence, all that the agent observes is external binary feedback on some goal-achievement task. Consider the following priors and likelihood values:
\begin{center}
\begin{tabular}{c c c c}
Belief & $P(A)$ & $P(D=\mathrm{success}|A)$ & $P(D=\mathrm{fail}|A)$ \\ 
$A$    & .99    & .90                       & .10        \\  
$A^c$  & .01    & .05                       & .95         
\end{tabular}
\end{center}
The posterior from a 'success' outcome will be .9994 (an increase of .0094), while a 'fail' outcome will give us a posterior of .9124 (a decrease of .0776). Thus, high prior confidence results in asymmetric updates for different outcomes. Now consider a maximally uncertain prior, $P(A) = .5$: the posteriors will change by +.45 and -.40, for 'success' and 'fail' outcomes, respectively. In this case, the update is larger and relatively less asymmetric. Such dynamics are not captured by point-estimate heuristic methods. 

% Elaborating on the model
In the simple example above, the agent attends only to binary feedback to update its beliefs. While performance feedback affects subjective confidence judgments \cite{marti_certainty_2018,rouault_forming_2019}, other factors relating to task performance might be at play, especially when reliable external feedback is absent \cite[e.g.,][]{rouault_forming_2019,holm_episodic_2019,locke_performance_2020}. For instance, one's sense of competence in performing a pull-up might grow by raising oneself higher and higher across multiple attempts, without ever reaching the bar. This implies that feelings of progress may be supported not only by task-achievement events but also by monitoring proximity to the task.

% Hypotheses about continuous variables and likelihood ratio as LP
The nature of information that contributes to task-proximity depends on how the agent represents the task. When trying to answer a question, one might consider the utility of self-generated candidate answers \cite{coenen_asking_2019}; when trying to shoot a target, one might attend to the distance between the target and the reticle \cite{locke_performance_2020}. Such evaluations are useful for assessing a proxy measure for one's competence before any task-achievement information (e.g., success feedback) is available.

% A sketch for a competence proxy computation
Suppose the learner has an idea of what good task performance looks like. For example, we know that a skillful bow-and-arrow shooter is one that tends to hit the target close to the center. Performance, in this case, can be indexed by the distance to the target's center $d$. Assuming that the learner expects some variation in performance, competence can be represented by a probability distribution over $d$, e.g., $d \sim \mathcal{N}(\mu, \sigma)$, and, thus, summarized by distribution parameters $\mu$ and $\sigma$. Expert competence, in turn, can be represented by particular values of these parameters, e.g., $\mu^*$ and $\sigma^*$. In order to track one's proximity to this expert-level competence, one needs to compare it with the actual performance parameters, $\mu$ and $\sigma$, inferred from the observed data. It is appropriate to assume that these parameters are not represented as fixed point estimates but as uncertain quantities. This assumption is intuitive, since a novice bow-and-arrow shooter might not have any specific guesses of how far, on average, her shots will be. Instead, she might hold prior expectations, $p(\mu,\sigma)$, and update them in light of new evidence $d$:
\begin{equation}
    p(\mu,\sigma|d) = \frac{P(d|\mu, \sigma)p(\mu, \sigma)}{\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}P(d|\mu,\sigma)p(\mu,\sigma)d\mu d\sigma}
\end{equation}
The proxy competence, $\bar c$, can be defined as the ratio of two likelihood terms: (1) the likelihood of the data under the "expert competence" hypothesis and (2) the marginal likelihood of the data: 
\begin{equation}
    \bar c = \frac{P(d|\mu^*,\sigma^*)}{P(d)}
\end{equation}
which can be viewed as Bayesian model comparison involving hypotheses of varying complexity \cite{griffiths_bayesian_2008}: the prior distribution $p(\mu, \sigma)$ representing the more complex hypothesis ($H_1$) and parameter values $\mu^*$ and $\sigma^*$ representing a simpler hypothesis ($H_0$). As $H_1$ is more flexible in accounting for different values of $\mu$ and $\sigma$, it is expected to be more likely in situations when the learner is incompetent, since its behavior will produce data that is unlikely under $H_0$. Over the course of learning, the data changes, and so does the posterior, which becomes the new prior for marginalizing $P(d)$. Eventually, the marginal likelihood can approach 

% Support: JROLs in Metcalfe and Kornell -- change in people's judgments of learning predicted task engagement. Therefore, changing beliefs in the form of subjective judgments of being able to retrieve items in the future -- basically, a competence judgment -- corresponded to motivation to engage in a task. Blindsight -- people who think they won't be able to perform a task do not wager on it, even though their internal model of the task is competent. This might reflect a problem with a readout process -- a metacognitive deficiency. If blindisghted people used distributional confidence for wagering, neural activity that supports task behavior would be sufficient for informing the wagers. Learning rate is regulated by confidence -- neuromodulators that facilitate plasticity (e.g., dopamine) are released in response to distributional uncertainty -- i.e., conflict. 

% There is evidence that people keep track of their confidence in sensorimotor tasks (Locke et al.,). People are also motivated to know how they perform (Holm et al.,). People are also more confident when they receive feedback (Fleming Dayan et al.). Beliefs increase self-efficacy (Sharot Bromberg-Martin). 

% Changes in distributional confidence seem fundamental for \ac{LP} computation. 1) They are the basis for symbolic representations of point-estimates which might be unreliable for LP computation (Townsend's research) and 2) they control neurophysiological learning rates. 

% Bayesian models of cognition are rational, computational-level accounts of mechanistic processes; therefore, they make no claims about the representational substrates for beliefs or the mechanistic processes underlying their change \cite{griffiths_probabilistic_2010}. 


% How can learning dynamics be evaluated objectively? To get started, consider an analogy with classical mechanics. In classical mechanics, objects are characterized by their position measured in some coordinate system (which can be 2D or 3D, for example). Object position provides an instantaneous description -- a snapshot -- of the object. Velocity is the first-order temporal derivative of position and it describes how position changes per an infinitesimally small unit of time. 

% Analogously, we can think of a subject's instantaneous level of competence that varies with time. Instantaneous competence provides a snapshot of one's ability while the temporal derivative of instantaneous competence describes how it evolves over an arbitrarily small unit of time. If $x = f(t)$, then $f'(t) = \frac{dx}{dt}$. In words, if competence ($x$) is a function of time ($t$), then the temporal derivative of competence is the rate of change in competence over an arbitrarily small unit of time. 

% :::info
% Note that it is not necessary to know the mathematical form of $f(t)$ in order to get $f'(t)$ if all we care about is the numeric value of the derivative at $t$. Knowing the precise functional form is only necessary if the one wants to obtain the true derivative $\frac{dx}{dt}$, where $dt$ is an infinitesimally small. It is unlikely that humans track such minute temporal changes. Numerical approximation of this quantity for an arbitrary differential $\bar{dt}$ is simple: one just needs to compare the value of $x$ at time $t$ with $x$ at $t - dt$.
% :::

% The computation of a temporal derivative is essentially symbolic. That is, to carry it out, the computer (that who/which computes) must represent the dynamic quantity (e.g., competence) symbolically as time-series. In numerical approximation of the temporal derivative, the time differential itself has a symbolic representation. At this point, we can assume that our minds indeed perform such symbolic computations in order to produce JOLDs. This would mean that objective evaluation of progress is no different to the actual cognitive process, except that the latter might be carried out implicitly (i.e., unconsciously). Let's put this possibility aside and move on to another one, which, I feel, is better connected to other cognitive theories of learning (which is desirable in science, right?).

% ### Progress as a meta-learning process
% It is possible to imagine a mechanism that computes competence changes without any explicit symbolic representations. Knowledge gains can be indexed by the very change of the underlying sensorimotor learning system itself, as opposed to explicit computation and comparison of competence levels at different points in time. Different levels of change in the sensorimotor learning system might correspond to distinct neurochemical (or backpropagational) states that might represent progress for other cognitive systems downstream, e.g., for the linguistic system to generate verbal report and for the executive systems to guide attention/task engagement.

% To illustrate, imagine how a simplified sensorimotor system might learn from prediction errors (see Figma diagram below, which is inspired by [Reinforcement Learning](http://incompleteideas.net/book/the-book.html) and [Control Theory](https://www.researchgate.net/publication/2431469_Computational_Aspects_of_Motor_Control_and_Motor_Learning)). The process begins with a selected goal, e.g., to land the lander. Upon perceiving sensory context coming from the environment at time $t$, the system equipped with prior knowledge (internal inverse model) takes a series of actions to reach the goal. An efferent copy of the action(s) enters the internal forward model that predicts outcomes of the action(s) taken. The performed action(s) creates the next environmental state (at time $t+1$) which is picked up by the system as action(s) effect. This effect can then be used to update the system's two internal models:
% 1. The forward model is updated by a contrast between the sensed state (the effect) and the predicted state. Learning results in better predictions of the next states from actions and contexts.
% 2. The inverse model is updated by a contrast between the sensed (affected) state and the goal. Learning results is choosing better the actions to achieve selected goals.

% {%figma https://www.figma.com/file/SoF0Lhp7fmSZdWxCYsWvzR/Full-learning-loop?node-id=0%3A1 %}

% This highly simplified example of a sensorimotor learning system illustrates that the learning-progress signal does not need to depend on a symbolic computation of competence derivative (although it could, as I discuss below). Instead, the amount of learning progress scales with how much the subjective internal models themselves change. Note that there might be multiple sources of progress, if the existence of separable forward and inverse models is assumed.

% ### Progress as changing beliefs
% Another possible mechanism that allows to track learning progress without relying on symbolic differentiation involves explicit beliefs about the system's own abilities (collectively called "self-efficacy"). Here, we assume that the cognitive system features a so-called self-efficacy model that, given a goal, can output the subjective likelihood of achieving it. The goal-related outcome, representation of which is derived from sensed effects, can then be used to update the prior beliefs about goal-achievement capabilities of the agent. The amount by which this belief changes can also, in principle, inform the system's competence progress.

% It should be noted that "goals" are underspecified in this discussion. It would be great to have a productive conversation with anyone willing about how goals might be represented in our task. For instance, goals can be represented as categorical criterion (*landed* vs *not landed*) or by a continuous one (average distance to platform). A goal can be even more abstract than that: e.g., *to control the lander well enough to land it at will*.

% ### Critical review of tentative mechanisms

% In summary, I've discussed 3 ways to compute learning progress:
% 1. **Meta-learning**, where learning progress corresponds to the size of the update of an internal model (or some index of the size of the update).
% 2. **Belief-updating**, where learning progress corresponds to the size of the update of a self-efficacy model.
% 3. **Symbolic differentiation**, where learning progress corresponds to the result of symbolic differentiation of an arbitrary quantity, $x$.

% Next, I provide some "armchair" justifications for why it is useful study the belief-updating approach (as we will do in the Lunar Lander experiment) first.

% #### Meta-learning vs Belief-updating

% Mechanisms based on meta-learning and belief-updating do not seem incompatible with each other. The forward and inverse "meta-learning" and the "belief-updating" processes all compute different kinds of learning progress. Good internal-models constitute knowledge of either what happens following one's actions (forward model) or what actions to take in order to achieve a goal (inverse model), while a good self-model constitutes accurate knowledge of one's task-specific aptitude. Improvement of each of these models, then, is of a different kind.

% Knowing *whether* one can achieve something is not necessarily the same as knowing *how* to achieve it[^7]. There are cases where people's explicit beliefs about their abilities and their actual abilities diverge. One prominent example comes from a neuropsychological subject, known as [HM](https://en.wikipedia.org/wiki/Henry_Molaison), who lost his ability to form new declarative memories after a bilateral medial temporal lobectomy. HM, however, was able to retain a visuomotor skill that he acquired post-surgery ([Squire, 2009](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2649674/)). While HM's internal sensorimotor model clearly improved over time, he never remembered ever performing the task in the past ([Milner, 1965](https://www.semanticscholar.org/paper/Visually-guided-maze-learning-in-man%3A-effects-of-Milner/66a3ce58895d8ddab293e2412eb7616555f867f7)). Presumably, he never knew *whether* he could perform the visuomotor task, yet he knew (implicitly) *how* to do it. 

% Follow up studies of HM's learning abilities showed that he could adapt his behavior in multiple other contexts ([Corkin, 2002](https://www.nature.com/articles/nrn726)), suggesting that human memory is comprised of multiple distinct subsystems. Although a few learning mechanisms are common to many of these subsystems, some of them involve idiosyncratic neurochemical processes ([Bisaz et al., 2014](https://www.researchgate.net/publication/266746974_The_Neurobiological_Bases_of_Memory_Formation_From_Physiological_Conditions_to_Psychopathology)). This taxonomy of learning systems is based on types of memory ([Squire & Zola, 1996](https://www.pnas.org/content/93/24/13515)) and it can be distinguished form a functional distinction between two Complementary Learning Systems (CLS, [Kumaran et al., 2016](https://www.psychologie.uzh.ch/dam/jcr:83bd242c-50a5-45df-919e-3c134ff6979e/Kumaran.TiCS.2016.pdf)). 

% In light of this complexity, it seems unlikely that the brain has evolved several specific metacognitive mechanisms to monitor each learning system within the brain. Belief updating provides a general mechanism for evaluating one's knowledge. Since the goodness of knowledge can be assessed by specific tasks (i.e., tests), beliefs about the relevant goal-achievement capabilities can serve to track the quality of knowledge in a general way. Although the possibility of one or more specific metacognitive mechanisms is not entirely zero, this reasoning provides a reason for investigating mechanism(s) based on belief-updating.

% #### Belief-updating vs symbolic differentiation

% Next, I want to contrast the computation of learning dynamics through symbolic differentiation with the belief-updating mechanism. Unlike in the symbolic differentiation approach, computation of progress in the belief-updating process is implicit (not explicit). Here, progress is a by-product of the evolving self-efficacy beliefs.

% I already discussed another distinction between these mechanisms: JOLDs borne out of belief-updating do not rely on a time-differential. Therefore, a symbolic differentiation account of progress computation makes an ontological claim regarding the existence and involvement of a temporal differential, which has to be rationally justified and empirically validated. This is another reason why I think we can narrow down the scope of our study to focus on the belief-updating approach.

% Nevertheless, some version of what we might call symbolic differentiation is arguably present in human brains, but I believe it is not an integral part of the motivational system that guides task selection and task engagement universally. Symbolic differentiation is likely to be a part of a more general quantitative reasoning system capable of comparing arbitrary quantities. People may accurately answer questions about their JOLDs concerning different time windows by reasoning about their evolving ability[^9] *even if* other special-purpose, progress-evaluating mechanisms are implemented. And even though it might in fact influence one's task engagement, I don't think young children and even adults habitually engage in such quantitative reasoning practices.

% ::: info
% In summary, let me reiterate that I think that we should focus on the belief-updating mechanism in our study. Here is why:
% - There is an established tradition to model belief updating as a computation of the posterior distribution by Bayes rule.
% - Bayesian surprise operationalized as KL divergence has been shown to attract human attention ([Itti & Baldi, 2009](https://www.sciencedirect.com/science/article/pii/S0042698908004380); [Poli et al., 2020](https://advances.sciencemag.org/content/6/39/eabb5053)).
% - An investigation of mechanisms based on internal-model learning is beyond the scope of our study, because of the following reasons:
%     - We don't have a way to measure the learning signal of internal models.
%     - We don't have a good idea of which of many learning systems in the brain the meta-learning system is likely to monitor.
% - Symbolic differentiation is likely to be a general and acquired skill of quantitative reasoning, so does not seem to be an integral part of the motivational system (although it might influence motivation).
% :::

% ## Computational bases of subjective competence

% **We do not have a definitive idea of the nature of subjective beliefs about one's competence**.

% Luckily there is helpful work that provides some clues. [Martí et al. (2018)](https://www.researchgate.net/publication/327067047_Certainty_Is_Primarily_Determined_by_Past_Performance_During_Concept_Learning) found that confidence on a categorization task depends on response accuracy. Participants learned the meaning of a novel concept by trying to classify randomly drawn stimuli as positive or negative examples of that concept. After each guess, they received binary feedback from the experimenters. The authors found that self-reported confidence in understanding the concept (and thus, in providing correct categorization responses) depends on the amount of positive feedback received in the 5 most recent guessing attempts. From the conclusions of this study, we can predict that **subjective competence depends on a binary goal-achievement signal**. The more one succeeds in accomplishing the task -- that is, in landing the lander -- the more one feels competent.

% One problem with this conceptualization is that so long as the player fails to land, he or she is not expected to experience any changes in subjective confidence. However, people can believe that they are getting closer to landing the lander even without ever landing it. What signal might people use to evaluate their competence in the absence of positive reinforcement brought by goal achievement?

% [Locke et al. (2020)](https://shannonlocke.github.io/articles/Locke2020_sensorimotorConf.pdf) studied this question using a sensorimotor task. Specifically, they were interested in what determines humans' explicit subjective confidence judgments on a cloud-tracking task that did not provide binary feedback. The authors present evidence supporting the idea that instantaneous sensorimotor confidence[^2] depends at least partially on goal-based performance monitoring, operationalized as continuous tracking error. This was evident in the association between binary subjective confidence judgments and the temporally proximal tracking error levels.

% Notably, they did not rule out the possibility that other components, such as perceptual certainty and motor awareness, might also contribute to sensorimotor confidence[^3]. The effects of perceptual certainty, motor awareness, and performance error should all be random in our study, since we did not manipulate any these variables. **However, we are specifically interested in whether temporal derivatives of ++performance errors++ are correlated with JOLD reports**. Should we find no association, we will probably need to somehow control for the perceptual and motor factors in order to see the effect of performance error rates. On the other hand, should we find an association between errors and judgments, we can affirm that either (1) the motor and perceptual effects are not present or (2) that the effect of errors is strong enough to be detected despite the noise of the other two factors.

% We can check empirically if performance errors similar to those in Locke et al. (2020) are related to competence judgments collected through one of the NASA-TLX items. We can even go a step further and try to replicate the recency effect reported in their study by which recent performance errors predict competence judgments better than older errors.

% ::: success
% **Hypothesis** : Self-reports of competence (NASA-TLX) are negatively correlated with performance errors.
% :::

% It is easier to think of positive relationships, so instead of performance error, we will use the game score which is a direct opposite of error. To calculate the score, we will first derive a measure of average weighted distance (WAD) to the platform. This is simply the average across game frames of distances between the lander and the platform, where we weight each frame linearly, so that the weight at the end of the trial is 1 and the weight at the beginning of the trial is 0. 

% When WAD is low, it means that for most of the episode the lander was close to the target. When WAD is high, it means that the lander did not spend a lot of time near the target. This measure is similar to what Locke et al. (2020) used, but we shall tweak it a little in order to account for the trial outcome. High values of WAD are good if the player lands successfully because it means that the landing took little time. However, if one fails to land, lower WAD is better than higher WAD because lower values indicate that the lander was closer to the target on average (e.g., one was close to landing but failed at the end).

% Thus, we can operationalize performance score, $s$, as follows:

% $$s = \left\{ 
%   \begin{array}{ c l }
%     \textsf{WAD} & \quad \textrm{if landed} =1 \\
%     -\textsf{WAD} & \quad \textrm{otherwise}
%   \end{array}
% \right.$$

% This score combines both the binary outcome and the outcome-related continuous variable[^10]. We can relate it to the probabilistic confidence variable using the logistic function. Thus, competence, $c$, can be expressed as:

% $$
% c = \frac{1}{1+e^{-s}}
% $$

% Each trial, the player received new data which can be used to updated the belief about his or her competence:

% $$
% c = P(\mathrm{land=1} \mid D) \propto P(D \mid \theta) * P(\theta)
% $$

% <!--  
% ==**Control**== (interesting idea, but we cannot address it in the pilot study).
% ::: spoiler
% *Presumably*, positive competence judgments can be generated even when empirical goal-achievement rate is 0. For example, one can feel more competent than someone else in landing the lander even without ever landing it. One can even feel more competent that the next person without minimizing the error. These possibilities are not captured by achievement-based and error-based competence.

% It might be possible that **subjective competence judgments track the ability to *control* the state of the system**. We can base a measure of control on the concept of *empowerment*, which originates in AI ([Salge et al., 2012](https://arxiv.org/abs/1310.1863)). If we measure entire sequences of people's actions and lander trajectories, we can define control formally as the mutual information between the distributions of actions and sensory effects. We might discover that subjective competence depends on feelings of control.
% :::



% "Reconceptualizing novelty, complexity, and ambiguity in terms of metamemory rather than stimulus features has broken new ground in the study of curiosity’s determinants (e.g., Metcalfe et al., 2017), and helped guide new work on the underlying reward mechanisms involved in curiosity based on the methods of contemporary affective neuroscience (e.g., Gruber et al., 2014)" (Litman, 2019, p. 433).



