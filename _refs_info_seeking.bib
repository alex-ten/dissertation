@inproceedings{portelas_automatic_2020,
	address = {Yokohama, Japan},
	title = {Automatic {Curriculum} {Learning} {For} {Deep} {RL}: {A} {Short} {Survey}},
	isbn = {978-0-9992411-6-5},
	shorttitle = {Automatic {Curriculum} {Learning} {For} {Deep} {RL}},
	url = {https://www.ijcai.org/proceedings/2020/671},
	doi = {10.24963/ijcai.2020/671},
	abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efﬁciency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Portelas, Rémy and Colas, Cédric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
	month = jul,
	year = {2020},
	pages = {4819--4825},
	file = {Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short.pdf:C\:\\Users\\user\\Zotero\\storage\\6NEUKT6Z\\Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short.pdf:application/pdf},
}

@article{baranes_active_2013,
	title = {Active {Learning} of {Inverse} {Models} with {Intrinsically} {Motivated} {Goal} {Exploration} in {Robots}},
	volume = {61},
	issn = {09218890},
	url = {http://arxiv.org/abs/1301.4862},
	doi = {10.1016/j.robot.2012.05.008},
	abstract = {We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy pa- rameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.},
	number = {1},
	urldate = {2021-11-24},
	journal = {Robotics and Autonomous Systems},
	author = {Baranes, Adrien and Oudeyer, Pierre-Yves},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.4862},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	pages = {49--73},
}

@inproceedings{laversanne-finot_curiosity_2018,
	title = {Curiosity {Driven} {Exploration} of {Learned} {Disentangled} {Goal} {Spaces}},
	url = {https://proceedings.mlr.press/v87/laversanne-finot18a.html},
	abstract = {Intrinsically motivated goal exploration processes enable agents to explore efficiently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. In this paper we show that using a disentangled goal space (i.e. a representation where each latent variable is sensitive to a single degree of freedom) leads to better exploration performances than an entangled one. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment.},
	language = {en},
	urldate = {2021-11-24},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Laversanne-Finot, Adrien and Pere, Alexandre and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {487--504},
}

@article{nguyen_active_2012,
	title = {Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner},
	volume = {3},
	issn = {2081-4836},
	url = {http://arxiv.org/abs/1804.06819},
	doi = {10.2478/s13230-013-0110-z},
	abstract = {We present an active learning architecture that allows a robot to actively learn which data collection strategy is most efficient for acquiring motor skills to achieve multiple outcomes, and generalise over its experience to achieve new outcomes. The robot explores its environment both via interactive learning and goal-babbling. It learns at the same time when, who and what to actively imitate from several available teachers, and learns when not to use social guidance but use active goal-oriented self-exploration. This is formalised in the framework of life-long strategic learning. The proposed architecture, called Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy (SGIM-ACTS), relies on hierarchical active decisions of what and how to learn driven by empirical evaluation of learning progress for each learning strategy. We illustrate with an experiment where a simulated robot learns to control its arm for realising two kinds of different outcomes. It has to choose actively and hierarchically at each learning episode: 1) what to learn: which outcome is most interesting to select as a goal to focus on for goal-directed exploration; 2) how to learn: which data collection strategy to use among self-exploration, mimicry and emulation; 3) once he has decided when and what to imitate by choosing mimicry or emulation, then he has to choose who to imitate, from a set of different teachers. We show that SGIM-ACTS learns significantly more efficiently than using single learning strategies, and coherently selects the best strategy with respect to the chosen outcome, taking advantage of the available teachers (with different levels of skills).},
	number = {3},
	urldate = {2021-11-24},
	journal = {Paladyn, Journal of Behavioral Robotics},
	author = {Nguyen, Sao Mai and Oudeyer, Pierre-Yves},
	month = jan,
	year = {2012},
	note = {arXiv: 1804.06819},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{nguyen_socially_2012,
	address = {San Diego, United States},
	title = {Socially {Guided} {Intrinsically} {Motivated} {Learner}},
	url = {https://hal.archives-ouvertes.fr/hal-00936960},
	doi = {10.1109/DevLrn.2012.6400809},
	abstract = {This paper studies the coupling of two learning strategies: internally guided learning and social interaction. We present Socially Guided Intrinsic Motivation by Demonstration (SGIM-D) and its interactive learner version Socially Guided Intrinsic Motivation with Interactive learning at the Meta level (SGIM-IM), which are algorithms for learning inverse models in high dimensional continuous sensorimotor spaces. After describing the general framework of our algorithms, we illustrate with a fishing experiment.},
	urldate = {2021-11-24},
	booktitle = {{IEEE} {International} {Conference} on {Development} and {Learning}},
	author = {Nguyen, Sao Mai and Oudeyer, Pierre-Yves},
	month = nov,
	year = {2012},
	keywords = {Human Teacher, Imitation Learning, Intrinsic Motivation, Learning by Demonstration, Social Learning},
	pages = {--},
}

@article{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2021-11-24},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published at ICLR 2016},
}

@article{andrychowicz_hindsight_2018,
	title = {Hindsight {Experience} {Replay}},
	url = {http://arxiv.org/abs/1707.01495},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	urldate = {2021-11-24},
	journal = {arXiv:1707.01495 [cs]},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
	month = feb,
	year = {2018},
	note = {arXiv: 1707.01495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@article{laversanne-finot_intrinsically_2021,
	title = {Intrinsically {Motivated} {Exploration} of {Learned} {Goal} {Spaces}},
	volume = {14},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2020.555271},
	doi = {10.3389/fnbot.2020.555271},
	abstract = {Finding algorithms that allow agents to discover a wide variety of skills efficiently and autonomously, remains a challenge of Artificial Intelligence. Intrinsically Motivated Goal Exploration Processes (IMGEPs) have been shown to enable real world robots to learn repertoires of policies producing a wide range of diverse effects. They work by enabling agents to autonomously sample goals that they then try to achieve. In practice, this strategy leads to an efficient exploration of complex environments with high-dimensional continuous actions. Until recently, it was necessary to provide the agents with an engineered goal space containing relevant features of the environment. In this article we show that the goal space can be learned using deep representation learning algorithms, effectively reducing the burden of designing goal spaces. Our results pave the way to autonomous learning agents that are able to autonomously build a representation of the world and use this representation to explore the world efficiently. We present experiments in two environments using population-based IMGEPs. The first experiments are performed on a simple, yet challenging, simulated environment. Then, another set of experiments tests the applicability of those principles on a real-world robotic setup, where a 6-joint robotic arm learns to manipulate a ball inside an arena, by choosing goals in a space learned from its past experience.},
	urldate = {2021-11-24},
	journal = {Frontiers in Neurorobotics},
	author = {Laversanne-Finot, Adrien and Péré, Alexandre and Oudeyer, Pierre-Yves},
	year = {2021},
	pages = {109},
}

@article{oudeyer_intrinsic_2007,
	title = {Intrinsic {Motivation} {Systems} for {Autonomous} {Mental} {Development}},
	volume = {11},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2006.890271},
	abstract = {Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology},
	number = {2},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Oudeyer, Pierre-Yves and Kaplan, Frdric and Hafner, Verena V.},
	month = apr,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Evolutionary Computation},
	keywords = {Active learning, Autonomous mental development, autonomy, behavior, Cognitive robotics, complexity, Computational intelligence, Computer science, curiosity, development, developmental trajectory, epigenetic robotics, Humans, Intelligent robots, intrinsic motivation, Laboratories, learning, Neuroscience, Pediatrics, Psychology, reinforcement learning, values},
	pages = {265--286},
}

@article{doncieux_open-ended_2018,
	title = {Open-{Ended} {Learning}: {A} {Conceptual} {Framework} {Based} on {Representational} {Redescription}},
	volume = {12},
	issn = {1662-5218},
	shorttitle = {Open-{Ended} {Learning}},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2018.00059/full},
	doi = {10.3389/fnbot.2018.00059},
	abstract = {Reinforcement learning (RL) aims at building a policy that maximizes a task-related reward within a given domain. When the domain is known, i.e., when its states, actions and reward are deﬁned, Markov Decision Processes (MDPs) provide a convenient theoretical framework to formalize RL. But in an open-ended learning process, an agent or robot must solve an unbounded sequence of tasks that are not known in advance and the corresponding MDPs cannot be built at design time. This deﬁnes the main challenges of open-ended learning: how can the agent learn how to behave appropriately when the adequate states, actions and rewards representations are not given? In this paper, we propose a conceptual framework to address this question. We assume an agent endowed with low-level perception and action capabilities. This agent receives an external reward when it faces a task. It must discover the state and action representations that will let it cast the tasks as MDPs in order to solve them by RL. The relevance of the action or state representation is critical for the agent to learn efﬁciently. Considering that the agent starts with a low level, task-agnostic state and action spaces based on its low-level perception and action capabilities, we describe open-ended learning as the challenge of building the adequate representation of states and actions, i.e., of redescribing available representations. We suggest an iterative approach to this problem based on several successive Representational Redescription processes, and highlight the corresponding challenges in which intrinsic motivations play a key role.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Frontiers in Neurorobotics},
	author = {Doncieux, Stephane and Filliat, David and Díaz-Rodríguez, Natalia and Hospedales, Timothy and Duro, Richard and Coninx, Alexandre and Roijers, Diederik M. and Girard, Benoît and Perrin, Nicolas and Sigaud, Olivier},
	month = sep,
	year = {2018},
	pages = {59},
	file = {Doncieux et al. - 2018 - Open-Ended Learning A Conceptual Framework Based .pdf:C\:\\Users\\user\\Zotero\\storage\\LGBW4KQR\\Doncieux et al. - 2018 - Open-Ended Learning A Conceptual Framework Based .pdf:application/pdf},
}

@book{gigerenzer_heuristics_2011,
	title = {Heuristics},
	isbn = {978-0-19-974428-2},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199744282.001.0001/acprof-9780199744282},
	language = {en},
	urldate = {2020-12-02},
	publisher = {Oxford University Press},
	author = {Gigerenzer, Gerd and Hertwig, Ralph and Pachur, Thorsten},
	month = apr,
	year = {2011},
	doi = {10.1093/acprof:oso/9780199744282.001.0001},
	doi = {10.1093/acprof:oso/9780199744282.001.0001},
	file = {Gigerenzer et al. - 2011 - Heuristics.pdf:C\:\\Users\\user\\Zotero\\storage\\C4SD9X8R\\Gigerenzer et al. - 2011 - Heuristics.pdf:application/pdf},
}

@article{colas_language_2020,
	title = {Language as a {Cognitive} {Tool} to {Imagine} {Goals} in {Curiosity}-{Driven} {Exploration}},
	url = {http://arxiv.org/abs/2002.09253},
	abstract = {Developmental machine learning studies how artiﬁcial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either ﬁxed and hand-deﬁned or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, beneﬁt from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:2002.09253 [cs]},
	author = {Colas, Cédric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and Moulin-Frier, Clément and Dominey, Peter Ford and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.09253},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Colas et al. - 2020 - Language as a Cognitive Tool to Imagine Goals in C.pdf:C\:\\Users\\user\\Zotero\\storage\\MCV6H7FL\\Colas et al. - 2020 - Language as a Cognitive Tool to Imagine Goals in C.pdf:application/pdf},
}

@article{leibo_scalable_2021,
	title = {Scalable {Evaluation} of {Multi}-{Agent} {Reinforcement} {Learning} with {Melting} {Pot}},
	url = {http://arxiv.org/abs/2107.06857},
	abstract = {Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.},
	urldate = {2021-11-24},
	journal = {arXiv:2107.06857 [cs]},
	author = {Leibo, Joel Z. and Duéñez-Guzmán, Edgar and Vezhnevets, Alexander Sasha and Agapiou, John P. and Sunehag, Peter and Koster, Raphael and Matyas, Jayd and Beattie, Charles and Mordatch, Igor and Graepel, Thore},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.06857},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	annote = {Comment: Accepted to ICML 2021 and presented as a long talk; 33 pages; 9 figures},
}

@article{leibo_autocurricula_2019,
	title = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}},
	shorttitle = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}},
	url = {http://arxiv.org/abs/1903.00742},
	abstract = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
	urldate = {2021-11-24},
	journal = {arXiv:1903.00742 [cs, q-bio]},
	author = {Leibo, Joel Z. and Hughes, Edward and Lanctot, Marc and Graepel, Thore},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.00742},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 16 pages, 2 figures},
}

@article{potts_hominin_2013,
	title = {Hominin evolution in settings of strong environmental variability},
	volume = {73},
	issn = {0277-3791},
	url = {https://www.sciencedirect.com/science/article/pii/S0277379113001340},
	doi = {10.1016/j.quascirev.2013.04.003},
	abstract = {Investigations into how climate change shaped human evolution have begun to focus on environmental dynamics, i.e., the nature and tempo of climate and landscape variability, an approach that de-emphasizes static reconstructions of early hominin habitats. The interaction among insolation cycles is especially apparent in the paleoenvironmental records of the East African Rift System, where the longest records of human evolution are preserved. However, environmental indicators such as deep-sea oxygen isotopes, terrestrial dust flux, paleosol carbon isotopes, and lake sediments do not point consistently to any simple trend or climate driver of evolutionary change. Comparison of environmental indicators cautions against an exclusive focus on any given end-member of environmental fluctuation (driest or wettest, warmest or coolest), and argues for the impact of the entire range of variability in shaping evolutionary change. A model of alternating high and low climate variability for tropical Africa further implies that specific environmental indicators reflect different aspects of East African environmental dynamics. The model may thus help reconcile some of the conflicting interpretations about the environmental drivers of hominin evolution. First and last appearances of hominin lineages, benchmark biogeographic events, and the emergence of key adaptations and capacities to alter the surroundings are consistently concentrated in the predicted longest intervals of high climate variability. The view that emerges is that important changes in stone technology, sociality, and other aspects of hominin behavior can now be understood as adaptive responses to heightened habitat instability.},
	language = {en},
	urldate = {2021-11-24},
	journal = {Quaternary Science Reviews},
	author = {Potts, Richard},
	month = aug,
	year = {2013},
	keywords = {Adaptability, Africa, Environment, Human evolution, Olorgesailie, Paleoclimate, Variability selection},
	pages = {1--13},
}

@article{wilson_humans_2014,
	title = {Humans use directed and random exploration to solve the explore–exploit dilemma.},
	volume = {143},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038199},
	doi = {10.1037/a0038199},
	language = {en},
	number = {6},
	urldate = {2020-12-02},
	journal = {Journal of Experimental Psychology: General},
	author = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
	year = {2014},
	pages = {2074--2081},
	file = {Wilson et al. - 2014 - Humans use directed and random exploration to solv.pdf:C\:\\Users\\user\\Zotero\\storage\\SXJQ9EPT\\Wilson et al. - 2014 - Humans use directed and random exploration to solv.pdf:application/pdf},
}

@article{oudeyer_computational_2018,
	title = {Computational {Theories} of {Curiosity}-{Driven} {Learning}},
	url = {http://arxiv.org/abs/1802.10546},
	abstract = {What are the functions of curiosity? What are the mechanisms of curiosity-driven learning? We approach these questions about the living using concepts and tools from machine learning and developmental robotics. We argue that curiosity-driven learning enables organisms to make discoveries to solve complex problems with rare or deceptive rewards. By fostering exploration and discovery of a diversity of behavioural skills, and ignoring these rewards, curiosity can be efﬁcient to bootstrap learning when there is no information, or deceptive information, about local improvement towards these problems. We also explain the key role of curiosity for efﬁcient learning of world models. We review both normative and heuristic computational frameworks used to understand the mechanisms of curiosity in humans, conceptualizing the child as a sense-making organism. These frameworks enable us to discuss the bi-directional causal links between curiosity and learning, and to provide new hypotheses about the fundamental role of curiosity in self-organizing developmental structures through curriculum learning. We present various developmental robotics experiments that study these mechanisms in action, both supporting these hypotheses to understand better curiosity in humans and opening new research avenues in machine learning and artiﬁcial intelligence. Finally, we discuss challenges for the design of experimental paradigms for studying curiosity in psychology and cognitive neuroscience.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1802.10546 [cs]},
	author = {Oudeyer, Pierre-Yves},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.10546},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Oudeyer - 2018 - Computational Theories of Curiosity-Driven Learnin.pdf:C\:\\Users\\user\\Zotero\\storage\\FH5HRSCV\\Oudeyer - 2018 - Computational Theories of Curiosity-Driven Learnin.pdf:application/pdf},
}

@article{forestier_intrinsically_2020,
	title = {Intrinsically {Motivated} {Goal} {Exploration} {Processes} with {Automatic} {Curriculum} {Learning}},
	url = {http://arxiv.org/abs/1708.02190},
	abstract = {Intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children. It enables the discovery and acquisition of large repertoires of skills through self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous or self-supervised learning in machines. The IMGEP algorithmic architecture relies on several principles: 1) self-generation of goals, generalized as ﬁtness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation of the gathered data with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efﬁcient form of IMGEP, called Modular Population-Based IMGEP, that uses a population-based policy and an object-centered modularity in goals and mutations. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups including a real humanoid robot that can explore multiple spaces of goals with several hundred continuous dimensions. While no particular target goal is provided to the system, this curriculum allows the discovery of skills that act as stepping stone for learning more complex skills, e.g. nested tool use. We show that learning diverse spaces of goals with intrinsic motivations is more efﬁcient for learning complex skills than only trying to directly learn these complex skills.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1708.02190 [cs]},
	author = {Forestier, Sébastien and Portelas, Rémy and Mollard, Yoan and Oudeyer, Pierre-Yves},
	month = jul,
	year = {2020},
	note = {arXiv: 1708.02190},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Forestier et al. - 2020 - Intrinsically Motivated Goal Exploration Processes.pdf:C\:\\Users\\user\\Zotero\\storage\\LNQIZC9M\\Forestier et al. - 2020 - Intrinsically Motivated Goal Exploration Processes.pdf:application/pdf},
}

@article{linke_adapting_2020,
	title = {Adapting {Behaviour} via {Intrinsic} {Reward}: {A} {Survey} and {Empirical} {Study}},
	shorttitle = {Adapting {Behaviour} via {Intrinsic} {Reward}},
	url = {http://arxiv.org/abs/1906.07865},
	abstract = {Learning about many things can provide numerous beneﬁts to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience—how to adapt the system’s behaviour—to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven diﬃcult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare diﬀerent intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 15 diﬀerent rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behaviour, if each individual learner is introspective.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1906.07865 [cs, stat]},
	author = {Linke, Cam and Ady, Nadia M. and White, Martha and Degris, Thomas and White, Adam},
	month = aug,
	year = {2020},
	note = {arXiv: 1906.07865},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Linke et al. - 2020 - Adapting Behaviour via Intrinsic Reward A Survey .pdf:C\:\\Users\\user\\Zotero\\storage\\JGD9HLEW\\Linke et al. - 2020 - Adapting Behaviour via Intrinsic Reward A Survey .pdf:application/pdf},
}

@article{santucci_editorial_2020,
	title = {Editorial: {Intrinsically} {Motivated} {Open}-{Ended} {Learning} in {Autonomous} {Robots}},
	volume = {13},
	issn = {1662-5218},
	shorttitle = {Editorial},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2019.00115/full},
	doi = {10.3389/fnbot.2019.00115},
	language = {en},
	urldate = {2020-12-02},
	journal = {Frontiers in Neurorobotics},
	author = {Santucci, Vieri Giuliano and Oudeyer, Pierre-Yves and Barto, Andrew and Baldassarre, Gianluca},
	month = jan,
	year = {2020},
	pages = {115},
	file = {Santucci et al. - 2020 - Editorial Intrinsically Motivated Open-Ended Lear.pdf:C\:\\Users\\user\\Zotero\\storage\\IZYXEE2S\\Santucci et al. - 2020 - Editorial Intrinsically Motivated Open-Ended Lear.pdf:application/pdf},
}

@article{burda_large-scale_2018,
	title = {Large-{Scale} {Study} of {Curiosity}-{Driven} {Learning}},
	url = {http://arxiv.org/abs/1808.04355},
	abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the ﬁrst large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufﬁcient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1808.04355 [cs, stat]},
	author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04355},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf:C\:\\Users\\user\\Zotero\\storage\\UHRTM5V8\\Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf:application/pdf},
}

@article{savinov_episodic_2019,
	title = {Episodic {Curiosity} through {Reachability}},
	url = {http://arxiv.org/abs/1810.02274},
	abstract = {Rewards are sparse in the real world and most of today’s reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself — thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward — making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory — which incorporates rich information about environment dynamics. This allows us to overcome the known “couch-potato” issues of prior work — when the agent ﬁnds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in VizDoom, DMLab and MuJoCo. In navigational tasks from VizDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the ﬁrst-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1810.02274 [cs, stat]},
	author = {Savinov, Nikolay and Raichuk, Anton and Marinier, Raphaël and Vincent, Damien and Pollefeys, Marc and Lillicrap, Timothy and Gelly, Sylvain},
	month = aug,
	year = {2019},
	note = {arXiv: 1810.02274},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Savinov et al. - 2019 - Episodic Curiosity through Reachability.pdf:C\:\\Users\\user\\Zotero\\storage\\NMKI7KQG\\Savinov et al. - 2019 - Episodic Curiosity through Reachability.pdf:application/pdf},
}

@article{kovac_grimgep_2020,
	title = {{GRIMGEP}: {Learning} {Progress} for {Robust} {Goal} {Sampling} in {Visual} {Deep} {Reinforcement} {Learning}},
	shorttitle = {{GRIMGEP}},
	url = {http://arxiv.org/abs/2008.04388},
	abstract = {Autonomous agents using novelty based goal exploration are often efﬁcient in environments that require exploration. However, they get attracted to various forms of distracting unlearnable regions. To solve this problem, Absolute Learning Progress (ALP) has been used in reinforcement learning agents with predeﬁned goal features and access to expert knowledge. This work extends those concepts to unsupervised image-based goal exploration. We present the GRIMGEP framework: it provides a learned robust goal sampling prior that can be used on top of current state-of-the-art novelty seeking goal exploration approaches, enabling them to ignore noisy distracting regions while searching for novelty in the learnable regions. It clusters the goal space and estimates ALP for each cluster. These ALP estimates can then be used to detect the distracting regions, and build a prior that enables further goal sampling mechanisms to ignore them. We construct an image based environment with distractors, on which we show that wrapping current state-of-the-art goal exploration algorithms with our framework allows them to concentrate on interesting regions of the environment and drastically improve performances. The source code is available at https://sites.google.com/view/grimgep1.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:2008.04388 [cs, stat]},
	author = {Kovač, Grgur and Laversanne-Finot, Adrien and Oudeyer, Pierre-Yves},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.04388},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning, I.2.6, I.2.9},
	file = {Kovač et al. - 2020 - GRIMGEP Learning Progress for Robust Goal Samplin.pdf:C\:\\Users\\user\\Zotero\\storage\\YK4TBZ54\\Kovač et al. - 2020 - GRIMGEP Learning Progress for Robust Goal Samplin.pdf:application/pdf},
}

@article{barto_intrinsic_nodate,
	title = {Intrinsic {Motivation} {For} {Reinforcement} {Learning} {Systems}},
	abstract = {Motivation is a key factor in human learning. We learn best when we are highly motivated to learn. Psychologists distinguish between extrinsically-motivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically-motivated behavior, which is behavior done for its own sake. Is there an analogous distinction for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, can it be meaningful to distinguish between extrinsic and intrinsic motivation? In this paper, we argue that the answer to both questions is “yes,” and we describe some computational experiments that explore these ideas within the framework of computational reinforcement learning. In particular, we describe an approach by which artiﬁcial agents can learn hierarchies of reusable skills through a computational analog of intrinsic motivation.},
	language = {en},
	author = {Barto, Andrew G},
	pages = {6},
	file = {Barto - Intrinsic Motivation For Reinforcement Learning Sy.pdf:C\:\\Users\\user\\Zotero\\storage\\5RPLN7ZY\\Barto - Intrinsic Motivation For Reinforcement Learning Sy.pdf:application/pdf},
}

@article{bougie_fast_2020,
	title = {Fast and slow curiosity for high-level exploration in reinforcement learning},
	issn = {0924-669X, 1573-7497},
	url = {http://link.springer.com/10.1007/s10489-020-01849-3},
	doi = {10.1007/s10489-020-01849-3},
	abstract = {Deep reinforcement learning (DRL) algorithms rely on carefully designed environment rewards that are extrinsic to the agent. However, in many real-world scenarios rewards are sparse or delayed, motivating the need for discovering efficient exploration strategies. While intrinsically motivated agents hold promise of better local exploration, solving problems that require coordinated decisions over long-time horizons remains an open problem. We postulate that to discover such strategies, a DRL agent should be able to combine local and high-level exploration behaviors. To this end, we introduce the concept of fast and slow curiosity that aims to incentivize long-time horizon exploration. Our method decomposes the curiosity bonus into a fast reward that deals with local exploration and a slow reward that encourages global exploration. We formulate this bonus as the error in an agent’s ability to reconstruct the observations given their contexts. We further propose to dynamically weight local and high-level strategies by measuring state diversity. We evaluate our method on a variety of benchmark environments, including Minigrid, Super Mario Bros, and Atari games. Experimental results show that our agent outperforms prior approaches in most tasks in terms of exploration efficiency and mean scores.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Applied Intelligence},
	author = {Bougie, Nicolas and Ichise, Ryutaro},
	month = sep,
	year = {2020},
	file = {Bougie and Ichise - 2020 - Fast and slow curiosity for high-level exploration.pdf:C\:\\Users\\user\\Zotero\\storage\\TSYC9WHM\\Bougie and Ichise - 2020 - Fast and slow curiosity for high-level exploration.pdf:application/pdf},
}

@article{moulin-frier_self-organization_2014,
	title = {Self-organization of early vocal development in infants and machines: the role of intrinsic motivation},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {Self-organization of early vocal development in infants and machines},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.01006/abstract},
	doi = {10.3389/fpsyg.2013.01006},
	abstract = {We bridge the gap between two issues in infant development: vocal development and intrinsic motivation. We propose and experimentally test the hypothesis that general mechanisms of intrinsically motivated spontaneous exploration, also called curiosity-driven learning, can self-organize developmental stages during early vocal learning. We introduce a computational model of intrinsically motivated vocal exploration, which allows the learner to autonomously structure its own vocal experiments, and thus its own learning schedule, through a drive to maximize competence progress. This model relies on a physical model of the vocal tract, the auditory system and the agent’s motor control as well as vocalizations of social peers. We present computational experiments that show how such a mechanism can explain the adaptive transition from vocal self-exploration with little inﬂuence from the speech environment, to a later stage where vocal exploration becomes inﬂuenced by vocalizations of peers. Within the initial self-exploration phase, we show that a sequence of vocal production stages self-organizes, and shares properties with data from infant developmental psychology: the vocal learner ﬁrst discovers how to control phonation, then focuses on vocal variations of unarticulated sounds, and ﬁnally automatically discovers and focuses on babbling with articulated proto-syllables. As the vocal learner becomes more proﬁcient at producing complex sounds, imitating vocalizations of peers starts to provide high learning progress explaining an automatic shift from self-exploration to vocal imitation.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Frontiers in Psychology},
	author = {Moulin-Frier, Clément and Nguyen, Sao Mai and Oudeyer, Pierre-Yves},
	year = {2014},
	file = {Moulin-Frier et al. - 2014 - Self-organization of early vocal development in in.pdf:C\:\\Users\\user\\Zotero\\storage\\D8CPRKIS\\Moulin-Frier et al. - 2014 - Self-organization of early vocal development in in.pdf:application/pdf},
}

@article{oudeyer_what_2007,
	title = {What is {Intrinsic} {Motivation}? {A} {Typology} of {Computational} {Approaches}},
	volume = {1},
	issn = {1662-5218},
	shorttitle = {What is {Intrinsic} {Motivation}?},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2533589/},
	doi = {10.3389/neuro.12.006.2007},
	abstract = {Intrinsic motivation, centrally involved in spontaneous exploration and curiosity, is a crucial concept in developmental psychology. It has been argued to be a crucial mechanism for open-ended cognitive development in humans, and as such has gathered a growing interest from developmental roboticists in the recent years. The goal of this paper is threefold. First, it provides a synthesis of the different approaches of intrinsic motivation in psychology. Second, by interpreting these approaches in a computational reinforcement learning framework, we argue that they are not operational and even sometimes inconsistent. Third, we set the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches. This typology is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We argue that this kind of computational typology might be useful for opening new avenues for research both in psychology and developmental robotics.},
	urldate = {2021-11-24},
	journal = {Frontiers in Neurorobotics},
	author = {Oudeyer, Pierre-Yves and Kaplan, Frederic},
	month = nov,
	year = {2007},
	pmid = {18958277},
	pmcid = {PMC2533589},
	pages = {6},
}

@incollection{mirolli_functions_2013,
	address = {Berlin, Heidelberg},
	title = {Functions and {Mechanisms} of {Intrinsic} {Motivations}},
	isbn = {978-3-642-32375-1},
	url = {https://doi.org/10.1007/978-3-642-32375-1_3},
	abstract = {Mammals, and humans in particular, are endowed with an exceptional capacity for cumulative learning. This capacity crucially depends on the presence of intrinsic motivations, that is, motivations that are directly related not to an organism’s survival and reproduction but rather to its ability to learn. Recently, there have been a number of attempts to model and reproduce intrinsic motivations in artificial systems. Different kinds of intrinsic motivations have been proposed both in psychology and in machine learning and robotics: some are based on the knowledge of the learning system, while others are based on its competence. In this contribution, we discuss the distinction between knowledge-based and competence-based intrinsic motivations with respect to both the functional roles that motivations play in learning and the mechanisms by which those functions are implemented. In particular, after arguing that the principal function of intrinsic motivations consists in allowing the development of a repertoire of skills (rather than of knowledge), we suggest that at least two different sub-functions can be identified: (a) discovering which skills might be acquired and (b) deciding which skill to train when. We propose that in biological organisms, knowledge-based intrinsic motivation mechanisms might implement the former function, whereas competence-based mechanisms might underlie the latter one.},
	language = {en},
	urldate = {2021-11-24},
	booktitle = {Intrinsically {Motivated} {Learning} in {Natural} and {Artificial} {Systems}},
	publisher = {Springer},
	author = {Mirolli, Marco and Baldassarre, Gianluca},
	editor = {Baldassarre, Gianluca and Mirolli, Marco},
	year = {2013},
	doi = {10.1007/978-3-642-32375-1_3},
	keywords = {Intrinsic Motivation, Learning Agent, Reinforcement Learning, Salient Event, Superior Colliculus},
	pages = {49--72},
}

@article{benureau_behavioral_2016,
	title = {Behavioral {Diversity} {Generation} in {Autonomous} {Exploration} through {Reuse} of {Past} {Experience}},
	volume = {3},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/article/10.3389/frobt.2016.00008},
	doi = {10.3389/frobt.2016.00008},
	abstract = {The production of behavioral diversity – producing a diversity of effects – is an essential strategy for robots exploring the world when facing situations where interaction possibilities are unknown or non-obvious. It allows to discover new aspects of the environment that cannot be inferred or deduced from available knowledge. However, creating behavioral diversity in situations where it is most crucial – new and unknown ones – is far from trivial. In particular in large and redundant sensorimotor spaces, only small areas are interesting to explore for any practical purpose. When the environment does not provide clues or gradient toward those areas, trying to discover those areas relies on chance. To address this problem, we introduce a method to create behavioral diversity in a new sensorimotor task by re-enacting actions that allowed to produce behavioral diversity in a previous task, along with a measure that quantifies this diversity. We show that our method can learn how to interact with an object by reusing experience from another, that it adapts to instances of morphological changes and of dissimilarity between tasks, and how scaffolding behaviors can emerge by simply switching the attention of the robot to different parts of the environment. Finally, we show that the method can robustly use simulated experiences and crude cognitive models to generate behavioral diversity in real robots.},
	urldate = {2021-11-24},
	journal = {Frontiers in Robotics and AI},
	author = {Benureau, Fabien C. Y. and Oudeyer, Pierre-Yves},
	year = {2016},
	pages = {8},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\IY3WY79K\\Benureau and Oudeyer - 2016 - Behavioral Diversity Generation in Autonomous Expl.pdf:application/pdf},
}

@article{kim_active_2020,
	title = {Active {World} {Model} {Learning} with {Progress} {Curiosity}},
	url = {http://arxiv.org/abs/2007.07853},
	abstract = {World models are self-supervised predictive models of how the world evolves. Humans learn world models by curiously exploring their environment, in the process acquiring compact abstractions of high bandwidth sensory inputs, the ability to plan across long temporal horizons, and an understanding of the behavioral patterns of other agents. In this work, we study how to design such a curiosity-driven Active World Model Learning (AWML) system. To do so, we construct a curious agent building world models while visually exploring a 3D physical environment rich with distillations of representative real-world agents. We propose an AWML system driven by \${\textbackslash}gamma\$-Progress: a scalable and effective learning progress-based curiosity signal. We show that \${\textbackslash}gamma\$-Progress naturally gives rise to an exploration policy that directs attention to complex but learnable dynamics in a balanced manner, thus overcoming the "white noise problem". As a result, our \${\textbackslash}gamma\$-Progress-driven controller achieves significantly higher AWML performance than baseline controllers equipped with state-of-the-art exploration strategies such as Random Network Distillation and Model Disagreement.},
	urldate = {2021-11-24},
	journal = {arXiv:2007.07853 [cs, stat]},
	author = {Kim, Kuno and Sano, Megumi and De Freitas, Julian and Haber, Nick and Yamins, Daniel},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.07853},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2020. Video of results at https://bit.ly/31vg7v1},
}

@article{aubret_survey_2019,
	title = {A survey on intrinsic motivation in reinforcement learning},
	url = {http://arxiv.org/abs/1908.06976},
	abstract = {The reinforcement learning (RL) research area is very active, with an important number of new contributions; especially considering the emergent field of deep RL (DRL). However a number of scientific and technical challenges still need to be addressed, amongst which we can mention the ability to abstract actions or the difficulty to explore the environment which can be addressed by intrinsic motivation (IM). In this article, we provide a survey on the role of intrinsic motivation in DRL. We categorize the different kinds of intrinsic motivations and detail for each category, its advantages and limitations with respect to the mentioned challenges. Additionnally, we conduct an in-depth investigation of substantial current research questions, that are currently under study or not addressed at all in the considered research area of DRL. We choose to survey these research works, from the perspective of learning how to achieve tasks. We suggest then, that solving current challenges could lead to a larger developmental architecture which may tackle most of the tasks. We describe this developmental architecture on the basis of several building blocks composed of a RL algorithm and an IM module compressing information.},
	urldate = {2021-11-24},
	journal = {arXiv:1908.06976 [cs]},
	author = {Aubret, Arthur and Matignon, Laetitia and Hassas, Salima},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.06976},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{oudeyer_curiosity_nodate,
	title = {Curiosity and {Intrinsic} {Motivation} for {Autonomous} {Machine} {Learning}},
	language = {en},
	author = {Oudeyer, Pierre-Yves and Lopes, Manuel and Kidd, Celeste},
	pages = {2},
	file = {Oudeyer et al. - Curiosity and Intrinsic Motivation for Autonomous .pdf:C\:\\Users\\user\\Zotero\\storage\\6T3R46WJ\\Oudeyer et al. - Curiosity and Intrinsic Motivation for Autonomous .pdf:application/pdf},
}

@article{holm_episodic_2019,
	title = {Episodic curiosity for avoiding asteroids: {Per}-trial information gain for choice outcomes drive information seeking},
	volume = {9},
	issn = {2045-2322},
	shorttitle = {Episodic curiosity for avoiding asteroids},
	url = {http://www.nature.com/articles/s41598-019-47671-x},
	doi = {10.1038/s41598-019-47671-x},
	language = {en},
	number = {1},
	urldate = {2020-12-02},
	journal = {Scientific Reports},
	author = {Holm, Linus and Wadenholt, Gustaf and Schrater, Paul},
	month = dec,
	year = {2019},
	pages = {11265},
	file = {Holm et al. - 2019 - Episodic curiosity for avoiding asteroids Per-tri.pdf:C\:\\Users\\user\\Zotero\\storage\\8KH3V62C\\Holm et al. - 2019 - Episodic curiosity for avoiding asteroids Per-tri.pdf:application/pdf},
}

@article{portelas_teacher_2019,
	title = {Teacher algorithms for curriculum learning of {Deep} {RL} in continuously parameterized environments},
	url = {http://arxiv.org/abs/1910.07224},
	abstract = {We consider the problem of how a teacher algorithm can enable an unknown Deep Reinforcement Learning (DRL) student to become good at a skill over a wide range of diverse environments. To do so, we study how a teacher algorithm can learn to generate a learning curriculum, whereby it sequentially samples parameters controlling a stochastic procedural generation of environments. Because it does not initially know the capacities of its student, a key challenge for the teacher is to discover which environments are easy, difﬁcult or unlearnable, and in what order to propose them to maximize the efﬁciency of learning over the learnable ones. To achieve this, this problem is transformed into a surrogate continuous bandit problem where the teacher samples environments in order to maximize absolute learning progress of its student. We present a new algorithm modeling absolute learning progress with Gaussian mixture models (ALP-GMM). We also adapt existing algorithms and provide a complete study in the context of DRL. Using parameterized variants of the BipedalWalker environment, we study their efﬁciency to personalize a learning curriculum for different learners (embodiments), their robustness to the ratio of learnable/unlearnable environments, and their scalability to non-linear and high-dimensional parameter spaces. Videos and code are available at https://github.com/flowersteam/teachDeepRL.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1910.07224 [cs, stat]},
	author = {Portelas, Rémy and Colas, Cédric and Hofmann, Katja and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07224},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Portelas et al. - 2019 - Teacher algorithms for curriculum learning of Deep.pdf:C\:\\Users\\user\\Zotero\\storage\\DBGMXT9Q\\Portelas et al. - 2019 - Teacher algorithms for curriculum learning of Deep.pdf:application/pdf},
}

@article{westermann_neuroconstructivism_2007,
	title = {Neuroconstructivism},
	volume = {10},
	issn = {1363755X, 14677687},
	url = {http://doi.wiley.com/10.1111/j.1467-7687.2007.00567.x},
	doi = {10.1111/j.1467-7687.2007.00567.x},
	abstract = {Neuroconstructivism is a theoretical framework focusing on the construction of representations in the developing brain. Cognitive development is explained as emerging from the experience-dependent development of neural structures supporting mental representations. Neural development occurs in the context of multiple interacting constraints acting on different levels, from the individual cell to the external environment of the developing child. Cognitive development can thus be understood as a trajectory originating from the constraints on the underlying neural structures. This perspective offers an integrated view of normal and abnormal development as well as of development and adult processing, and it stands apart from traditional cognitive approaches in taking seriously the constraints on cognition inherent to the substrate that delivers it.},
	language = {en},
	number = {1},
	urldate = {2020-12-02},
	journal = {Developmental Science},
	author = {Westermann, Gert and Mareschal, Denis and Johnson, Mark H. and Sirois, Sylvain and Spratling, Michael W. and Thomas, Michael S.C.},
	month = jan,
	year = {2007},
	pages = {75--83},
	file = {Westermann et al. - 2007 - Neuroconstructivism.pdf:C\:\\Users\\user\\Zotero\\storage\\AQZDFT5W\\Westermann et al. - 2007 - Neuroconstructivism.pdf:application/pdf},
}

@incollection{bazhydai_curiosity_2020,
	title = {Curiosity and {Exploration}},
	isbn = {978-0-12-816511-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128093245058041},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {Encyclopedia of {Infant} and {Early} {Childhood} {Development}},
	publisher = {Elsevier},
	author = {Bazhydai, Marina and Twomey, Katherine and Westermann, Gert},
	year = {2020},
	doi = {10.1016/B978-0-12-809324-5.05804-1},
	pages = {370--378},
	file = {Bazhydai et al. - 2020 - Curiosity and Exploration.pdf:C\:\\Users\\user\\Zotero\\storage\\M7NB6H7A\\Bazhydai et al. - 2020 - Curiosity and Exploration.pdf:application/pdf},
}

@article{gureckis_self-directed_2012,
	title = {Self-{Directed} {Learning}: {A} {Cognitive} and {Computational} {Perspective}},
	volume = {7},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Self-{Directed} {Learning}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691612454304},
	doi = {10.1177/1745691612454304},
	abstract = {A widely advocated idea in education is that people learn better when the flow of experience is under their control (i.e., learning is self-directed). However, the reasons why volitional control might result in superior acquisition and the limits to such advantages remain poorly understood. In this article, we review the issue from both a cognitive and computational perspective. On the cognitive side, self-directed learning allows individuals to focus effort on useful information they do not yet possess, can expose information that is inaccessible via passive observation, and may enhance the encoding and retention of materials. On the computational side, the development of efficient “active learning” algorithms that can select their own training data is an emerging research topic in machine learning. This review argues that recent advances in these related fields may offer a fresh theoretical perspective on how people gather information to support their own learning.},
	language = {en},
	number = {5},
	urldate = {2020-12-02},
	journal = {Perspectives on Psychological Science},
	author = {Gureckis, Todd M. and Markant, Douglas B.},
	month = sep,
	year = {2012},
	pages = {464--481},
	file = {Gureckis and Markant - 2012 - Self-Directed Learning A Cognitive and Computatio.pdf:C\:\\Users\\user\\Zotero\\storage\\RC8XEVU7\\Gureckis and Markant - 2012 - Self-Directed Learning A Cognitive and Computatio.pdf:application/pdf},
}

@article{colas_curious_2019,
	title = {{CURIOUS}: {Intrinsically} {Motivated} {Modular} {Multi}-{Goal} {Reinforcement} {Learning}},
	shorttitle = {{CURIOUS}},
	url = {http://arxiv.org/abs/1810.06284},
	abstract = {In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1810.06284 [cs]},
	author = {Colas, Cédric and Fournier, Pierre and Sigaud, Olivier and Chetouani, Mohamed and Oudeyer, Pierre-Yves},
	month = may,
	year = {2019},
	note = {arXiv: 1810.06284},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Colas et al. - 2019 - CURIOUS Intrinsically Motivated Modular Multi-Goa.pdf:C\:\\Users\\user\\Zotero\\storage\\AFTBR8ZY\\Colas et al. - 2019 - CURIOUS Intrinsically Motivated Modular Multi-Goa.pdf:application/pdf},
}

@incollection{horst_computational_2019,
	address = {New York, NY: Routledge, [2019] {\textbar} Series: Routledge},
	edition = {1},
	title = {Computational and {Robotic} {Models} of {Early} {Language} {Development}},
	isbn = {978-1-315-11062-2},
	url = {https://www.taylorfrancis.com/books/9781351616621/chapters/10.4324/9781315110622-5},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {International {Handbook} of {Language} {Acquisition}},
	publisher = {Routledge},
	author = {Oudeyer, Pierre-Yves and Kachergis, George and Schueller, William},
	editor = {Horst, Jessica S. and von Koss Torkildsen, Janne},
	month = may,
	year = {2019},
	doi = {10.4324/9781315110622-5},
	pages = {76--101},
	file = {Oudeyer et al. - 2019 - Computational and Robotic Models of Early Language.pdf:C\:\\Users\\user\\Zotero\\storage\\UETBFC8E\\Oudeyer et al. - 2019 - Computational and Robotic Models of Early Language.pdf:application/pdf},
}

@article{nussenbaum_reinforcement_2019,
	title = {Reinforcement learning across development: {What} insights can we draw from a decade of research?},
	volume = {40},
	issn = {18789293},
	shorttitle = {Reinforcement learning across development},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1878929319303202},
	doi = {10.1016/j.dcn.2019.100733},
	abstract = {The past decade has seen the emergence of the use of reinforcement learning models to study developmental change in value-based learning. It is unclear, however, whether these computational modeling studies, which have employed a wide variety of tasks and model variants, have reached convergent conclusions. In this review, we examine whether the tuning of model parameters that govern different aspects of learning and decisionmaking processes vary consistently as a function of age, and what neurocognitive developmental changes may account for differences in these parameter estimates across development. We explore whether patterns of developmental change in these estimates are better described by differences in the extent to which individuals adapt their learning processes to the statistics of different environments, or by more static learning biases that emerge across varied contexts. We focus specifically on learning rates and inverse temperature parameter esti­ mates, and find evidence that from childhood to adulthood, individuals become better at optimally weighting recent outcomes during learning across diverse contexts and less exploratory in their value-based decisionmaking. We provide recommendations for how these two possibilities — and potential alternative accounts —can be tested more directly to build a cohesive body of research that yields greater insight into the development of core learning processes.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Developmental Cognitive Neuroscience},
	author = {Nussenbaum, Kate and Hartley, Catherine A.},
	month = dec,
	year = {2019},
	pages = {100733},
	file = {Nussenbaum and Hartley - 2019 - Reinforcement learning across development What in.pdf:C\:\\Users\\user\\Zotero\\storage\\ULU6Q693\\Nussenbaum and Hartley - 2019 - Reinforcement learning across development What in.pdf:application/pdf},
}

@article{botvinick_hierarchically_2009,
	title = {Hierarchically organized behavior and its neural foundations: {A} reinforcement learning perspective},
	volume = {113},
	issn = {00100277},
	shorttitle = {Hierarchically organized behavior and its neural foundations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027708002059},
	doi = {10.1016/j.cognition.2008.08.011},
	abstract = {Research on human and animal behavior has long emphasized its hierarchical structure—the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reﬂect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Speciﬁcally, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests speciﬁc ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identiﬁes new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
	language = {en},
	number = {3},
	urldate = {2020-12-02},
	journal = {Cognition},
	author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andew G.},
	month = dec,
	year = {2009},
	pages = {262--280},
	file = {Botvinick et al. - 2009 - Hierarchically organized behavior and its neural f.pdf:C\:\\Users\\user\\Zotero\\storage\\ZMT56WVD\\Botvinick et al. - 2009 - Hierarchically organized behavior and its neural f.pdf:application/pdf},
}

@inproceedings{moulin-frier_exploration_2013,
	address = {Osaka, Japan},
	title = {Exploration strategies in developmental robotics: {A} unified probabilistic framework},
	isbn = {978-1-4799-1036-6},
	shorttitle = {Exploration strategies in developmental robotics},
	url = {http://ieeexplore.ieee.org/document/6652535/},
	doi = {10.1109/DevLrn.2013.6652535},
	abstract = {We present a probabilistic framework unifying two important families of exploration mechanisms recently shown to be efﬁcient to learn complex non-linear redundant sensorimotor mappings. These two explorations mechanisms are: 1) goal babbling, 2) active learning driven by the maximization of empirically measured learning progress. We show how this generic framework allows to model several recent algorithmic architectures for exploration. Then, we propose a particular implementation using Gaussian Mixture Models, which at the same time provides an original empirical measure of the competence progress. Finally, we perform computer simulations on two simulated setups: the control of the end effector of a 7-DoF arm and the control of the formants produced by an articulatory synthesizer.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {2013 {IEEE} {Third} {Joint} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL})},
	publisher = {IEEE},
	author = {Moulin-Frier, Clément and Oudeyer, Pierre-Yves},
	month = aug,
	year = {2013},
	pages = {1--6},
	file = {Moulin-Frier and Oudeyer - 2013 - Exploration strategies in developmental robotics .pdf:C\:\\Users\\user\\Zotero\\storage\\QTMGCZHT\\Moulin-Frier and Oudeyer - 2013 - Exploration strategies in developmental robotics .pdf:application/pdf},
}

@inproceedings{lefort_active_2015,
	address = {Providence, RI, USA},
	title = {Active learning of local predictable representations with artificial curiosity},
	isbn = {978-1-4673-9320-1},
	url = {http://ieeexplore.ieee.org/document/7346145/},
	doi = {10.1109/DEVLRN.2015.7346145},
	abstract = {In this article, we present some preliminary work on integrating an artiﬁcial curiosity mechanism in PROPRE, a generic and modular neural architecture, to obtain online, openended and active learning of a sensory-motor space, where large areas can be unlearnable. PROPRE consists of the combination of the projection of the input motor ﬂow, using a self-organizing map, with the regression of the sensory output ﬂow from this projection representation, using a linear regression. The main feature of PROPRE is the use of a predictability module that provides an interestingness measure for the current motor stimulus depending on a simple evaluation of the sensory prediction quality. This measure modulates the projection learning so that to favor the representations that predict the output better than a local average. Especially, this leads to the learning of local representations where an input/output relationship is deﬁned [1]. In this article, we propose an artiﬁcial curiosity mechanism based on the monitoring of learning progress, as proposed in [2], in the neighborhood of each local representation. Thus, PROPRE simultaneously learns interesting representations of the input ﬂow (depending on their capacities to predict the output) and explores actively this input space where the learning progress is the higher. We illustrate our architecture on the learning of a direct model of an arm whose hand can only be perceived in a restricted visual space. The modulation of the projection learning leads to a better performance and the use of the curiosity mechanism provides quicker learning and even improves the ﬁnal performance.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {2015 {Joint} {IEEE} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL}-{EpiRob})},
	publisher = {IEEE},
	author = {Lefort, Mathieu and Gepperth, Alexander},
	month = aug,
	year = {2015},
	pages = {228--233},
	file = {Lefort and Gepperth - 2015 - Active learning of local predictable representatio.pdf:C\:\\Users\\user\\Zotero\\storage\\QRNH8DC6\\Lefort and Gepperth - 2015 - Active learning of local predictable representatio.pdf:application/pdf},
}

@article{twomey_curiosity-based_2018,
	title = {Curiosity-based learning in infants: a neurocomputational approach},
	volume = {21},
	issn = {1363755X},
	shorttitle = {Curiosity-based learning in infants},
	url = {http://doi.wiley.com/10.1111/desc.12629},
	doi = {10.1111/desc.12629},
	abstract = {Infants are curious learners who drive their own cognitive development by imposing structure on their learning environment as they explore. Understanding the mechanisms by which infants structure their own learning is therefore critical to our understanding of development. Here we propose an explicit mechanism for intrinsically motivated information selection that maximizes learning. We first present a neurocomputational model of infant visual category learning, capturing existing empirical data on the role of environmental complexity on learning. Next we “set the model free”, allowing it to select its own stimuli based on a formalization of curiosity and three alternative selection mechanisms. We demonstrate that maximal learning emerges when the model is able to maximize stimulus novelty relative to its internal states, depending on the interaction across learning between the structure of the environment and the plasticity in the learner itself. We discuss the implications of this new curiosity mechanism for both existing computational models of reinforcement learning and for our understanding of this fundamental mechanism in early development.},
	language = {en},
	number = {4},
	urldate = {2020-12-02},
	journal = {Developmental Science},
	author = {Twomey, Katherine and Westermann, Gert},
	month = jul,
	year = {2018},
	pages = {e12629},
	file = {Twomey and Westermann - 2018 - Curiosity-based learning in infants a neurocomput.pdf:C\:\\Users\\user\\Zotero\\storage\\VWAJHIJW\\Twomey and Westermann - 2018 - Curiosity-based learning in infants a neurocomput.pdf:application/pdf},
}

@article{nisioti_grounding_2020,
	title = {Grounding {Artificial} {Intelligence} in the {Origins} of {Human} {Behavior}},
	url = {http://arxiv.org/abs/2012.08564},
	abstract = {Recent advances in Artificial Intelligence (AI) have revived the quest for agents able to acquire an open-ended repertoire of skills. However, although this ability is fundamentally related to the characteristics of human intelligence, research in this field rarely considers the processes that may have guided the emergence of complex cognitive capacities during the evolution of the species. Research in Human Behavioral Ecology (HBE) seeks to understand how the behaviors characterizing human nature can be conceived as adaptive responses to major changes in the structure of our ecological niche. In this paper, we propose a framework highlighting the role of environmental complexity in open-ended skill acquisition, grounded in major hypotheses from HBE and recent contributions in Reinforcement learning (RL). We use this framework to highlight fundamental links between the two disciplines, as well as to identify feedback loops that bootstrap ecological complexity and create promising research directions for AI researchers.},
	urldate = {2021-11-24},
	journal = {arXiv:2012.08564 [cs]},
	author = {Nisioti, Eleni and Moulin-Frier, Clément},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.08564},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\MWK5JYJC\\Nisioti and Moulin-Frier - 2020 - Grounding Artificial Intelligence in the Origins o.pdf:application/pdf},
}

@article{fogarty_niche_2017,
	title = {The niche construction of cultural complexity: interactions between innovations, population size and the environment},
	volume = {372},
	issn = {1471-2970},
	shorttitle = {The niche construction of cultural complexity},
	doi = {10.1098/rstb.2016.0428},
	abstract = {Niche construction is a process through which organisms alter their environments and, in doing so, influence or change the selective pressures to which they are subject. 'Cultural niche construction' refers specifically to the effect of cultural traits on the selective environments of other biological or cultural traits and may be especially important in human evolution. In addition, the relationship between population size and cultural accumulation has been the subject of extensive debate, in part because anthropological studies have demonstrated a significant association between population size and toolkit complexity in only a subset of studied cultures. Here, we review the role of cultural innovation in constructing human evolutionary niches and introduce a new model to describe the accumulation of human cultural traits that incorporates the effects of cultural niche construction. We consider the results of this model in light of available data on human toolkit sizes across populations to help elucidate the important differences between food-gathering societies and food-producing societies, in which niche construction may be a more potent force. These results support the idea that a population's relationship with its environment, represented here by cultural niche construction, should be considered alongside population size in studies of cultural complexity.This article is part of the themed issue 'Process and pattern in innovations from cells to societies'.},
	language = {eng},
	number = {1735},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Fogarty, Laurel and Creanza, Nicole},
	month = dec,
	year = {2017},
	pmid = {29061900},
	pmcid = {PMC5665815},
	keywords = {cultural accumulation, cultural evolution, Cultural Evolution, Diffusion of Innovation, Environment, environmental change, Humans, innovation, Inventions, Models, Psychological, Population Density, population size, subsistence strategies},
	pages = {20160428},
	file = {Full Text:C\:\\Users\\user\\Zotero\\storage\\JM9CY2IN\\Fogarty and Creanza - 2017 - The niche construction of cultural complexity int.pdf:application/pdf},
}

@article{lucas_industrial_2004,
	title = {The {Industrial} {Revolution}: {Past} and {Future}},
	volume = {XLIV},
	language = {en},
	number = {8},
	journal = {Economic Education Bulletin},
	author = {Lucas, Robert E},
	year = {2004},
	pages = {8},
	file = {Lucas - The Industrial Revolution Past and Future.pdf:C\:\\Users\\user\\Zotero\\storage\\88MA8BFY\\Lucas - The Industrial Revolution Past and Future.pdf:application/pdf},
}

@book{hull_principles_1943,
	address = {Oxford, England},
	series = {Principles of behavior: an introduction to behavior theory},
	title = {Principles of behavior: an introduction to behavior theory},
	shorttitle = {Principles of behavior},
	abstract = {After a general presentation of the nature of scientific theory in its deductive aspects and a defense of an objective point of view with reference to molar adaptive behavior in contrast to subjectivism, teleology, and emergentism, the author gives a detailed exposition of the fundamental principles of behavior. Innate behavior and its relation to the termination of needs are basic facts, as are also such physiological events as sensory after-discharge and internal oscillatory, variable factors. The development of the theory offers an account of behavior in terms of reinforcement, generalization, motivation, inhibition, oscillation, and response evocation which may be thought of as a chain of conditions beginning with the physical stimulus and ending in the response. 13 major symbolic constructs, directly or indirectly anchored to 6 objectively observable events, are utilized in explanation of the above chain of conditions of behavior. An examination of stimulus compounds and patterns leads the author, in opposition to Gestalt psychology, to the view that patterning is a secondary rather than a primary principle of behavior. The 22 chapters of the book contain lists of references, technical notes, and derivations of formulae. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Appleton-Century},
	author = {Hull, C. L.},
	year = {1943},
	note = {Pages: x, 422},
}

@book{maturana_autopoiesis_1980,
	address = {Dordrecht, Holland ; Boston},
	edition = {1st edition},
	title = {Autopoiesis and {Cognition}: {The} {Realization} of the {Living}},
	isbn = {978-90-277-1016-1},
	shorttitle = {Autopoiesis and {Cognition}},
	language = {English},
	publisher = {D. Reidel Publishing Company},
	author = {Maturana, H. R. and Varela, F. J.},
	month = jan,
	year = {1980},
}

@book{nake_asthetik_1974,
	title = {Ästhetik als {Informationsverarbeitung}: {Grundlagen} und {Anwendungen} der {Informatik} im {Bereich} ästhetischer {Produktion} und {Kritik}},
	isbn = {978-3-7091-7098-4},
	shorttitle = {Ästhetik als {Informationsverarbeitung}},
	language = {Deutsch},
	publisher = {Springer-Verlag/Wien},
	author = {Nake, Frieder},
	month = jan,
	year = {1974},
}

@article{mcclelland_place_2009,
	title = {The place of modeling in cognitive science},
	volume = {1},
	issn = {1756-8765},
	doi = {10.1111/j.1756-8765.2008.01003.x},
	abstract = {I consider the role of cognitive modeling in cognitive science. Modeling, and the computers that enable it, are central to the field, but the role of modeling is often misunderstood. Models are not intended to capture fully the processes they attempt to elucidate. Rather, they are explorations of ideas about the nature of cognitive processes. In these explorations, simplification is essential-through simplification, the implications of the central ideas become more transparent. This is not to say that simplification has no downsides; it does, and these are discussed. I then consider several contemporary frameworks for cognitive modeling, stressing the idea that each framework is useful in its own particular ways. Increases in computer power (by a factor of about 4 million) since 1958 have enabled new modeling paradigms to emerge, but these also depend on new ways of thinking. Will new paradigms emerge again with the next 1,000-fold increase?},
	language = {eng},
	number = {1},
	journal = {Topics in Cognitive Science},
	author = {McClelland, James L.},
	month = jan,
	year = {2009},
	pmid = {25164798},
	keywords = {Bayesian approaches, Cognitive architectures, Cognitive Science, Computer simulation, Connectionist models, Dynamical systems, Humans, Hybrid models, Modeling frameworks, Models, Theoretical, Neural Networks, Computer, Symbolic models of cognition},
	pages = {11--38},
}

@article{lin_story_2020,
	title = {A {Story} of {Two} {Streams}: {Reinforcement} {Learning} {Models} from {Human} {Behavior} and {Neuropsychiatry}},
	shorttitle = {A {Story} of {Two} {Streams}},
	url = {http://arxiv.org/abs/1906.11286},
	abstract = {Drawing an inspiration from behavioral studies of human decision making, we propose here a more general and flexible parametric framework for reinforcement learning that extends standard Q-learning to a two-stream model for processing positive and negative rewards, and allows to incorporate a wide range of reward-processing biases -- an important component of human decision making which can help us better understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems, as well as various neuropsychiatric conditions associated with disruptions in normal reward processing. From the computational perspective, we observe that the proposed Split-QL model and its clinically inspired variants consistently outperform standard Q-Learning and SARSA methods, as well as recently proposed Double Q-Learning approaches, on simulated tasks with particular reward distributions, a real-world dataset capturing human decision-making in gambling tasks, and the Pac-Man game in a lifelong learning setting across different reward stationarities.},
	urldate = {2021-11-24},
	journal = {arXiv:1906.11286 [cs, q-bio, stat]},
	author = {Lin, Baihan and Cecchi, Guillermo and Bouneffouf, Djallel and Reinen, Jenna and Rish, Irina},
	month = apr,
	year = {2020},
	note = {arXiv: 1906.11286},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	annote = {Comment: Published in AAMAS 2020 as a full paper. This article supersedes our work arXiv:1706.02897 into RL setting and extends extensively into RL games, cognitive modeling, and gambling tasks in lifelong learning setting},
}

@article{grizou_curious_2020,
	title = {A curious formulation robot enables the discovery of a novel protocell behavior},
	volume = {6},
	issn = {2375-2548},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6994213/},
	doi = {10.1126/sciadv.aay4237},
	abstract = {A bespoke robotic system driven by a curiosity algorithm discovers emergent behaviors of oil-in-water protocell formulations., We describe a chemical robotic assistant equipped with a curiosity algorithm (CA) that can efficiently explore the states a complex chemical system can exhibit. The CA-robot is designed to explore formulations in an open-ended way with no explicit optimization target. By applying the CA-robot to the study of self-propelling multicomponent oil-in-water protocell droplets, we are able to observe an order of magnitude more variety in droplet behaviors than possible with a random parameter search and given the same budget. We demonstrate that the CA-robot enabled the observation of a sudden and highly specific response of droplets to slight temperature changes. Six modes of self-propelled droplet motion were identified and classified using a time-temperature phase diagram and probed using a variety of techniques including NMR. This work illustrates how CAs can make better use of a limited experimental budget and significantly increase the rate of unpredictable observations, leading to new discoveries with potential applications in formulation chemistry.},
	number = {5},
	urldate = {2021-11-24},
	journal = {Science Advances},
	author = {Grizou, Jonathan and Points, Laurie J. and Sharma, Abhishek and Cronin, Leroy},
	month = jan,
	year = {2020},
	pmid = {32064348},
	pmcid = {PMC6994213},
	pages = {eaay4237},
}

@article{delmas_fostering_2018,
	title = {Fostering {Health} {Education} {With} a {Serious} {Game} in {Children} {With} {Asthma}: {Pilot} {Studies} for {Assessing} {Learning} {Efficacy} and {Automatized} {Learning} {Personalization}},
	volume = {3},
	issn = {2504-284X},
	shorttitle = {Fostering {Health} {Education} {With} a {Serious} {Game} in {Children} {With} {Asthma}},
	url = {https://www.frontiersin.org/article/10.3389/feduc.2018.00099},
	doi = {10.3389/feduc.2018.00099},
	abstract = {Coupled with Health Education programs, an e-learning platform—KidBreath—was participatory designed and assessed in situ (Study 1) and was augmented and tested with an Intelligent Tutoring System (ITS) based on Multi-Armed Bandit Methods (Study 2). For each study, the impact of KidBreath practice was assessed in children with asthma in terms of pedagogical efficacy (knowledge of the illness), pedagogical efficiency (usability, type of motivation and level of interest elicited), and therapeutic effect (illness perception, system's expectation and judgement in disease self-management, child's implication in study). For the Study 1, asthma children aged 8 to 11 years used the tool at home without time pressure for 2 months according to a predefined learning sequence defined by the research team. Results supported pedagogical efficacy of KidBreath, with a significant increase of general knowledge about asthma after use. It also featured a greater learning gain for children knowing the least about the illness before use. Results on pedagogical efficiency revealed a great intrinsic motivation elicited by KidBreath showing a deep level of interest in the edutainment activities. Study 2 explored an augmented version of KidBreath with learning optimization algorithm (called ZPDES) after its use during 1 month. Pedagogical efficacy was less conclusive than Study 1 because less content was displayed due to algorithm parameters. However, the ITS-augmented KidBreath use showed a strong impact in pedagogical efficiency and therapeutic adherence features. Even if implementation improvements must be done in future works, this preliminary study highlighted the viability of our methods to design an ITS as serious game in health education context for all chronic diseases.},
	urldate = {2021-11-24},
	journal = {Frontiers in Education},
	author = {Delmas, Alexandra and Clement, Benjamin and Oudeyer, Pierre-Yves and Sauzéon, Helène},
	year = {2018},
	pages = {99},
}

@inproceedings{clement_comparison_2016,
	address = {Raleigh, United States},
	series = {Proceedings of the 9th {International} {Conference} on {Educational} {Data} {Mining}},
	title = {A {Comparison} of {Automatic} {Teaching} {Strategies} for {Heterogeneous} {Student} {Populations}},
	url = {https://hal.inria.fr/hal-01360338},
	abstract = {Online planning of good teaching sequences has the potential to provide a truly personalized teaching experience with a huge impact on the motivation and learning of students. In this work we compare two main approaches to achieve such a goal, POMDPs that can find an optimal long-term path, and Multi-armed bandits that optimize policies locally and greedily but that are computationally more efficient while requiring a simpler learner model. Even with the availability of data from several tutoring systems, it is never possible to have a highly accurate student model or one that is tuned for each particular student. We study what is the impact of the quality of the student model on the final results obtained with the two algorithms. Our hypothesis is that the higher flexibility of multi-armed bandits in terms of the complexity and precision of the student model will compensate for the lack of longer term planning featured in POMDPs. We present several simulated results showing the limits and robustness of each approach and a comparison of heterogeneous populations of students.},
	urldate = {2021-11-24},
	booktitle = {{EDM} 16 - 9th {International} {Conference} on {Educational} {Data} {Mining}},
	author = {Clément, Benjamin and Oudeyer, Pierre-Yves and Lopes, Manuel},
	month = jun,
	year = {2016},
	file = {HAL PDF Full Text:C\:\\Users\\user\\Zotero\\storage\\ACDHJJSH\\Clément et al. - 2016 - A Comparison of Automatic Teaching Strategies for .pdf:application/pdf},
}

@article{clement_multi-armed_2015,
	title = {Multi-{Armed} {Bandits} for {Intelligent} {Tutoring} {Systems}},
	doi = {10.5281/ZENODO.3554667},
	abstract = {An approach to Intelligent Tutoring Systems which adaptively personalizes sequences of learning activities to maximize skills acquired by students, taking into account the limited time and motivational resources is presented. We present an approach to Intelligent Tutoring Systems which adaptively personalizes sequences of learning activities to maximize skills acquired by students, taking into account the limited time and motivational resources. At a given point in time, the system proposes to the students the activity which makes them progress faster. We introduce two algorithms that rely on the empirical estimation of the learning progress, RiARiT that uses information about the difficulty of each exercise and ZPDES that uses much less knowledge about the problem. 
 
The system is based on the combination of three approaches. First, it leverages recent models of intrinsically motivated learning by transposing them to active teaching, relying on empirical estimation of learning progress provided by specific activities to particular students. Second, it uses state-of-the-art Multi-Arm Bandit (MAB) techniques to efficiently manage the exploration/exploitation challenge of this optimization process. Third, it leverages expert knowledge to constrain and bootstrap initial exploration of the MAB, while requiring only coarse guidance information of the expert and allowing the system to deal with didactic gaps in its knowledge. The system is evaluated in a scenario where 7-8 year old schoolchildren learn how to decompose numbers while manipulating money. Systematic experiments are presented with simulated students, followed by results of a user study across a population of 400 school children.},
	journal = {EDM},
	author = {Clément, Benjamin and Roy, Didier and Oudeyer, Pierre-Yves and Lopes, Manuel},
	year = {2015},
}

@inproceedings{pan_reinforcement_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Reinforcement learning based curiosity-driven testing of {Android} applications},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397354},
	doi = {10.1145/3395363.3397354},
	abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
	urldate = {2021-11-24},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
	month = jul,
	year = {2020},
	keywords = {Android app testing, functional scenario division, reinforcement learning},
	pages = {153--164},
}

@inproceedings{schmidhuber_possibility_1991,
	title = {A possibility for implementing curiosity and boredom in model-building neural controllers},
	doi = {10.7551/mitpress/3115.003.0030},
	abstract = {It is described how the particular algorithm (as well as similar model-building algorithms) may be augmented by dynamic curiosity and boredom in a natural manner by introducing (delayed) reinforcement for actions that increase the model network's knowledge about the world. This paper introduces a framework for`curious neural controllers' which employ an adaptive world model for goal directed on-line learning. First an on-line reinforcement learning algorithm for autonomousànimats' is described. The algorithm is based on two fully recurrent`self-supervised' continually running networks which learn in parallel. One of the networks learns to represent a complete model of the environmental dynamics and is called thèmodel network'. It provides completècredit assignment paths' into the past for the second network which controls the animats physical actions in a possibly reactive environment. The an-imats goal is to maximize cumulative reinforcement and minimize cumulativèpain'. The algorithm has properties which allow to implement something like the desire to improve the model network's knowledge about the world. This is related to curiosity. It is described how the particular algorithm (as well as similar model-building algorithms) may be augmented by dynamic curiosity and boredom in a natural manner. This may be done by introducing (delayed) reinforcement for actions that increase the model network's knowledge about the world. This in turn requires the model network to model its own ignorance, thus showing a rudimentary form of self-introspective behavior.},
	author = {Schmidhuber, J.},
	year = {1991},
	file = {Submitted Version:C\:\\Users\\user\\Zotero\\storage\\7SP2N64Y\\Schmidhuber - 1991 - A possibility for implementing curiosity and bored.pdf:application/pdf},
}

@book{andreae_thinking_1977,
	title = {Thinking with the {Teachable} {Machine}},
	isbn = {978-0-12-060050-2},
	language = {en},
	publisher = {Academic Press},
	author = {Andreae, John Hugh},
	year = {1977},
	note = {Google-Books-ID: 4cdfAAAAMAAJ},
}

@article{oudeyer_discovering_2006,
	title = {Discovering communication},
	volume = {18},
	issn = {0954-0091},
	url = {https://doi.org/10.1080/09540090600768567},
	doi = {10.1080/09540090600768567},
	abstract = {What kind of motivation drives child language development? This article presents a computational model and a robotic experiment to articulate the hypothesis that children discover communication as a result of exploring and playing with their environment. The considered robotic agent is intrinsically motivated towards situations in which it optimally progresses in learning. To experience optimal learning progress, it must avoid situations already familiar but also situations where nothing can be learned. The robot is placed in an environment in which both communicating and non-communicating objects are present. As a consequence of its intrinsic motivation, the robot explores this environment in an organized manner focussing first on non-communicative activities and then discovering the learning potential of certain types of interactive behavior. In this experiment, the agent ends up being interested by communication through vocal interactions without having a specific drive for communication.},
	number = {2},
	urldate = {2021-11-24},
	journal = {Connection Science},
	author = {Oudeyer, Pierre-Yves and Kaplan, Frédéric},
	month = jun,
	year = {2006},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/09540090600768567},
	keywords = {Communication, Development, Intrinsic motivation, Robotics, Stages, Vocalizations},
	pages = {189--206},
}

@book{oller_emergence_2000,
	address = {Mahwah, NJ, US},
	series = {The emergence of the speech capacity},
	title = {The emergence of the speech capacity},
	isbn = {978-0-8058-2628-9 978-0-8058-2629-6},
	abstract = {Recent studies of vocal development in infants have refocused on how the speech capacity is founded and how it may have evolved in the human species. Vocalizations in the very first months of life appear to provide previously unrecognized clues to the earliest steps in the process by which language came to exist and the processes by which communicative disorders arise. The author argues that infant "protophones," precursors to speech, are best interpreted in the context of an infrastructural (IS) model of speech. The model details the manner in which well-formed speech units are constructed, and it reveals how infant vocalizations mature through the first months of life by increasingly adhering to the rules of well-formed speech. The author lays out many advantages of an IS approach. IS interpretation illuminates the significance of vocal stages and highlights clinically significant deviations, such as the delays in vocal development that occur in deaf infants. An IS approach also specifies potential paths of evolution for vocal communicative systems. IS properties and principles of potential communicative systems prove to be organized according to a natural logic. An IS analysis also provides a stable basis for comparisons across species. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Lawrence Erlbaum Associates Publishers},
	author = {Oller, D. Kimbrough},
	year = {2000},
	note = {Pages: xvii, 428},
	keywords = {Infant Development, Infant Vocalization, Models, Speech Development},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2021-11-24},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7540
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computer science
Subject\_term\_id: computer-science},
	keywords = {Computer science},
	pages = {529--533},
}

@article{dobzhansky_nothing_1973,
	title = {Nothing in {Biology} {Makes} {Sense} except in the {Light} of {Evolution}},
	volume = {35},
	issn = {0002-7685},
	url = {https://doi.org/10.2307/4444260},
	doi = {10.2307/4444260},
	number = {3},
	urldate = {2021-11-24},
	journal = {The American Biology Teacher},
	author = {Dobzhansky, Theodosius},
	month = mar,
	year = {1973},
	pages = {125--129},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\3H45UY23\\Nothing-in-Biology-Makes-Sense-except-in-the-Light.html:text/html;Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\VH5MGEVI\\Dobzhansky - 1973 - Nothing in Biology Makes Sense except in the Light.pdf:application/pdf},
}

@inproceedings{stout_competence_2010,
	title = {Competence progress intrinsic motivation},
	doi = {10.1109/DEVLRN.2010.5578835},
	abstract = {One important role of an agent's motivational system is to choose, at any given moment, which of a number of skills the agent should attempt to improve. Many researchers have suggested “intrinsically motivated” systems that receive internal reward for model learning progress, but for the most part this notion has not been applied with respect to skill competence, or to choose between skills. In this paper we propose an agent motivated to gain competence in its environment by learning a number of skills, addressing head-on the mechanism of competence progress motivation for the purpose of governing the efficient learning of skills. We demonstrate this new approach in a simple illustrative domain and show that it outperforms a naïve agent, achieving higher competence faster by focusing attention and learning effort on skills for which progress can be made while ignoring those skills that are already learned or are at the moment too difficult.},
	booktitle = {2010 {IEEE} 9th {International} {Conference} on {Development} and {Learning}},
	author = {Stout, Andrew and Barto, Andrew G.},
	month = aug,
	year = {2010},
	note = {ISSN: 2161-9476},
	keywords = {Conferences, Equations, Knowledge based systems, Learning, Markov processes, Proposals, Robots},
	pages = {257--262},
}

@article{pere_unsupervised_2018,
	title = {Unsupervised {Learning} of {Goal} {Spaces} for {Intrinsically} {Motivated} {Goal} {Exploration}},
	url = {http://arxiv.org/abs/1803.00781},
	abstract = {Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a speciﬁcally engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: ﬁrst, in a perceptual learning stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments where a simulated robot arm interacts with an object, and we show that exploration algorithms using such learned representations can match the performance obtained using engineered representations.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1803.00781 [cs]},
	author = {Péré, Alexandre and Forestier, Sébastien and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.00781},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Péré et al. - 2018 - Unsupervised Learning of Goal Spaces for Intrinsic.pdf:C\:\\Users\\user\\Zotero\\storage\\FZCIX9LY\\Péré et al. - 2018 - Unsupervised Learning of Goal Spaces for Intrinsic.pdf:application/pdf},
}

@inproceedings{forestier_modular_2016,
	address = {Daejeon, South Korea},
	title = {Modular active curiosity-driven discovery of tool use},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759584/},
	doi = {10.1109/IROS.2016.7759584},
	abstract = {This article studies algorithms used by a learner to explore high-dimensional structured sensorimotor spaces such as in tool use discovery. In particular, we consider goal babbling architectures that were designed to explore and learn solutions to ﬁelds of sensorimotor problems, i.e. to acquire inverse models mapping a space of parameterized sensorimotor problems/effects to a corresponding space of parameterized motor primitives. However, so far these architectures have not been used in high-dimensional spaces of effects. Here, we show the limits of existing goal babbling architectures for efﬁcient exploration in such spaces, and introduce a novel exploration architecture called Model Babbling (MB). MB exploits efﬁciently a modular representation of the space of parameterized problems/effects. We also study an active version of Model Babbling (the MACOB architecture). These architectures are compared in a simulated experimental setup with an arm that can discover and learn how to move objects using two tools with different properties, embedding structured high-dimensional continuous motor and sensory spaces.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Forestier, Sebastien and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2016},
	pages = {3965--3972},
	file = {Forestier and Oudeyer - 2016 - Modular active curiosity-driven discovery of tool .pdf:C\:\\Users\\user\\Zotero\\storage\\RT8EK2GQ\\Forestier and Oudeyer - 2016 - Modular active curiosity-driven discovery of tool .pdf:application/pdf},
}

@article{coenen_asking_2019,
	title = {Asking the right questions about the psychology of human inquiry: {Nine} open challenges},
	volume = {26},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Asking the right questions about the psychology of human inquiry},
	url = {http://link.springer.com/10.3758/s13423-018-1470-5},
	doi = {10.3758/s13423-018-1470-5},
	abstract = {The ability to act on the world with the goal of gaining information is core to human adaptability and intelligence. Perhaps the most successful and influential account of such abilities is the Optimal Experiment Design (OED) hypothesis, which argues that humans intuitively perform experiments on the world similar to the way an effective scientist plans an experiment. The widespread application of this theory within many areas of psychology calls for a critical evaluation of the theory’s core claims. Despite many successes, we argue that the OED hypothesis remains lacking as a theory of human inquiry and that research in the area often fails to confront some of the most interesting and important questions. In this critical review, we raise and discuss nine open questions about the psychology of human inquiry.},
	language = {en},
	number = {5},
	urldate = {2020-12-02},
	journal = {Psychonomic Bulletin \& Review},
	author = {Coenen, Anna and Nelson, Jonathan D. and Gureckis, Todd M.},
	month = oct,
	year = {2019},
	pages = {1548--1587},
	file = {Coenen et al. - 2019 - Asking the right questions about the psychology of.pdf:C\:\\Users\\user\\Zotero\\storage\\DVWM2JFS\\Coenen et al. - 2019 - Asking the right questions about the psychology of.pdf:application/pdf},
}

@article{oudeyer_what_2007,
	title = {What is intrinsic motivation? {A} typology of computational approaches},
	volume = {1},
	issn = {1662-5218},
	shorttitle = {What is intrinsic motivation?},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.12.006.2007/abstract},
	doi = {10.3389/neuro.12.006.2007},
	urldate = {2020-12-02},
	journal = {Frontiers in Neurorobotics},
	author = {Oudeyer, Pierre-Yves},
	year = {2007},
	file = {Full Text:C\:\\Users\\user\\Zotero\\storage\\R84RGPJK\\Oudeyer - 2007 - What is intrinsic motivation A typology of comput.pdf:application/pdf},
}

@article{nair_visual_2018,
	title = {Visual {Reinforcement} {Learning} with {Imagined} {Goals}},
	url = {http://arxiv.org/abs/1807.04742},
	abstract = {For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.},
	urldate = {2020-11-18},
	journal = {arXiv:1807.04742 [cs, stat]},
	author = {Nair, Ashvin and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
	month = dec,
	year = {2018},
	note = {arXiv: 1807.04742},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\J6DJ4GAC\\1807.html:text/html},
}

@inproceedings{eysenbach_diversity_2019,
	title = {Diversity is {All} {You} {Need}: {Learning} {Skills} without a {Reward} {Function}},
	url = {https://openreview.net/forum?id=SJx63jRqFm},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	year = {2019},
}

@article{stadie_incentivizing_2015,
	title = {Incentivizing exploration in reinforcement learning with deep predictive models},
	journal = {arXiv preprint arXiv:1507.00814},
	author = {Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
	year = {2015},
}

@article{held_automatic_2017,
	title = {Automatic {Goal} {Generation} for {Reinforcement} {Learning} {Agents}},
	url = {http://arxiv.org/abs/1705.06366},
	abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
	urldate = {2018-01-31},
	author = {Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06366},
}

@incollection{andersen_active_2017,
	title = {Active {Exploration} for {Learning} {Symbolic} {Representations}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Andersen, Garrett and Konidaris, George},
	editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
	year = {2017},
	pages = {5016--5026},
}

@incollection{houthooft_vime_2016,
	title = {{VIME}: {Variational} {Information} {Maximizing} {Exploration}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Houthooft, Rein and Chen, Xi and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	editor = {Lee, D D and Sugiyama, M and Luxburg, U V and Guyon, I and Garnett, R},
	year = {2016},
	pages = {1109--1117},
}

@article{achiam_surprise-based_2017,
	title = {Surprise-based intrinsic motivation for deep reinforcement learning},
	journal = {arXiv preprint arXiv:1703.01732},
	author = {Achiam, Joshua and Sastry, Shankar},
	year = {2017},
}

@inproceedings{jaques_social_2019,
	title = {Social {Influence} as {Intrinsic} {Motivation} for {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=B1lG42C9Km},
	abstract = {We derive a new intrinsic social motivation for multi-agent reinforcement learning (MARL), in which agents are rewarded for having causal influence over another agent's actions. Causal influence is assessed using counterfactual reasoning. The reward does not depend on observing another agent's reward function, and is thus a more realistic approach to MARL than taken in previous work. We show that the causal influence reward is related to maximizing the mutual information between agents' actions. We test the approach in challenging social dilemma environments, where it consistently leads to enhanced cooperation between agents and higher collective reward. Moreover, we find that rewarding influence can lead agents to develop emergent communication protocols. We therefore employ influence to train agents to use an explicit communication channel, and find that it leads to more effective communication and higher collective reward. Finally, we show that influence can be computed by equipping each agent with an internal model that predicts the actions of other agents. This allows the social influence reward to be computed without the use of a centralised controller, and as such represents a significantly more general and scalable inductive bias for MARL with independent agents.},
	booktitle = {Proceedings of the 35 th {International} {Conference} on {Machine} {Learning}, {Stockholm}, {Sweden}},
	author = {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro A. and Strouse, DJ and Leibo, Joel Z. and de Freitas, Nando},
	year = {2019},
	note = {arXiv: 1810.08647},
}

@inproceedings{bellemare_unifying_2016,
	title = {Unifying count-based exploration and intrinsic motivation},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	year = {2016},
	pages = {1471--1479},
	file = {Bellemare et al. - 2016 - Unifying count-based exploration and intrinsic mot.pdf:C\:\\Users\\user\\Zotero\\storage\\XV8QXLQ6\\Bellemare et al. - 2016 - Unifying count-based exploration and intrinsic mot.pdf:application/pdf},
}

@article{pong_skew-fit_2020,
	title = {Skew-{Fit}: {State}-{Covering} {Self}-{Supervised} {Reinforcement} {Learning}},
	shorttitle = {Skew-{Fit}},
	url = {http://arxiv.org/abs/1903.03698},
	abstract = {Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. We prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks. Moreover, we demonstrate that {\textbackslash}METHOD enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.},
	urldate = {2020-05-15},
	journal = {arXiv:1903.03698 [cs, stat]},
	author = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
	year = {2020},
	note = {arXiv: 1903.03698},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\4VRSFGCG\\1903.html:text/html},
}

@article{cully_robots_2015,
	title = {Robots that can adapt like animals},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14422},
	doi = {10.1038/nature14422},
	abstract = {An intelligent trial-and-error learning algorithm is presented that allows robots to adapt in minutes to compensate for a wide variety of types of damage.},
	language = {en},
	number = {7553},
	urldate = {2020-11-18},
	journal = {Nature},
	author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	pages = {503--507},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\K6ZEGTCB\\nature14422.html:text/html},
}

@article{osband_optimistic_2017,
	title = {On {Optimistic} versus {Randomized} {Exploration} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1706.04241},
	abstract = {We discuss the relative merits of optimistic and randomized approaches to exploration in reinforcement learning. Optimistic approaches presented in the literature apply an optimistic boost to the value estimate at each state-action pair and select actions that are greedy with respect to the resulting optimistic value function. Randomized approaches sample from among statistically plausible value functions and select actions that are greedy with respect to the random sample. Prior computational experience suggests that randomized approaches can lead to far more statistically efficient learning. We present two simple analytic examples that elucidate why this is the case. In principle, there should be optimistic approaches that fare well relative to randomized approaches, but that would require intractable computation. Optimistic approaches that have been proposed in the literature sacrifice statistical efficiency for the sake of computational efficiency. Randomized approaches, on the other hand, may enable simultaneous statistical and computational efficiency.},
	urldate = {2018-01-30},
	author = {Osband, Ian and Van Roy, Benjamin},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04241},
}

@inproceedings{alet_meta-learning_2020,
	title = {Meta-learning curiosity algorithms},
	url = {https://openreview.net/forum?id=BygdyxHFDS},
	abstract = {We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high...},
	urldate = {2020-04-21},
	author = {Alet, Ferran and Schneider, Martin F. and Lozano-Perez, Tomas and Kaelbling, Leslie Pack},
	year = {2020},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\XJXY7KNY\\forum.html:text/html},
}

@inproceedings{rahaman_learning_2019,
	title = {Learning the {Arrow} of {Time} for {Problems} in {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=rylJkpEtwS},
	abstract = {We learn the arrow of time for MDPs and use it to measure reachability, detect side-effects and obtain a curiosity reward signal.},
	language = {en},
	urldate = {2020-11-18},
	author = {Rahaman, Nasim and Wolf, Steffen and Goyal, Anirudh and Remme, Roman and Bengio, Yoshua},
	month = sep,
	year = {2019},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\FSMI4M2D\\forum.html:text/html},
}

@inproceedings{chitnis_intrinsic_2020,
	title = {Intrinsic {Motivation} for {Encouraging} {Synergistic} {Behavior}},
	url = {https://openreview.net/forum?id=SJleNCNtDH},
	abstract = {We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a...},
	urldate = {2020-04-22},
	author = {Chitnis, Rohan and Tulsiani, Shubham and Gupta, Saurabh and Gupta, Abhinav},
	year = {2020},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\6VNCE5NE\\forum.html:text/html},
}

@article{sukhbaatar_intrinsic_2017,
	title = {Intrinsic {Motivation} and {Automatic} {Curricula} via {Asymmetric} {Self}-{Play}},
	url = {http://arxiv.org/abs/1703.05407},
	abstract = {We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.},
	urldate = {2018-02-13},
	author = {Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05407},
}

@article{kulkarni_hierarchical_2016,
	title = {Hierarchical deep reinforcement learning: {Integrating} temporal abstraction and intrinsic motivation},
	journal = {arXiv preprint arXiv:1604.06057},
	author = {Kulkarni, Tejas D and Narasimhan, Karthik R and Saeedi, Ardavan and Tenenbaum, Joshua B},
	year = {2016},
}

@inproceedings{fu_ex2_2017,
	title = {Ex2: {Exploration} with exemplar models for deep reinforcement learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Fu, Justin and Co-Reyes, John and Levine, Sergey},
	year = {2017},
	pages = {2574--2584},
}

@article{singh_intrinsically_2010,
	title = {Intrinsically motivated reinforcement learning: {An} evolutionary perspective},
	volume = {2},
	number = {2},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg, Jonathan},
	year = {2010},
	note = {Publisher: IEEE},
	pages = {70--82},
}

@incollection{salge_empowermentintroduction_2014,
	title = {Empowerment–{An} {Introduction}},
	url = {http://link.springer.com/10.1007/978-3-642-53734-9_4},
	urldate = {2018-01-31},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Salge, Christoph and Glackin, Cornelius and Polani, Daniel},
	year = {2014},
	doi = {10.1007/978-3-642-53734-9_4},
	pages = {67--114},
}

@inproceedings{baker_emergent_2020,
	title = {Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}},
	url = {https://openreview.net/forum?id=SkxpxJBKwS},
	abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
	urldate = {2020-05-18},
	author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
	year = {2020},
	note = {tex.ids: Baker2019
arXiv: 1909.07528},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\VHQTIFLG\\forum.html:text/html},
}

@article{doncieux_dream_2020,
	title = {{DREAM} {Architecture}: a {Developmental} {Approach} to {Open}-{Ended} {Learning} in {Robotics}},
	shorttitle = {{DREAM} {Architecture}},
	url = {http://arxiv.org/abs/2005.06223},
	abstract = {Robots are still limited to controlled conditions, that the robot designer knows with enough details to endow the robot with the appropriate models or behaviors. Learning algorithms add some flexibility with the ability to discover the appropriate behavior given either some demonstrations or a reward to guide its exploration with a reinforcement learning algorithm. Reinforcement learning algorithms rely on the definition of state and action spaces that define reachable behaviors. Their adaptation capability critically depends on the representations of these spaces: small and discrete spaces result in fast learning while large and continuous spaces are challenging and either require a long training period or prevent the robot from converging to an appropriate behavior. Beside the operational cycle of policy execution and the learning cycle, which works at a slower time scale to acquire new policies, we introduce the redescription cycle, a third cycle working at an even slower time scale to generate or adapt the required representations to the robot, its environment and the task. We introduce the challenges raised by this cycle and we present DREAM (Deferred Restructuring of Experience in Autonomous Machines), a developmental cognitive architecture to bootstrap this redescription process stage by stage, build new state representations with appropriate motivations, and transfer the acquired knowledge across domains or tasks or even across robots. We describe results obtained so far with this approach and end up with a discussion of the questions it raises in Neuroscience.},
	urldate = {2020-06-14},
	journal = {arXiv:2005.06223 [cs]},
	author = {Doncieux, Stephane and Bredeche, Nicolas and Goff, Léni Le and Girard, Benoît and Coninx, Alexandre and Sigaud, Olivier and Khamassi, Mehdi and Díaz-Rodríguez, Natalia and Filliat, David and Hospedales, Timothy and Eiben, A. and Duro, Richard},
	month = may,
	year = {2020},
	note = {arXiv: 2005.06223},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Robotics, Cognitive architecture, Developmental learning, Intrinsic motivation, Open-ended, Transfer learning},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\JADR8JTR\\2005.html:text/html},
}

@inproceedings{pathak_curiosity-driven_2017,
	title = {Curiosity-driven exploration by self-supervised prediction},
	volume = {2017},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
	year = {2017},
}

@article{blaes_control_2019,
	title = {Control {What} {You} {Can}: {Intrinsically} {Motivated} {Task}-{Planning} {Agent}},
	volume = {32},
	shorttitle = {Control {What} {You} {Can}},
	url = {https://papers.nips.cc/paper/2019/hash/b6f97e6f0fd175613910d613d574d0cb-Abstract.html},
	language = {en},
	urldate = {2020-11-18},
	journal = {Advances in Neural Information Processing Systems},
	author = {Blaes, Sebastian and Vlastelica Pogančić, Marin and Zhu, Jiajie and Martius, Georg},
	year = {2019},
	pages = {12541--12552},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\CYVUD49R\\b6f97e6f0fd175613910d613d574d0cb-Abstract.html:text/html},
}

@article{graves_automated_2017,
	title = {Automated curriculum learning for neural networks},
	journal = {arXiv preprint arXiv:1704.03003},
	author = {Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
	year = {2017},
}

@incollection{tang_exploration_2017,
	title = {\#{Exploration}: {A} {Study} of {Count}-{Based} {Exploration} for {Deep} {Reinforcement} {Learning}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
	editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
	year = {2017},
	pages = {2750--2759},
}

@inproceedings{osband_deep_2016,
	title = {Deep exploration via bootstrapped {DQN}},
	abstract = {fficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games},
	booktitle = {Advances in neural information processing systems},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	year = {2016},
	pages = {4026--4034},
}

@article{cully_autonomous_2019,
	title = {Autonomous skill discovery with {Quality}-{Diversity} and {Unsupervised} {Descriptors}},
	url = {http://arxiv.org/abs/1905.11874},
	doi = {10.1145/3321707.3321804},
	abstract = {Quality-Diversity optimization is a new family of optimization algorithms that, instead of searching for a single optimal solution to solving a task, searches for a large collection of solutions that all solve the task in a different way. This approach is particularly promising for learning behavioral repertoires in robotics, as such a diversity of behaviors enables robots to be more versatile and resilient. However, these algorithms require the user to manually define behavioral descriptors, which is used to determine whether two solutions are different or similar. The choice of a behavioral descriptor is crucial, as it completely changes the solution types that the algorithm derives. In this paper, we introduce a new method to automatically define this descriptor by combining Quality-Diversity algorithms with unsupervised dimensionality reduction algorithms. This approach enables robots to autonomously discover the range of their capabilities while interacting with their environment. The results from two experimental scenarios demonstrate that robot can autonomously discover a large range of possible behaviors, without any prior knowledge about their morphology and environment. Furthermore, these behaviors are deemed to be similar to handcrafted solutions that uses domain knowledge and significantly more diverse than when using existing unsupervised methods.},
	urldate = {2020-11-18},
	journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
	author = {Cully, Antoine},
	month = jul,
	year = {2019},
	note = {arXiv: 1905.11874},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	pages = {81--89},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\9DRXLUTD\\1905.html:text/html},
}

@article{etcheverry_hierarchically_2021,
	title = {Hierarchically {Organized} {Latent} {Modules} for {Exploratory} {Search} in {Morphogenetic} {Systems}},
	url = {http://arxiv.org/abs/2007.01195},
	abstract = {Self-organization of complex morphological patterns from local interactions is a fascinating phenomenon in many natural and artificial systems. In the artificial world, typical examples of such morphogenetic systems are cellular automata. Yet, their mechanisms are often very hard to grasp and so far scientific discoveries of novel patterns have primarily been relying on manual tuning and ad hoc exploratory search. The problem of automated diversity-driven discovery in these systems was recently introduced [26, 62], highlighting that two key ingredients are autonomous exploration and unsupervised representation learning to describe "relevant" degrees of variations in the patterns. In this paper, we motivate the need for what we call Meta-diversity search, arguing that there is not a unique ground truth interesting diversity as it strongly depends on the final observer and its motives. Using a continuous game-of-life system for experiments, we provide empirical evidences that relying on monolithic architectures for the behavioral embedding design tends to bias the final discoveries (both for hand-defined and unsupervisedly-learned features) which are unlikely to be aligned with the interest of a final end-user. To address these issues, we introduce a novel dynamic and modular architecture that enables unsupervised learning of a hierarchy of diverse representations. Combined with intrinsically motivated goal exploration algorithms, we show that this system forms a discovery assistant that can efficiently adapt its diversity search towards preferences of a user using only a very small amount of user feedback.},
	urldate = {2021-10-31},
	journal = {arXiv:2007.01195 [nlin, stat]},
	author = {Etcheverry, Mayalen and Moulin-Frier, Clément and Oudeyer, Pierre-Yves},
	month = sep,
	year = {2021},
	note = {arXiv: 2007.01195},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Nonlinear Sciences - Cellular Automata and Lattice Gases},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\7PHZIMRW\\Etcheverry et al. - 2021 - Hierarchically Organized Latent Modules for Explor.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\QSAPWDVG\\2007.html:text/html},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	number = {6245},
	urldate = {2021-11-03},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {255--260},
}

@article{averbeck_theory_2015,
	title = {Theory of {Choice} in {Bandit}, {Information} {Sampling} and {Foraging} {Tasks}},
	volume = {11},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004164},
	doi = {10.1371/journal.pcbi.1004164},
	abstract = {Decision making has been studied with a wide array of tasks. Here we examine the theoretical structure of bandit, information sampling and foraging tasks. These tasks move beyond tasks where the choice in the current trial does not affect future expected rewards. We have modeled these tasks using Markov decision processes (MDPs). MDPs provide a general framework for modeling tasks in which decisions affect the information on which future choices will be made. Under the assumption that agents are maximizing expected rewards, MDPs provide normative solutions. We find that all three classes of tasks pose choices among actions which trade-off immediate and future expected rewards. The tasks drive these trade-offs in unique ways, however. For bandit and information sampling tasks, increasing uncertainty or the time horizon shifts value to actions that pay-off in the future. Correspondingly, decreasing uncertainty increases the relative value of actions that pay-off immediately. For foraging tasks the time-horizon plays the dominant role, as choices do not affect future uncertainty in these tasks.},
	language = {en},
	number = {3},
	urldate = {2021-11-03},
	journal = {PLOS Computational Biology},
	author = {Averbeck, Bruno B.},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Learning, Algorithms, Decision making, Foraging, Markov models, Markov processes, Polynomials, Probability distribution},
	pages = {e1004164},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\A3BTA2WD\\Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling a.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\A9XBI3I8\\article.html:text/html},
}

@inproceedings{thrun_lifelong_1994,
	title = {A lifelong learning perspective for mobile robot control},
	volume = {1},
	doi = {10.1109/IROS.1994.407413},
	abstract = {Designing robots that learn by themselves to perform complex real-world tasks is a still-open challenge for the field of robotics and artificial intelligence. In this paper the author presents the robot learning problem as a lifelong problem, in which a robot faces a collection of tasks over its entire lifetime. Such a scenario provides the opportunity to gather general-purpose knowledge that transfers across tasks. The author illustrates a particular leaning mechanism, explanation-based neural network learning, that transfers knowledge between related tasks via neural network action models. The learning approach is illustrated using a mobile robot, equipped with visual, ultrasonic and laser sensors. In less than 10 minutes operation time, the robot is able to learn to navigate to a marked target object in a natural office environment.{\textless}{\textgreater}},
	booktitle = {Proceedings of {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS}'94)},
	author = {Thrun, S.},
	month = sep,
	year = {1994},
	keywords = {Learning, Manipulator dynamics, Computational complexity, Hardware, Intelligent robots, Mobile robots, Robot control, Robot kinematics, Robot sensing systems, Sensor phenomena and characterization},
	pages = {23--30 vol.1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\user\\Zotero\\storage\\QKUAD5W3\\407413.html:text/html},
}

@inproceedings{cohn_active_1995,
	title = {Active {Learning} with {Statistical} {Models}},
	volume = {7},
	url = {https://proceedings.neurips.cc/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Cohn, David and Ghahramani, Zoubin and Jordan, Michael},
	editor = {Tesauro, G. and Touretzky, D. and Leen, T.},
	year = {1995},
}

@misc{noauthor_180205054_nodate,
	title = {[1802.05054] {GEP}-{PG}: {Decoupling} {Exploration} and {Exploitation} in {Deep} {Reinforcement} {Learning} {Algorithms}},
	url = {https://arxiv.org/abs/1802.05054#},
	urldate = {2021-11-03},
	file = {[1802.05054] GEP-PG\: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms:C\:\\Users\\user\\Zotero\\storage\\BE8X8W9C\\1802.html:text/html},
}

@article{colas_gep-pg_2018,
	title = {{GEP}-{PG}: {Decoupling} {Exploration} and {Exploitation} in {Deep} {Reinforcement} {Learning} {Algorithms}},
	shorttitle = {{GEP}-{PG}},
	url = {http://arxiv.org/abs/1802.05054},
	abstract = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments. Supplementary videos and discussion can be found at http://frama.link/gep\_pg, the code at http://github.com/flowersteam/geppg.},
	urldate = {2021-11-03},
	journal = {arXiv:1802.05054 [cs]},
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	month = sep,
	year = {2018},
	note = {arXiv: 1802.05054},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\5B95X8BG\\Colas et al. - 2018 - GEP-PG Decoupling Exploration and Exploitation in.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\JXV4E9H9\\1802.html:text/html},
}

@article{santucci_which_2013,
	title = {Which is the best intrinsic motivation signal for learning multiple skills?},
	volume = {7},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2013.00022},
	doi = {10.3389/fnbot.2013.00022},
	abstract = {Humans and other biological agents are able to autonomously learn and cache different skills in the absence of any biological pressure or any assigned task. In this respect, Intrinsic Motivations (i.e., motivations not connected to reward-related stimuli) play a cardinal role in animal learning, and can be considered as a fundamental tool for developing more autonomous and more adaptive artificial agents. In this work, we provide an exhaustive analysis of a scarcely investigated problem: which kind of IM reinforcement signal is the most suitable for driving the acquisition of multiple skills in the shortest time? To this purpose we implemented an artificial agent with a hierarchical architecture that allows to learn and cache different skills. We tested the system in a setup with continuous states and actions, in particular, with a kinematic robotic arm that has to learn different reaching tasks. We compare the results of different versions of the system driven by several different intrinsic motivation signals. The results show (a) that intrinsic reinforcements purely based on the knowledge of the system are not appropriate to guide the acquisition of multiple skills, and (b) that the stronger the link between the IM signal and the competence of the system, the better the performance.},
	urldate = {2021-11-11},
	journal = {Frontiers in Neurorobotics},
	author = {Santucci, Vieri and Baldassarre, Gianluca and Mirolli, Marco},
	year = {2013},
	pages = {22},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\S5FS9W34\\Santucci et al. - 2013 - Which is the best intrinsic motivation signal for .pdf:application/pdf},
}

@incollection{sun_bayesian_2008,
	address = {Cambridge},
	series = {Cambridge {Handbooks} in {Psychology}},
	title = {Bayesian {Models} of {Cognition}},
	isbn = {978-0-521-85741-3},
	url = {https://www.cambridge.org/core/books/cambridge-handbook-of-computational-psychology/bayesian-models-of-cognition/58B8B762EEA8AB340140D9B98A83090B},
	abstract = {The chapter focuses on problems in higher-level cognition: inferring causal structure from patterns of statistical correlation, learning about categories and hidden properties of objects, and learning the meanings of words. This chapter discusses the basic principles that underlie Bayesian models of cognition and several advanced techniques for probabilistic modeling and inference coming out of recent work in computer science and statistics. The first step is to summarize the logic of Bayesian inference based on probabilistic models. A discussion is then provided of three recent innovations that make it easier to define and use probabilistic models of complex domains: graphical models, hierarchical Bayesian models, and Markov chain Monte Carlo. The central ideas behind each of these techniques is illustrated by considering a detailed cognitive modeling application, drawn from causal learning, property induction, and language modeling, respectively.},
	urldate = {2021-11-13},
	booktitle = {The {Cambridge} {Handbook} of {Computational} {Psychology}},
	publisher = {Cambridge University Press},
	author = {Griffiths, Thomas L. and Kemp, Charles and Tenenbaum, Joshua B.},
	editor = {Sun, Ron},
	year = {2008},
	doi = {10.1017/CBO9780511816772.006},
	pages = {59--100},
	file = {Griﬃths et al. - Bayesian models of cognition.pdf:C\:\\Users\\user\\Zotero\\storage\\UJ299MXI\\Griﬃths et al. - Bayesian models of cognition.pdf:application/pdf;Submitted Version:C\:\\Users\\user\\Zotero\\storage\\5HCHSC24\\Griffiths et al. - 2008 - Bayesian Models of Cognition.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\XW63KQ95\\58B8B762EEA8AB340140D9B98A83090B.html:text/html},
}

@article{griffiths_how_2012,
	title = {How the {Bayesians} got their beliefs (and what those beliefs actually are): {Comment} on {Bowers} and {Davis} (2012).},
	volume = {138},
	issn = {1939-1455, 0033-2909},
	shorttitle = {How the {Bayesians} got their beliefs (and what those beliefs actually are)},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0026884},
	doi = {10.1037/a0026884},
	abstract = {Bowers and Davis (2012) criticize Bayesian modelers for telling “just so” stories about cognition and neuroscience. Their criticisms are weakened by not giving an accurate characterization of the motivation behind Bayesian modeling or the ways in which Bayesian models are used and by not evaluating this theoretical framework against specific alternatives. We address these points by clarifying our beliefs about the goals and status of Bayesian models and by identifying what we view as the unique merits of the Bayesian approach.},
	language = {en},
	number = {3},
	urldate = {2021-11-13},
	journal = {Psychological Bulletin},
	author = {Griffiths, Thomas L. and Chater, Nick and Norris, Dennis and Pouget, Alexandre},
	year = {2012},
	pages = {415--422},
	file = {Griffiths et al. - 2012 - How the Bayesians got their beliefs (and what thos.pdf:C\:\\Users\\user\\Zotero\\storage\\WUJHFVJW\\Griffiths et al. - 2012 - How the Bayesians got their beliefs (and what thos.pdf:application/pdf},
}

@article{perfors_bayesian_2012,
	title = {Bayesian {Models} of {Cognition}: {What}'s {Built} in {After} {All}?: {Bayesian} {Models} of {Cognition}},
	volume = {7},
	issn = {17479991},
	shorttitle = {Bayesian {Models} of {Cognition}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2011.00467.x},
	doi = {10.1111/j.1747-9991.2011.00467.x},
	abstract = {This article explores some of the philosophical implications of the Bayesian modeling paradigm. In particular, it focuses on the ramiﬁcations of the fact that Bayesian models pre-specify an inbuilt hypothesis space. To what extent does this pre-speciﬁcation correspond to simply ‘‘building the solution in’’? I argue that any learner (whether computer or human) must have a built-in hypothesis space in precisely the same sense that Bayesian models have one. This has implications for the nature of learning, Fodor’s puzzle of concept acquisition, and the role of modeling in cognitive science.},
	language = {en},
	number = {2},
	urldate = {2021-11-13},
	journal = {Philosophy Compass},
	author = {Perfors, Amy},
	month = feb,
	year = {2012},
	pages = {127--138},
	file = {Perfors - 2012 - Bayesian Models of Cognition What's Built in Afte.pdf:C\:\\Users\\user\\Zotero\\storage\\LEQZ3K94\\Perfors - 2012 - Bayesian Models of Cognition What's Built in Afte.pdf:application/pdf},
}

@article{baranes_r-iac_2009,
	title = {R-{IAC}: {Robust} {Intrinsically} {Motivated} {Exploration} and {Active} {Learning}},
	volume = {1},
	issn = {1943-0612},
	shorttitle = {R-{IAC}},
	doi = {10.1109/TAMD.2009.2037513},
	abstract = {Intelligent adaptive curiosity (IAC) was initially introduced as a developmental mechanism allowing a robot to self-organize developmental trajectories of increasing complexity without preprogramming the particular developmental stages. In this paper, we argue that IAC and other intrinsically motivated learning heuristics could be viewed as active learning algorithms that are particularly suited for learning forward models in unprepared sensorimotor spaces with large unlearnable subspaces. Then, we introduce a novel formulation of IAC, called robust intelligent adaptive curiosity (R-IAC), and show that its performances as an intrinsically motivated active learning algorithm are far superior to IAC in a complex sensorimotor space where only a small subspace is neither unlearnable nor trivial. We also show results in which the learnt forward model is reused in a control scheme. Finally, an open source accompanying software containing these algorithms as well as tools to reproduce all the experiments presented in this paper is made publicly available.},
	number = {3},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Baranes, Adrien and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Autonomous Mental Development},
	keywords = {Active learning, artificial curiosity, developmental robotics, exploration, Humans, Intelligent sensors, intrinsic motivation, Machine learning, Open source software, Orbital robotics, Psychology, Robot kinematics, Robot sensing systems, Robustness, sensorimotor learning, Software tools},
	pages = {155--169},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\user\\Zotero\\storage\\S8P9A9I9\\5342516.html:text/html},
}

@article{baranes_r-iac_2009-1,
	title = {R-{IAC}: {Robust} {Intrinsically} {Motivated} {Exploration} and {Active} {Learning}},
	volume = {1},
	issn = {1943-0612},
	shorttitle = {R-{IAC}},
	doi = {10.1109/TAMD.2009.2037513},
	abstract = {Intelligent adaptive curiosity (IAC) was initially introduced as a developmental mechanism allowing a robot to self-organize developmental trajectories of increasing complexity without preprogramming the particular developmental stages. In this paper, we argue that IAC and other intrinsically motivated learning heuristics could be viewed as active learning algorithms that are particularly suited for learning forward models in unprepared sensorimotor spaces with large unlearnable subspaces. Then, we introduce a novel formulation of IAC, called robust intelligent adaptive curiosity (R-IAC), and show that its performances as an intrinsically motivated active learning algorithm are far superior to IAC in a complex sensorimotor space where only a small subspace is neither unlearnable nor trivial. We also show results in which the learnt forward model is reused in a control scheme. Finally, an open source accompanying software containing these algorithms as well as tools to reproduce all the experiments presented in this paper is made publicly available.},
	number = {3},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Baranes, Adrien and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Autonomous Mental Development},
	keywords = {Active learning, artificial curiosity, developmental robotics, exploration, Humans, Intelligent sensors, intrinsic motivation, Machine learning, Open source software, Orbital robotics, Psychology, Robot kinematics, Robot sensing systems, Robustness, sensorimotor learning, Software tools},
	pages = {155--169},
}

@inproceedings{saegusa_active_2009,
	title = {Active motor babbling for sensorimotor learning},
	doi = {10.1109/ROBIO.2009.4913101},
	abstract = {For a complex autonomous robotic system such as a humanoid robot, motor-babbling-based sensorimotor learning is considered an effective method to develop an internal model of the self-body and the environment autonomously. In this paper, we propose a method of sensorimotor learning and evaluate it performance in active learning. The proposed model is characterized by a function we call the “confidence”, and is a measure of the reliability of state prediction and control. The confidence for the state can be a good measure to bias the next exploration strategy of data sampling, and to direct its attention to areas in the state domain less reliably predicted and controlled. We consider the confidence function to be a first step toward an active behavior design for autonomous environment adaptation. The approach was experimentally validated using the humanoid robot James.},
	booktitle = {2008 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}},
	author = {Saegusa, Ryo and Metta, Giorgio and Sandini, Giulio and Sakka, Sophie},
	month = feb,
	year = {2009},
	keywords = {Biomimetics, Cognitive robotics, confidence, humanoid robot, Humanoid robots, Inverse problems, Kinematics, Motor drives, neural networks, Predictive models, Robot sensing systems, Sampling methods, Sensor systems, sensorimotor learning, state prediction},
	pages = {794--799},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\user\\Zotero\\storage\\V8LKNUF6\\4913101.html:text/html},
}

@misc{caligiore_using_2008,
	title = {Using {Motor} {Babbling} and {Hebb} {Rules} for {Modeling} the {Development} of {Reaching} with {Obstacles} and {Grasping}},
	url = {https://www.semanticscholar.org/paper/Using-Motor-Babbling-and-Hebb-Rules-for-Modeling-of-Caligiore-Ferrauto/f7d6d07627720039c216a21adcc25c502f636dbe},
	abstract = {An influential hypothesis of developmental psychology states that, in the first months of their life, infants perform exploratory/random movements (“motor babbling”) in order to create associations between such movements and the resulting perceived effects. These associations are later used as building blocks to tackle more complex sensorimotor behaviours. Due to its underlying simplicity, motor babbling might be a learning strategy widely used in the early phases of child development. Various models of this process have been proposed that focus on the acquisition ofreaching skills based on the synchronous association between the positions of the seen hand (or grasped object) and the proprioception of the postures that cause them. This research tries to understand, on a computational basis, if the principles underlying motor babbling can be extended to the acquisition of behaviours more complex than reaching, such as the execution of nonlinear movement trajectories for avoiding obstacles or the acquisition of movements directed to grasp objects. These behaviours are challenging for motor babbling as they involve the execution of movements, or sequences of movements, in time, and so they cannot be learned on the basis of simple synchronous associations between their neural representation s and perceptive neural representations. The paper aims to show that infants might still use motor babbling for the development of these behaviours by overcoming its time-limits on the basis of complementary mechanisms such as Pattern Generators and innate reflexes. The computational viability of this hypothesis is demonstrated by testing the proposed models with a 3D simulated dynamic eye-arm-hand robot working on a plane.},
	language = {en},
	urldate = {2021-11-24},
	author = {Caligiore, D. and Ferrauto, Tomassino and Parisi, D. and Accornero, N. and Capozza, M. and Baldassarre, G.},
	year = {2008},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\HG2E9TEL\\f7d6d07627720039c216a21adcc25c502f636dbe.html:text/html},
}

@article{jaderberg_reinforcement_2016,
	title = {Reinforcement {Learning} with {Unsupervised} {Auxiliary} {Tasks}},
	url = {http://arxiv.org/abs/1611.05397},
	abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880{\textbackslash}\% expert human performance, and a challenging suite of first-person, three-dimensional {\textbackslash}emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\${\textbackslash}times\$ and averaging 87{\textbackslash}\% expert human performance on Labyrinth.},
	urldate = {2021-11-24},
	journal = {arXiv:1611.05397 [cs]},
	author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05397},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\6FST28ZU\\Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\2UR88KHD\\1611.html:text/html},
}

@article{haber_learning_2018,
	title = {Learning to {Play} with {Intrinsically}-{Motivated} {Self}-{Aware} {Agents}},
	url = {http://arxiv.org/abs/1802.07442},
	abstract = {Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a "world-model" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit "self-model" that allows the agent to track the error map of its own world-model, and then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in complex novel physical environments.},
	urldate = {2021-11-24},
	journal = {arXiv:1802.07442 [cs, stat]},
	author = {Haber, Nick and Mrowca, Damian and Fei-Fei, Li and Yamins, Daniel L. K.},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.07442},
	keywords = {68, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\X8SZNL4F\\Haber et al. - 2018 - Learning to Play with Intrinsically-Motivated Self.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\VI9QH8E5\\1802.html:text/html},
}

@article{bougie_skill-based_2019,
	title = {Skill-based curiosity for intrinsically motivated reinforcement learning},
	doi = {10.1007/s10994-019-05845-8},
	abstract = {This work proposes a novel end-to-end curiosity mechanism for deep reinforcement learning methods, that allows an agent to gradually acquire new skills and compares the performance of an augmented agent that uses the authors' curiosity reward to state-of-the-art learners. Reinforcement learning methods rely on rewards provided by the environment that are extrinsic to the agent. However, many real-world scenarios involve sparse or delayed rewards. In such cases, the agent can develop its own intrinsic reward function called curiosity to enable the agent to explore its environment in the quest of new skills. We propose a novel end-to-end curiosity mechanism for deep reinforcement learning methods, that allows an agent to gradually acquire new skills. Our method scales to high-dimensional problems, avoids the need of directly predicting the future, and, can perform in sequential decision scenarios. We formulate the curiosity as the ability of the agent to predict its own knowledge about the task. We base the prediction on the idea of skill learning to incentivize the discovery of new skills, and guide exploration towards promising solutions. To further improve data efficiency and generalization of the agent, we propose to learn a latent representation of the skills. We present a variety of sparse reward tasks in MiniGrid, MuJoCo, and Atari games. We compare the performance of an augmented agent that uses our curiosity reward to state-of-the-art learners. Experimental evaluation exhibits higher performance compared to reinforcement learning models that only learn by maximizing extrinsic rewards.},
	journal = {Machine Learning},
	author = {Bougie, Nicolas and Ichise, R.},
	year = {2019},
	file = {Full Text:C\:\\Users\\user\\Zotero\\storage\\JTV9N4CG\\Bougie and Ichise - 2019 - Skill-based curiosity for intrinsically motivated .pdf:application/pdf},
}

@article{berseth_smirl_2021,
	title = {{SMiRL}: {Surprise} {Minimizing} {Reinforcement} {Learning} in {Unstable} {Environments}},
	shorttitle = {{SMiRL}},
	url = {http://arxiv.org/abs/1912.05510},
	abstract = {Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning.},
	urldate = {2021-11-24},
	journal = {arXiv:1912.05510 [cs, stat]},
	author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
	month = feb,
	year = {2021},
	note = {arXiv: 1912.05510},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, G.3, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\ZLHYF6BF\\Berseth et al. - 2021 - SMiRL Surprise Minimizing Reinforcement Learning .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\9NEVEC9A\\1912.html:text/html},
}

@article{colas_intrinsically_2021,
	title = {Intrinsically {Motivated} {Goal}-{Conditioned} {Reinforcement} {Learning}: a {Short} {Survey}},
	shorttitle = {Intrinsically {Motivated} {Goal}-{Conditioned} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2012.09830},
	abstract = {Building autonomous machines that can explore open-ended environments, discover possible interactions and autonomously build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autonomous and intrinsically motivated learning agents that can generate, select and learn to solve their own problems. In recent years, we have seen a convergence of developmental approaches, and developmental robotics in particular, with deep reinforcement learning (RL) methods, forming the new domain of developmental machine learning. Within this new domain, we review here a set of methods where deep RL algorithms are trained to tackle the developmental robotics problem of the autonomous acquisition of open-ended repertoires of skills. Intrinsically motivated goal-conditioned RL algorithms train agents to learn to represent, generate and pursue their own goals. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions, which results in new challenges compared to traditional RL algorithms designed to tackle pre-defined sets of goals using external reward signals. This paper proposes a typology of these methods at the intersection of deep RL and developmental approaches, surveys recent approaches and discusses future avenues.},
	urldate = {2021-11-24},
	journal = {arXiv:2012.09830 [cs]},
	author = {Colas, Cédric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	month = jul,
	year = {2021},
	note = {arXiv: 2012.09830},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\4GD5Q273\\Colas et al. - 2021 - Intrinsically Motivated Goal-Conditioned Reinforce.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\3LR8XF3K\\2012.html:text/html},
}

@article{jordan_forward_1992,
	title = {Forward models: {Supervised} learning with a distal teacher},
	volume = {16},
	issn = {0364-0213},
	shorttitle = {Forward models},
	url = {https://www.sciencedirect.com/science/article/pii/036402139290036T},
	doi = {10.1016/0364-0213(92)90036-T},
	abstract = {Internal models of the environment have an important role to play in adaptive systems, in general, and are of particular importance for the supervised learning paradigm. In this article we demonstrate that certain classical problems associated with the notion of the “teacher” in supervised learning can be solved by judicious use of learned internal models as components of the adaptive system. In particular, we show how supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes. Our approach applies to any supervised learning algorithm that is capable of learning in multilayer networks.},
	language = {en},
	number = {3},
	urldate = {2021-11-24},
	journal = {Cognitive Science},
	author = {Jordan, Michael and Rumelhart, David},
	month = jul,
	year = {1992},
	pages = {307--354},
	file = {ScienceDirect Snapshot:C\:\\Users\\user\\Zotero\\storage\\CIAJMNPR\\036402139290036T.html:text/html;Full Text:C\:\\Users\\user\\Zotero\\storage\\N6ZFSCYW\\Jordan and Rumelhart - 1992 - Forward models Supervised learning with a distal .pdf:application/pdf},
}

@article{rolf_goal_2010,
	title = {Goal {Babbling} {Permits} {Direct} {Learning} of {Inverse} {Kinematics}},
	volume = {2},
	issn = {1943-0612},
	doi = {10.1109/TAMD.2010.2062511},
	abstract = {We present an approach to learn inverse kinematics of redundant systems without prior- or expert-knowledge. The method allows for an iterative bootstrapping and refinement of the inverse kinematics estimate. The essential novelty lies in a path-based sampling approach: we generate training data along paths, which result from execution of the currently learned estimate along a desired path towards a goal. The information structure thereby induced enables an efficient detection and resolution of inconsistent samples solely from directly observable data. We derive and illustrate the exploration and learning process with a low-dimensional kinematic example that provides direct insight into the bootstrapping process. We further show that the method scales for high dimensional problems, such as the Honda humanoid robot or hyperredundant planar arms with up to 50 degrees of freedom.},
	number = {3},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Rolf, Matthias and Steil, Jochen J. and Gienger, Michael},
	month = sep,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Autonomous Mental Development},
	keywords = {Biological system modeling, Goal babbling, Humanoid robots, Humans, inverse kinematics, Inverse problems, Iterative methods, Manipulators, motor exploration, motor learning, Pediatrics, Predictive models, Robot kinematics, Sampling methods},
	pages = {216--229},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\user\\Zotero\\storage\\FI3GJHNA\\5535131.html:text/html},
}

@article{takahashi_dynamic_2017,
	title = {Dynamic motion learning for multi-{DOF} flexible-joint robots using active–passive motor babbling through deep learning},
	volume = {31},
	issn = {0169-1864},
	url = {https://doi.org/10.1080/01691864.2017.1383939},
	doi = {10.1080/01691864.2017.1383939},
	abstract = {This paper proposes a learning strategy for robots with flexible joints having multi-degrees of freedom in order to achieve dynamic motion tasks. In spite of there being several potential benefits of flexible-joint robots such as exploitation of intrinsic dynamics and passive adaptation to environmental changes with mechanical compliance, controlling such robots is challenging because of increased complexity of their dynamics. To achieve dynamic movements, we introduce a two-phase learning framework of the body dynamics of the robot using a recurrent neural network motivated by a deep learning strategy. The proposed methodology comprises a pre-training phase with motor babbling and a fine-tuning phase with additional learning of the target tasks. In the pre-training phase, we consider active and passive exploratory motions for efficient acquisition of body dynamics. In the fine-tuning phase, the learned body dynamics are adjusted for specific tasks. We demonstrate the effectiveness of the proposed methodology in achieving dynamic tasks involving constrained movement requiring interactions with the environment on a simulated robot model and an actual PR2 robot both of which have a compliantly actuated seven degree-of-freedom arm. The results illustrate a reduction in the required number of training iterations for task learning and generalization capabilities for untrained situations.},
	number = {18},
	urldate = {2021-11-24},
	journal = {Advanced Robotics},
	author = {Takahashi, Kuniyuki and Ogata, Tetsuya and Nakanishi, Jun and Cheng, Gordon and Sugano, Shigeki},
	month = sep,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01691864.2017.1383939},
	keywords = {deep learning, dynamic motion learning, flexible-joint robot, Motor babbling, recurrent neural network},
	pages = {1002--1015},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\YQCH8RDI\\Takahashi et al. - 2017 - Dynamic motion learning for multi-DOF flexible-joi.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\MZASPBAR\\01691864.2017.html:text/html},
}

@article{reinke_intrinsically_2020,
	title = {Intrinsically {Motivated} {Discovery} of {Diverse} {Patterns} in {Self}-{Organizing} {Systems}},
	url = {http://arxiv.org/abs/1908.06663},
	abstract = {In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the interesting features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts.},
	urldate = {2021-11-24},
	journal = {arXiv:1908.06663 [cs, stat]},
	author = {Reinke, Chris and Etcheverry, Mayalen and Oudeyer, Pierre-Yves},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.06663},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\UB2886ES\\Reinke et al. - 2020 - Intrinsically Motivated Discovery of Diverse Patte.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\WTBFLNTB\\1908.html:text/html},
}

@Article{Dubey,
  author   = {Dubey, Rachit and Griffiths, Thomas L},
  title    = {Reconciling {Novelty} and {Complexity} {Through} a {Rational} {Analysis} of {Curiosity}},
  pages    = {22},
  abstract = {Curiosity is considered to be the essence of science and an integral component of cognition. What prompts curiosity in a learner? Previous theoretical accounts of curiosity remain divided—novelty-based theories propose that new and highly uncertain stimuli pique curiosity, whereas complexity-based theories propose that stimuli with an intermediate degree of uncertainty stimulate curiosity. In this article, we present a rational analysis of curiosity by considering the computational problem underlying curiosity, which allows us to model these distinct accounts of curiosity in a common framework. Our approach posits that a rational agent should explore stimuli that maximally increase the usefulness of its knowledge and that curiosity is the mechanism by which humans approximate this rational behavior. Critically, our analysis show that the causal structure of the environment can determine whether curiosity is driven by either highly uncertain or moderately uncertain stimuli. This suggests that previous theories need not be in contention but are special cases of a more general account of curiosity. Experimental results confirm our predictions and demonstrate that our theory explains a wide range of findings about human curiosity, including its subjectivity and malleability.},
  file     = {Dubey and Griffiths - Reconciling Novelty and Complexity Through a Ratio.pdf:C\:\\Users\\user\\Zotero\\storage\\WJ58FMJG\\Dubey and Griffiths - Reconciling Novelty and Complexity Through a Ratio.pdf:application/pdf},
  language = {en},
}

@misc{schueller2018complexity,
      title={Complexity Reduction in the Negotiation of New Lexical Conventions}, 
      author={William Schueller and Vittorio Loreto and Pierre-Yves Oudeyer},
      year={2018},
      eprint={1805.05631},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@Article{Barto,
  author   = {Barto, Andrew G and Singh, Satinder and Chentanez, Nuttapong},
  title    = {Intrinsically {Motivated} {Learning} of {Hierarchical} {Collections} of {Skills}},
  pages    = {8},
  abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, speciﬁcally, new concepts related to skills and new learning algorithms for learning with skill hierarchies.},
  file     = {Barto et al. - Intrinsically Motivated Learning of Hierarchical C.pdf:C\:\\Users\\user\\Zotero\\storage\\7ZYI9L6R\\Barto et al. - Intrinsically Motivated Learning of Hierarchical C.pdf:application/pdf},
  language = {en},
}

@Article{Poli2020,
  author   = {Poli, F. and Serino, G. and Mars, R. B. and Hunnius, S.},
  journal  = {Science Advances},
  title    = {Infants tailor their attention to maximize learning},
  year     = {2020},
  issn     = {2375-2548},
  month    = sep,
  number   = {39},
  pages    = {eabb5053},
  volume   = {6},
  abstract = {Infants’ remarkable learning abilities allow them to rapidly acquire many complex skills. It has been suggested that infants achieve this learning by optimally allocating their attention to relevant stimuli in the environment, but the underlying mechanisms remain poorly understood. Here, we modeled infants’ looking behavior during a learning task through an ideal learner that quantified the informational structure of environmental stimuli. We show that saccadic latencies, looking time, and time spent engaged with a stimulus sequence are explained by the properties of the learning environments, including the level of surprise of the stimulus, overall predictability of the environment, and progress in learning the environmental structure. These findings reveal the factors that shape infants’ advanced learning, emphasizing their predisposition to seek out stimuli that maximize learning.},
  doi      = {10.1126/sciadv.abb5053},
  file     = {Poli et al. - 2020 - Infants tailor their attention to maximize learnin.pdf:C\:\\Users\\user\\Zotero\\storage\\6EWBWA45\\Poli et al. - 2020 - Infants tailor their attention to maximize learnin.pdf:application/pdf},
  language = {en},
  url      = {https://advances.sciencemag.org/lookup/doi/10.1126/sciadv.abb5053},
  urldate  = {2020-12-02},
}

@Misc{Fiedler2004,
  author   = {Fiedler, Glen},
  month    = jun,
  title    = {Fix {Your} {Timestep}!},
  year     = {2004},
  abstract = {Introduction Hi, I’m Glenn Fiedler and welcome to Game Physics.
In the previous article we discussed how to integrate the equations of motion using a numerical integrator. Integration sounds complicated, but it’s just a way to advance the your physics simulation forward by some small amount of time called “delta time” (or dt for short).
But how to choose this delta time value? This may seem like a trivial subject but in fact there are many different ways to do it, each with their own strengths and weaknesses - so read on!},
  file     = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\4IUWHFRH\\fix_your_timestep.html:text/html},
  journal  = {Gaffer On Games},
  language = {en-us},
  url      = {https://gafferongames.com/post/fix_your_timestep/},
  urldate  = {2021-12-26},
}
