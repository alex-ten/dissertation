
@article{noauthor_notitle_nodate,
}

@inproceedings{portelas_automatic_2020,
	address = {Yokohama, Japan},
	title = {Automatic {Curriculum} {Learning} {For} {Deep} {RL}: {A} {Short} {Survey}},
	isbn = {978-0-9992411-6-5},
	shorttitle = {Automatic {Curriculum} {Learning} {For} {Deep} {RL}},
	url = {https://www.ijcai.org/proceedings/2020/671},
	doi = {10.24963/ijcai.2020/671},
	abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efﬁciency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Portelas, Rémy and Colas, Cédric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
	month = jul,
	year = {2020},
	pages = {4819--4825},
	file = {Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short.pdf:C\:\\Users\\user\\Zotero\\storage\\6NEUKT6Z\\Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short.pdf:application/pdf},
}

@article{doncieux_open-ended_2018,
	title = {Open-{Ended} {Learning}: {A} {Conceptual} {Framework} {Based} on {Representational} {Redescription}},
	volume = {12},
	issn = {1662-5218},
	shorttitle = {Open-{Ended} {Learning}},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2018.00059/full},
	doi = {10.3389/fnbot.2018.00059},
	abstract = {Reinforcement learning (RL) aims at building a policy that maximizes a task-related reward within a given domain. When the domain is known, i.e., when its states, actions and reward are deﬁned, Markov Decision Processes (MDPs) provide a convenient theoretical framework to formalize RL. But in an open-ended learning process, an agent or robot must solve an unbounded sequence of tasks that are not known in advance and the corresponding MDPs cannot be built at design time. This deﬁnes the main challenges of open-ended learning: how can the agent learn how to behave appropriately when the adequate states, actions and rewards representations are not given? In this paper, we propose a conceptual framework to address this question. We assume an agent endowed with low-level perception and action capabilities. This agent receives an external reward when it faces a task. It must discover the state and action representations that will let it cast the tasks as MDPs in order to solve them by RL. The relevance of the action or state representation is critical for the agent to learn efﬁciently. Considering that the agent starts with a low level, task-agnostic state and action spaces based on its low-level perception and action capabilities, we describe open-ended learning as the challenge of building the adequate representation of states and actions, i.e., of redescribing available representations. We suggest an iterative approach to this problem based on several successive Representational Redescription processes, and highlight the corresponding challenges in which intrinsic motivations play a key role.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Frontiers in Neurorobotics},
	author = {Doncieux, Stephane and Filliat, David and Díaz-Rodríguez, Natalia and Hospedales, Timothy and Duro, Richard and Coninx, Alexandre and Roijers, Diederik M. and Girard, Benoît and Perrin, Nicolas and Sigaud, Olivier},
	month = sep,
	year = {2018},
	pages = {59},
	file = {Doncieux et al. - 2018 - Open-Ended Learning A Conceptual Framework Based .pdf:C\:\\Users\\user\\Zotero\\storage\\LGBW4KQR\\Doncieux et al. - 2018 - Open-Ended Learning A Conceptual Framework Based .pdf:application/pdf},
}

@book{gigerenzer_heuristics_2011,
	title = {Heuristics},
	isbn = {978-0-19-974428-2},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199744282.001.0001/acprof-9780199744282},
	language = {en},
	urldate = {2020-12-02},
	publisher = {Oxford University Press},
	author = {Gigerenzer, Gerd and Hertwig, Ralph and Pachur, Thorsten},
	month = apr,
	year = {2011},
	doi = {10.1093/acprof:oso/9780199744282.001.0001},
	doi = {10.1093/acprof:oso/9780199744282.001.0001},
	file = {Gigerenzer et al. - 2011 - Heuristics.pdf:C\:\\Users\\user\\Zotero\\storage\\C4SD9X8R\\Gigerenzer et al. - 2011 - Heuristics.pdf:application/pdf},
}

@article{colas_language_2020,
	title = {Language as a {Cognitive} {Tool} to {Imagine} {Goals} in {Curiosity}-{Driven} {Exploration}},
	url = {http://arxiv.org/abs/2002.09253},
	abstract = {Developmental machine learning studies how artiﬁcial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either ﬁxed and hand-deﬁned or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, beneﬁt from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:2002.09253 [cs]},
	author = {Colas, Cédric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and Moulin-Frier, Clément and Dominey, Peter Ford and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.09253},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Colas et al. - 2020 - Language as a Cognitive Tool to Imagine Goals in C.pdf:C\:\\Users\\user\\Zotero\\storage\\MCV6H7FL\\Colas et al. - 2020 - Language as a Cognitive Tool to Imagine Goals in C.pdf:application/pdf},
}

@article{wilson_humans_2014,
	title = {Humans use directed and random exploration to solve the explore–exploit dilemma.},
	volume = {143},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038199},
	doi = {10.1037/a0038199},
	language = {en},
	number = {6},
	urldate = {2020-12-02},
	journal = {Journal of Experimental Psychology: General},
	author = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
	year = {2014},
	pages = {2074--2081},
	file = {Wilson et al. - 2014 - Humans use directed and random exploration to solv.pdf:C\:\\Users\\user\\Zotero\\storage\\SXJQ9EPT\\Wilson et al. - 2014 - Humans use directed and random exploration to solv.pdf:application/pdf},
}

@article{dubey_reconciling_nodate,
	title = {Reconciling {Novelty} and {Complexity} {Through} a {Rational} {Analysis} of {Curiosity}},
	abstract = {Curiosity is considered to be the essence of science and an integral component of cognition. What prompts curiosity in a learner? Previous theoretical accounts of curiosity remain divided—novelty-based theories propose that new and highly uncertain stimuli pique curiosity, whereas complexity-based theories propose that stimuli with an intermediate degree of uncertainty stimulate curiosity. In this article, we present a rational analysis of curiosity by considering the computational problem underlying curiosity, which allows us to model these distinct accounts of curiosity in a common framework. Our approach posits that a rational agent should explore stimuli that maximally increase the usefulness of its knowledge and that curiosity is the mechanism by which humans approximate this rational behavior. Critically, our analysis show that the causal structure of the environment can determine whether curiosity is driven by either highly uncertain or moderately uncertain stimuli. This suggests that previous theories need not be in contention but are special cases of a more general account of curiosity. Experimental results confirm our predictions and demonstrate that our theory explains a wide range of findings about human curiosity, including its subjectivity and malleability.},
	language = {en},
	author = {Dubey, Rachit and Griffiths, Thomas L},
	pages = {22},
	file = {Dubey and Griffiths - Reconciling Novelty and Complexity Through a Ratio.pdf:C\:\\Users\\user\\Zotero\\storage\\WJ58FMJG\\Dubey and Griffiths - Reconciling Novelty and Complexity Through a Ratio.pdf:application/pdf},
}

@article{barto_intrinsically_nodate,
	title = {Intrinsically {Motivated} {Learning} of {Hierarchical} {Collections} of {Skills}},
	abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, speciﬁcally, new concepts related to skills and new learning algorithms for learning with skill hierarchies.},
	language = {en},
	author = {Barto, Andrew G and Singh, Satinder and Chentanez, Nuttapong},
	pages = {8},
	file = {Barto et al. - Intrinsically Motivated Learning of Hierarchical C.pdf:C\:\\Users\\user\\Zotero\\storage\\7ZYI9L6R\\Barto et al. - Intrinsically Motivated Learning of Hierarchical C.pdf:application/pdf},
}

@article{oudeyer_computational_2018,
	title = {Computational {Theories} of {Curiosity}-{Driven} {Learning}},
	url = {http://arxiv.org/abs/1802.10546},
	abstract = {What are the functions of curiosity? What are the mechanisms of curiosity-driven learning? We approach these questions about the living using concepts and tools from machine learning and developmental robotics. We argue that curiosity-driven learning enables organisms to make discoveries to solve complex problems with rare or deceptive rewards. By fostering exploration and discovery of a diversity of behavioural skills, and ignoring these rewards, curiosity can be efﬁcient to bootstrap learning when there is no information, or deceptive information, about local improvement towards these problems. We also explain the key role of curiosity for efﬁcient learning of world models. We review both normative and heuristic computational frameworks used to understand the mechanisms of curiosity in humans, conceptualizing the child as a sense-making organism. These frameworks enable us to discuss the bi-directional causal links between curiosity and learning, and to provide new hypotheses about the fundamental role of curiosity in self-organizing developmental structures through curriculum learning. We present various developmental robotics experiments that study these mechanisms in action, both supporting these hypotheses to understand better curiosity in humans and opening new research avenues in machine learning and artiﬁcial intelligence. Finally, we discuss challenges for the design of experimental paradigms for studying curiosity in psychology and cognitive neuroscience.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1802.10546 [cs]},
	author = {Oudeyer, Pierre-Yves},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.10546},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Oudeyer - 2018 - Computational Theories of Curiosity-Driven Learnin.pdf:C\:\\Users\\user\\Zotero\\storage\\FH5HRSCV\\Oudeyer - 2018 - Computational Theories of Curiosity-Driven Learnin.pdf:application/pdf},
}

@article{forestier_intrinsically_2020,
	title = {Intrinsically {Motivated} {Goal} {Exploration} {Processes} with {Automatic} {Curriculum} {Learning}},
	url = {http://arxiv.org/abs/1708.02190},
	abstract = {Intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children. It enables the discovery and acquisition of large repertoires of skills through self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous or self-supervised learning in machines. The IMGEP algorithmic architecture relies on several principles: 1) self-generation of goals, generalized as ﬁtness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation of the gathered data with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efﬁcient form of IMGEP, called Modular Population-Based IMGEP, that uses a population-based policy and an object-centered modularity in goals and mutations. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups including a real humanoid robot that can explore multiple spaces of goals with several hundred continuous dimensions. While no particular target goal is provided to the system, this curriculum allows the discovery of skills that act as stepping stone for learning more complex skills, e.g. nested tool use. We show that learning diverse spaces of goals with intrinsic motivations is more efﬁcient for learning complex skills than only trying to directly learn these complex skills.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1708.02190 [cs]},
	author = {Forestier, Sébastien and Portelas, Rémy and Mollard, Yoan and Oudeyer, Pierre-Yves},
	month = jul,
	year = {2020},
	note = {arXiv: 1708.02190},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Forestier et al. - 2020 - Intrinsically Motivated Goal Exploration Processes.pdf:C\:\\Users\\user\\Zotero\\storage\\LNQIZC9M\\Forestier et al. - 2020 - Intrinsically Motivated Goal Exploration Processes.pdf:application/pdf},
}

@article{linke_adapting_2020,
	title = {Adapting {Behaviour} via {Intrinsic} {Reward}: {A} {Survey} and {Empirical} {Study}},
	shorttitle = {Adapting {Behaviour} via {Intrinsic} {Reward}},
	url = {http://arxiv.org/abs/1906.07865},
	abstract = {Learning about many things can provide numerous beneﬁts to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience—how to adapt the system’s behaviour—to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven diﬃcult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare diﬀerent intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 15 diﬀerent rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behaviour, if each individual learner is introspective.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1906.07865 [cs, stat]},
	author = {Linke, Cam and Ady, Nadia M. and White, Martha and Degris, Thomas and White, Adam},
	month = aug,
	year = {2020},
	note = {arXiv: 1906.07865},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Linke et al. - 2020 - Adapting Behaviour via Intrinsic Reward A Survey .pdf:C\:\\Users\\user\\Zotero\\storage\\JGD9HLEW\\Linke et al. - 2020 - Adapting Behaviour via Intrinsic Reward A Survey .pdf:application/pdf},
}

@article{santucci_editorial_2020,
	title = {Editorial: {Intrinsically} {Motivated} {Open}-{Ended} {Learning} in {Autonomous} {Robots}},
	volume = {13},
	issn = {1662-5218},
	shorttitle = {Editorial},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2019.00115/full},
	doi = {10.3389/fnbot.2019.00115},
	language = {en},
	urldate = {2020-12-02},
	journal = {Frontiers in Neurorobotics},
	author = {Santucci, Vieri Giuliano and Oudeyer, Pierre-Yves and Barto, Andrew and Baldassarre, Gianluca},
	month = jan,
	year = {2020},
	pages = {115},
	file = {Santucci et al. - 2020 - Editorial Intrinsically Motivated Open-Ended Lear.pdf:C\:\\Users\\user\\Zotero\\storage\\IZYXEE2S\\Santucci et al. - 2020 - Editorial Intrinsically Motivated Open-Ended Lear.pdf:application/pdf},
}

@article{burda_large-scale_2018,
	title = {Large-{Scale} {Study} of {Curiosity}-{Driven} {Learning}},
	url = {http://arxiv.org/abs/1808.04355},
	abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the ﬁrst large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufﬁcient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1808.04355 [cs, stat]},
	author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04355},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf:C\:\\Users\\user\\Zotero\\storage\\UHRTM5V8\\Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf:application/pdf},
}

@article{savinov_episodic_2019,
	title = {Episodic {Curiosity} through {Reachability}},
	url = {http://arxiv.org/abs/1810.02274},
	abstract = {Rewards are sparse in the real world and most of today’s reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself — thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward — making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory — which incorporates rich information about environment dynamics. This allows us to overcome the known “couch-potato” issues of prior work — when the agent ﬁnds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in VizDoom, DMLab and MuJoCo. In navigational tasks from VizDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the ﬁrst-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1810.02274 [cs, stat]},
	author = {Savinov, Nikolay and Raichuk, Anton and Marinier, Raphaël and Vincent, Damien and Pollefeys, Marc and Lillicrap, Timothy and Gelly, Sylvain},
	month = aug,
	year = {2019},
	note = {arXiv: 1810.02274},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Savinov et al. - 2019 - Episodic Curiosity through Reachability.pdf:C\:\\Users\\user\\Zotero\\storage\\NMKI7KQG\\Savinov et al. - 2019 - Episodic Curiosity through Reachability.pdf:application/pdf},
}

@article{kovac_grimgep_2020,
	title = {{GRIMGEP}: {Learning} {Progress} for {Robust} {Goal} {Sampling} in {Visual} {Deep} {Reinforcement} {Learning}},
	shorttitle = {{GRIMGEP}},
	url = {http://arxiv.org/abs/2008.04388},
	abstract = {Autonomous agents using novelty based goal exploration are often efﬁcient in environments that require exploration. However, they get attracted to various forms of distracting unlearnable regions. To solve this problem, Absolute Learning Progress (ALP) has been used in reinforcement learning agents with predeﬁned goal features and access to expert knowledge. This work extends those concepts to unsupervised image-based goal exploration. We present the GRIMGEP framework: it provides a learned robust goal sampling prior that can be used on top of current state-of-the-art novelty seeking goal exploration approaches, enabling them to ignore noisy distracting regions while searching for novelty in the learnable regions. It clusters the goal space and estimates ALP for each cluster. These ALP estimates can then be used to detect the distracting regions, and build a prior that enables further goal sampling mechanisms to ignore them. We construct an image based environment with distractors, on which we show that wrapping current state-of-the-art goal exploration algorithms with our framework allows them to concentrate on interesting regions of the environment and drastically improve performances. The source code is available at https://sites.google.com/view/grimgep1.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:2008.04388 [cs, stat]},
	author = {Kovač, Grgur and Laversanne-Finot, Adrien and Oudeyer, Pierre-Yves},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.04388},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning, I.2.6, I.2.9},
	file = {Kovač et al. - 2020 - GRIMGEP Learning Progress for Robust Goal Samplin.pdf:C\:\\Users\\user\\Zotero\\storage\\YK4TBZ54\\Kovač et al. - 2020 - GRIMGEP Learning Progress for Robust Goal Samplin.pdf:application/pdf},
}

@article{barto_intrinsic_nodate,
	title = {Intrinsic {Motivation} {For} {Reinforcement} {Learning} {Systems}},
	abstract = {Motivation is a key factor in human learning. We learn best when we are highly motivated to learn. Psychologists distinguish between extrinsically-motivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically-motivated behavior, which is behavior done for its own sake. Is there an analogous distinction for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, can it be meaningful to distinguish between extrinsic and intrinsic motivation? In this paper, we argue that the answer to both questions is “yes,” and we describe some computational experiments that explore these ideas within the framework of computational reinforcement learning. In particular, we describe an approach by which artiﬁcial agents can learn hierarchies of reusable skills through a computational analog of intrinsic motivation.},
	language = {en},
	author = {Barto, Andrew G},
	pages = {6},
	file = {Barto - Intrinsic Motivation For Reinforcement Learning Sy.pdf:C\:\\Users\\user\\Zotero\\storage\\5RPLN7ZY\\Barto - Intrinsic Motivation For Reinforcement Learning Sy.pdf:application/pdf},
}

@article{bougie_fast_2020,
	title = {Fast and slow curiosity for high-level exploration in reinforcement learning},
	issn = {0924-669X, 1573-7497},
	url = {http://link.springer.com/10.1007/s10489-020-01849-3},
	doi = {10.1007/s10489-020-01849-3},
	abstract = {Deep reinforcement learning (DRL) algorithms rely on carefully designed environment rewards that are extrinsic to the agent. However, in many real-world scenarios rewards are sparse or delayed, motivating the need for discovering efficient exploration strategies. While intrinsically motivated agents hold promise of better local exploration, solving problems that require coordinated decisions over long-time horizons remains an open problem. We postulate that to discover such strategies, a DRL agent should be able to combine local and high-level exploration behaviors. To this end, we introduce the concept of fast and slow curiosity that aims to incentivize long-time horizon exploration. Our method decomposes the curiosity bonus into a fast reward that deals with local exploration and a slow reward that encourages global exploration. We formulate this bonus as the error in an agent’s ability to reconstruct the observations given their contexts. We further propose to dynamically weight local and high-level strategies by measuring state diversity. We evaluate our method on a variety of benchmark environments, including Minigrid, Super Mario Bros, and Atari games. Experimental results show that our agent outperforms prior approaches in most tasks in terms of exploration efficiency and mean scores.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Applied Intelligence},
	author = {Bougie, Nicolas and Ichise, Ryutaro},
	month = sep,
	year = {2020},
	file = {Bougie and Ichise - 2020 - Fast and slow curiosity for high-level exploration.pdf:C\:\\Users\\user\\Zotero\\storage\\TSYC9WHM\\Bougie and Ichise - 2020 - Fast and slow curiosity for high-level exploration.pdf:application/pdf},
}

@article{moulin-frier_self-organization_2014,
	title = {Self-organization of early vocal development in infants and machines: the role of intrinsic motivation},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {Self-organization of early vocal development in infants and machines},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.01006/abstract},
	doi = {10.3389/fpsyg.2013.01006},
	abstract = {We bridge the gap between two issues in infant development: vocal development and intrinsic motivation. We propose and experimentally test the hypothesis that general mechanisms of intrinsically motivated spontaneous exploration, also called curiosity-driven learning, can self-organize developmental stages during early vocal learning. We introduce a computational model of intrinsically motivated vocal exploration, which allows the learner to autonomously structure its own vocal experiments, and thus its own learning schedule, through a drive to maximize competence progress. This model relies on a physical model of the vocal tract, the auditory system and the agent’s motor control as well as vocalizations of social peers. We present computational experiments that show how such a mechanism can explain the adaptive transition from vocal self-exploration with little inﬂuence from the speech environment, to a later stage where vocal exploration becomes inﬂuenced by vocalizations of peers. Within the initial self-exploration phase, we show that a sequence of vocal production stages self-organizes, and shares properties with data from infant developmental psychology: the vocal learner ﬁrst discovers how to control phonation, then focuses on vocal variations of unarticulated sounds, and ﬁnally automatically discovers and focuses on babbling with articulated proto-syllables. As the vocal learner becomes more proﬁcient at producing complex sounds, imitating vocalizations of peers starts to provide high learning progress explaining an automatic shift from self-exploration to vocal imitation.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Frontiers in Psychology},
	author = {Moulin-Frier, Clément and Nguyen, Sao M. and Oudeyer, Pierre-Yves},
	year = {2014},
	file = {Moulin-Frier et al. - 2014 - Self-organization of early vocal development in in.pdf:C\:\\Users\\user\\Zotero\\storage\\D8CPRKIS\\Moulin-Frier et al. - 2014 - Self-organization of early vocal development in in.pdf:application/pdf},
}

@article{oudeyer_curiosity_nodate,
	title = {Curiosity and {Intrinsic} {Motivation} for {Autonomous} {Machine} {Learning}},
	language = {en},
	author = {Oudeyer, Pierre-Yves and Lopes, Manuel and Kidd, Celeste},
	pages = {2},
	file = {Oudeyer et al. - Curiosity and Intrinsic Motivation for Autonomous .pdf:C\:\\Users\\user\\Zotero\\storage\\6T3R46WJ\\Oudeyer et al. - Curiosity and Intrinsic Motivation for Autonomous .pdf:application/pdf},
}

@article{holm_episodic_2019,
	title = {Episodic curiosity for avoiding asteroids: {Per}-trial information gain for choice outcomes drive information seeking},
	volume = {9},
	issn = {2045-2322},
	shorttitle = {Episodic curiosity for avoiding asteroids},
	url = {http://www.nature.com/articles/s41598-019-47671-x},
	doi = {10.1038/s41598-019-47671-x},
	language = {en},
	number = {1},
	urldate = {2020-12-02},
	journal = {Scientific Reports},
	author = {Holm, Linus and Wadenholt, Gustaf and Schrater, Paul},
	month = dec,
	year = {2019},
	pages = {11265},
	file = {Holm et al. - 2019 - Episodic curiosity for avoiding asteroids Per-tri.pdf:C\:\\Users\\user\\Zotero\\storage\\8KH3V62C\\Holm et al. - 2019 - Episodic curiosity for avoiding asteroids Per-tri.pdf:application/pdf},
}

@article{portelas_teacher_2019,
	title = {Teacher algorithms for curriculum learning of {Deep} {RL} in continuously parameterized environments},
	url = {http://arxiv.org/abs/1910.07224},
	abstract = {We consider the problem of how a teacher algorithm can enable an unknown Deep Reinforcement Learning (DRL) student to become good at a skill over a wide range of diverse environments. To do so, we study how a teacher algorithm can learn to generate a learning curriculum, whereby it sequentially samples parameters controlling a stochastic procedural generation of environments. Because it does not initially know the capacities of its student, a key challenge for the teacher is to discover which environments are easy, difﬁcult or unlearnable, and in what order to propose them to maximize the efﬁciency of learning over the learnable ones. To achieve this, this problem is transformed into a surrogate continuous bandit problem where the teacher samples environments in order to maximize absolute learning progress of its student. We present a new algorithm modeling absolute learning progress with Gaussian mixture models (ALP-GMM). We also adapt existing algorithms and provide a complete study in the context of DRL. Using parameterized variants of the BipedalWalker environment, we study their efﬁciency to personalize a learning curriculum for different learners (embodiments), their robustness to the ratio of learnable/unlearnable environments, and their scalability to non-linear and high-dimensional parameter spaces. Videos and code are available at https://github.com/flowersteam/teachDeepRL.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1910.07224 [cs, stat]},
	author = {Portelas, Rémy and Colas, Cédric and Hofmann, Katja and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07224},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Portelas et al. - 2019 - Teacher algorithms for curriculum learning of Deep.pdf:C\:\\Users\\user\\Zotero\\storage\\DBGMXT9Q\\Portelas et al. - 2019 - Teacher algorithms for curriculum learning of Deep.pdf:application/pdf},
}

@article{westermann_neuroconstructivism_2007,
	title = {Neuroconstructivism},
	volume = {10},
	issn = {1363755X, 14677687},
	url = {http://doi.wiley.com/10.1111/j.1467-7687.2007.00567.x},
	doi = {10.1111/j.1467-7687.2007.00567.x},
	abstract = {Neuroconstructivism is a theoretical framework focusing on the construction of representations in the developing brain. Cognitive development is explained as emerging from the experience-dependent development of neural structures supporting mental representations. Neural development occurs in the context of multiple interacting constraints acting on different levels, from the individual cell to the external environment of the developing child. Cognitive development can thus be understood as a trajectory originating from the constraints on the underlying neural structures. This perspective offers an integrated view of normal and abnormal development as well as of development and adult processing, and it stands apart from traditional cognitive approaches in taking seriously the constraints on cognition inherent to the substrate that delivers it.},
	language = {en},
	number = {1},
	urldate = {2020-12-02},
	journal = {Developmental Science},
	author = {Westermann, Gert and Mareschal, Denis and Johnson, Mark H. and Sirois, Sylvain and Spratling, Michael W. and Thomas, Michael S.C.},
	month = jan,
	year = {2007},
	pages = {75--83},
	file = {Westermann et al. - 2007 - Neuroconstructivism.pdf:C\:\\Users\\user\\Zotero\\storage\\AQZDFT5W\\Westermann et al. - 2007 - Neuroconstructivism.pdf:application/pdf},
}

@incollection{bazhydai_curiosity_2020,
	title = {Curiosity and {Exploration}},
	isbn = {978-0-12-816511-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128093245058041},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {Encyclopedia of {Infant} and {Early} {Childhood} {Development}},
	publisher = {Elsevier},
	author = {Bazhydai, Marina and Twomey, Katherine and Westermann, Gert},
	year = {2020},
	doi = {10.1016/B978-0-12-809324-5.05804-1},
	pages = {370--378},
	file = {Bazhydai et al. - 2020 - Curiosity and Exploration.pdf:C\:\\Users\\user\\Zotero\\storage\\M7NB6H7A\\Bazhydai et al. - 2020 - Curiosity and Exploration.pdf:application/pdf},
}

@article{gureckis_self-directed_2012,
	title = {Self-{Directed} {Learning}: {A} {Cognitive} and {Computational} {Perspective}},
	volume = {7},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Self-{Directed} {Learning}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691612454304},
	doi = {10.1177/1745691612454304},
	abstract = {A widely advocated idea in education is that people learn better when the flow of experience is under their control (i.e., learning is self-directed). However, the reasons why volitional control might result in superior acquisition and the limits to such advantages remain poorly understood. In this article, we review the issue from both a cognitive and computational perspective. On the cognitive side, self-directed learning allows individuals to focus effort on useful information they do not yet possess, can expose information that is inaccessible via passive observation, and may enhance the encoding and retention of materials. On the computational side, the development of efficient “active learning” algorithms that can select their own training data is an emerging research topic in machine learning. This review argues that recent advances in these related fields may offer a fresh theoretical perspective on how people gather information to support their own learning.},
	language = {en},
	number = {5},
	urldate = {2020-12-02},
	journal = {Perspectives on Psychological Science},
	author = {Gureckis, Todd M. and Markant, Douglas B.},
	month = sep,
	year = {2012},
	pages = {464--481},
	file = {Gureckis and Markant - 2012 - Self-Directed Learning A Cognitive and Computatio.pdf:C\:\\Users\\user\\Zotero\\storage\\RC8XEVU7\\Gureckis and Markant - 2012 - Self-Directed Learning A Cognitive and Computatio.pdf:application/pdf},
}

@article{colas_curious_2019,
	title = {{CURIOUS}: {Intrinsically} {Motivated} {Modular} {Multi}-{Goal} {Reinforcement} {Learning}},
	shorttitle = {{CURIOUS}},
	url = {http://arxiv.org/abs/1810.06284},
	abstract = {In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1810.06284 [cs]},
	author = {Colas, Cédric and Fournier, Pierre and Sigaud, Olivier and Chetouani, Mohamed and Oudeyer, Pierre-Yves},
	month = may,
	year = {2019},
	note = {arXiv: 1810.06284},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Colas et al. - 2019 - CURIOUS Intrinsically Motivated Modular Multi-Goa.pdf:C\:\\Users\\user\\Zotero\\storage\\AFTBR8ZY\\Colas et al. - 2019 - CURIOUS Intrinsically Motivated Modular Multi-Goa.pdf:application/pdf},
}

@incollection{horst_computational_2019,
	address = {New York, NY: Routledge, [2019] {\textbar} Series: Routledge},
	edition = {1},
	title = {Computational and {Robotic} {Models} of {Early} {Language} {Development}},
	isbn = {978-1-315-11062-2},
	url = {https://www.taylorfrancis.com/books/9781351616621/chapters/10.4324/9781315110622-5},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {International {Handbook} of {Language} {Acquisition}},
	publisher = {Routledge},
	author = {Oudeyer, Pierre-Yves and Kachergis, George and Schueller, William},
	editor = {Horst, Jessica S. and von Koss Torkildsen, Janne},
	month = may,
	year = {2019},
	doi = {10.4324/9781315110622-5},
	pages = {76--101},
	file = {Oudeyer et al. - 2019 - Computational and Robotic Models of Early Language.pdf:C\:\\Users\\user\\Zotero\\storage\\UETBFC8E\\Oudeyer et al. - 2019 - Computational and Robotic Models of Early Language.pdf:application/pdf},
}

@article{nussenbaum_reinforcement_2019,
	title = {Reinforcement learning across development: {What} insights can we draw from a decade of research?},
	volume = {40},
	issn = {18789293},
	shorttitle = {Reinforcement learning across development},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1878929319303202},
	doi = {10.1016/j.dcn.2019.100733},
	abstract = {The past decade has seen the emergence of the use of reinforcement learning models to study developmental change in value-based learning. It is unclear, however, whether these computational modeling studies, which have employed a wide variety of tasks and model variants, have reached convergent conclusions. In this review, we examine whether the tuning of model parameters that govern different aspects of learning and decisionmaking processes vary consistently as a function of age, and what neurocognitive developmental changes may account for differences in these parameter estimates across development. We explore whether patterns of developmental change in these estimates are better described by differences in the extent to which individuals adapt their learning processes to the statistics of different environments, or by more static learning biases that emerge across varied contexts. We focus specifically on learning rates and inverse temperature parameter esti­ mates, and find evidence that from childhood to adulthood, individuals become better at optimally weighting recent outcomes during learning across diverse contexts and less exploratory in their value-based decisionmaking. We provide recommendations for how these two possibilities — and potential alternative accounts —can be tested more directly to build a cohesive body of research that yields greater insight into the development of core learning processes.},
	language = {en},
	urldate = {2020-12-02},
	journal = {Developmental Cognitive Neuroscience},
	author = {Nussenbaum, Kate and Hartley, Catherine A.},
	month = dec,
	year = {2019},
	pages = {100733},
	file = {Nussenbaum and Hartley - 2019 - Reinforcement learning across development What in.pdf:C\:\\Users\\user\\Zotero\\storage\\ULU6Q693\\Nussenbaum and Hartley - 2019 - Reinforcement learning across development What in.pdf:application/pdf},
}

@article{botvinick_hierarchically_2009,
	title = {Hierarchically organized behavior and its neural foundations: {A} reinforcement learning perspective},
	volume = {113},
	issn = {00100277},
	shorttitle = {Hierarchically organized behavior and its neural foundations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027708002059},
	doi = {10.1016/j.cognition.2008.08.011},
	abstract = {Research on human and animal behavior has long emphasized its hierarchical structure—the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reﬂect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Speciﬁcally, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests speciﬁc ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identiﬁes new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
	language = {en},
	number = {3},
	urldate = {2020-12-02},
	journal = {Cognition},
	author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andew G.},
	month = dec,
	year = {2009},
	pages = {262--280},
	file = {Botvinick et al. - 2009 - Hierarchically organized behavior and its neural f.pdf:C\:\\Users\\user\\Zotero\\storage\\ZMT56WVD\\Botvinick et al. - 2009 - Hierarchically organized behavior and its neural f.pdf:application/pdf},
}

@inproceedings{moulin-frier_exploration_2013,
	address = {Osaka, Japan},
	title = {Exploration strategies in developmental robotics: {A} unified probabilistic framework},
	isbn = {978-1-4799-1036-6},
	shorttitle = {Exploration strategies in developmental robotics},
	url = {http://ieeexplore.ieee.org/document/6652535/},
	doi = {10.1109/DevLrn.2013.6652535},
	abstract = {We present a probabilistic framework unifying two important families of exploration mechanisms recently shown to be efﬁcient to learn complex non-linear redundant sensorimotor mappings. These two explorations mechanisms are: 1) goal babbling, 2) active learning driven by the maximization of empirically measured learning progress. We show how this generic framework allows to model several recent algorithmic architectures for exploration. Then, we propose a particular implementation using Gaussian Mixture Models, which at the same time provides an original empirical measure of the competence progress. Finally, we perform computer simulations on two simulated setups: the control of the end effector of a 7-DoF arm and the control of the formants produced by an articulatory synthesizer.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {2013 {IEEE} {Third} {Joint} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL})},
	publisher = {IEEE},
	author = {Moulin-Frier, Clement and Oudeyer, Pierre-Yves},
	month = aug,
	year = {2013},
	pages = {1--6},
	file = {Moulin-Frier and Oudeyer - 2013 - Exploration strategies in developmental robotics .pdf:C\:\\Users\\user\\Zotero\\storage\\QTMGCZHT\\Moulin-Frier and Oudeyer - 2013 - Exploration strategies in developmental robotics .pdf:application/pdf},
}

@inproceedings{lefort_active_2015,
	address = {Providence, RI, USA},
	title = {Active learning of local predictable representations with artificial curiosity},
	isbn = {978-1-4673-9320-1},
	url = {http://ieeexplore.ieee.org/document/7346145/},
	doi = {10.1109/DEVLRN.2015.7346145},
	abstract = {In this article, we present some preliminary work on integrating an artiﬁcial curiosity mechanism in PROPRE, a generic and modular neural architecture, to obtain online, openended and active learning of a sensory-motor space, where large areas can be unlearnable. PROPRE consists of the combination of the projection of the input motor ﬂow, using a self-organizing map, with the regression of the sensory output ﬂow from this projection representation, using a linear regression. The main feature of PROPRE is the use of a predictability module that provides an interestingness measure for the current motor stimulus depending on a simple evaluation of the sensory prediction quality. This measure modulates the projection learning so that to favor the representations that predict the output better than a local average. Especially, this leads to the learning of local representations where an input/output relationship is deﬁned [1]. In this article, we propose an artiﬁcial curiosity mechanism based on the monitoring of learning progress, as proposed in [2], in the neighborhood of each local representation. Thus, PROPRE simultaneously learns interesting representations of the input ﬂow (depending on their capacities to predict the output) and explores actively this input space where the learning progress is the higher. We illustrate our architecture on the learning of a direct model of an arm whose hand can only be perceived in a restricted visual space. The modulation of the projection learning leads to a better performance and the use of the curiosity mechanism provides quicker learning and even improves the ﬁnal performance.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {2015 {Joint} {IEEE} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL}-{EpiRob})},
	publisher = {IEEE},
	author = {Lefort, Mathieu and Gepperth, Alexander},
	month = aug,
	year = {2015},
	pages = {228--233},
	file = {Lefort and Gepperth - 2015 - Active learning of local predictable representatio.pdf:C\:\\Users\\user\\Zotero\\storage\\QRNH8DC6\\Lefort and Gepperth - 2015 - Active learning of local predictable representatio.pdf:application/pdf},
}

@article{twomey_curiosity-based_2018,
	title = {Curiosity-based learning in infants: a neurocomputational approach},
	volume = {21},
	issn = {1363755X},
	shorttitle = {Curiosity-based learning in infants},
	url = {http://doi.wiley.com/10.1111/desc.12629},
	doi = {10.1111/desc.12629},
	abstract = {Infants are curious learners who drive their own cognitive development by imposing structure on their learning environment as they explore. Understanding the mechanisms by which infants structure their own learning is therefore critical to our understanding of development. Here we propose an explicit mechanism for intrinsically motivated information selection that maximizes learning. We first present a neurocomputational model of infant visual category learning, capturing existing empirical data on the role of environmental complexity on learning. Next we “set the model free”, allowing it to select its own stimuli based on a formalization of curiosity and three alternative selection mechanisms. We demonstrate that maximal learning emerges when the model is able to maximize stimulus novelty relative to its internal states, depending on the interaction across learning between the structure of the environment and the plasticity in the learner itself. We discuss the implications of this new curiosity mechanism for both existing computational models of reinforcement learning and for our understanding of this fundamental mechanism in early development.},
	language = {en},
	number = {4},
	urldate = {2020-12-02},
	journal = {Developmental Science},
	author = {Twomey, Katherine E. and Westermann, Gert},
	month = jul,
	year = {2018},
	pages = {e12629},
	file = {Twomey and Westermann - 2018 - Curiosity-based learning in infants a neurocomput.pdf:C\:\\Users\\user\\Zotero\\storage\\VWAJHIJW\\Twomey and Westermann - 2018 - Curiosity-based learning in infants a neurocomput.pdf:application/pdf},
}

@article{poli_infants_2020,
	title = {Infants tailor their attention to maximize learning},
	volume = {6},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/lookup/doi/10.1126/sciadv.abb5053},
	doi = {10.1126/sciadv.abb5053},
	abstract = {Infants’ remarkable learning abilities allow them to rapidly acquire many complex skills. It has been suggested that infants achieve this learning by optimally allocating their attention to relevant stimuli in the environment, but the underlying mechanisms remain poorly understood. Here, we modeled infants’ looking behavior during a learning task through an ideal learner that quantified the informational structure of environmental stimuli. We show that saccadic latencies, looking time, and time spent engaged with a stimulus sequence are explained by the properties of the learning environments, including the level of surprise of the stimulus, overall predictability of the environment, and progress in learning the environmental structure. These findings reveal the factors that shape infants’ advanced learning, emphasizing their predisposition to seek out stimuli that maximize learning.},
	language = {en},
	number = {39},
	urldate = {2020-12-02},
	journal = {Science Advances},
	author = {Poli, F. and Serino, G. and Mars, R. B. and Hunnius, S.},
	month = sep,
	year = {2020},
	pages = {eabb5053},
	file = {Poli et al. - 2020 - Infants tailor their attention to maximize learnin.pdf:C\:\\Users\\user\\Zotero\\storage\\6EWBWA45\\Poli et al. - 2020 - Infants tailor their attention to maximize learnin.pdf:application/pdf},
}

@article{pere_unsupervised_2018,
	title = {Unsupervised {Learning} of {Goal} {Spaces} for {Intrinsically} {Motivated} {Goal} {Exploration}},
	url = {http://arxiv.org/abs/1803.00781},
	abstract = {Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a speciﬁcally engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: ﬁrst, in a perceptual learning stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments where a simulated robot arm interacts with an object, and we show that exploration algorithms using such learned representations can match the performance obtained using engineered representations.},
	language = {en},
	urldate = {2020-12-02},
	journal = {arXiv:1803.00781 [cs]},
	author = {Péré, Alexandre and Forestier, Sébastien and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.00781},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Péré et al. - 2018 - Unsupervised Learning of Goal Spaces for Intrinsic.pdf:C\:\\Users\\user\\Zotero\\storage\\FZCIX9LY\\Péré et al. - 2018 - Unsupervised Learning of Goal Spaces for Intrinsic.pdf:application/pdf},
}

@inproceedings{forestier_modular_2016,
	address = {Daejeon, South Korea},
	title = {Modular active curiosity-driven discovery of tool use},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759584/},
	doi = {10.1109/IROS.2016.7759584},
	abstract = {This article studies algorithms used by a learner to explore high-dimensional structured sensorimotor spaces such as in tool use discovery. In particular, we consider goal babbling architectures that were designed to explore and learn solutions to ﬁelds of sensorimotor problems, i.e. to acquire inverse models mapping a space of parameterized sensorimotor problems/effects to a corresponding space of parameterized motor primitives. However, so far these architectures have not been used in high-dimensional spaces of effects. Here, we show the limits of existing goal babbling architectures for efﬁcient exploration in such spaces, and introduce a novel exploration architecture called Model Babbling (MB). MB exploits efﬁciently a modular representation of the space of parameterized problems/effects. We also study an active version of Model Babbling (the MACOB architecture). These architectures are compared in a simulated experimental setup with an arm that can discover and learn how to move objects using two tools with different properties, embedding structured high-dimensional continuous motor and sensory spaces.},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Forestier, Sebastien and Oudeyer, Pierre-Yves},
	month = oct,
	year = {2016},
	pages = {3965--3972},
	file = {Forestier and Oudeyer - 2016 - Modular active curiosity-driven discovery of tool .pdf:C\:\\Users\\user\\Zotero\\storage\\RT8EK2GQ\\Forestier and Oudeyer - 2016 - Modular active curiosity-driven discovery of tool .pdf:application/pdf},
}

@article{coenen_asking_2019,
	title = {Asking the right questions about the psychology of human inquiry: {Nine} open challenges},
	volume = {26},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Asking the right questions about the psychology of human inquiry},
	url = {http://link.springer.com/10.3758/s13423-018-1470-5},
	doi = {10.3758/s13423-018-1470-5},
	abstract = {The ability to act on the world with the goal of gaining information is core to human adaptability and intelligence. Perhaps the most successful and influential account of such abilities is the Optimal Experiment Design (OED) hypothesis, which argues that humans intuitively perform experiments on the world similar to the way an effective scientist plans an experiment. The widespread application of this theory within many areas of psychology calls for a critical evaluation of the theory’s core claims. Despite many successes, we argue that the OED hypothesis remains lacking as a theory of human inquiry and that research in the area often fails to confront some of the most interesting and important questions. In this critical review, we raise and discuss nine open questions about the psychology of human inquiry.},
	language = {en},
	number = {5},
	urldate = {2020-12-02},
	journal = {Psychonomic Bulletin \& Review},
	author = {Coenen, Anna and Nelson, Jonathan D. and Gureckis, Todd M.},
	month = oct,
	year = {2019},
	pages = {1548--1587},
	file = {Coenen et al. - 2019 - Asking the right questions about the psychology of.pdf:C\:\\Users\\user\\Zotero\\storage\\DVWM2JFS\\Coenen et al. - 2019 - Asking the right questions about the psychology of.pdf:application/pdf},
}

@article{oudeyer_what_2007,
	title = {What is intrinsic motivation? {A} typology of computational approaches},
	volume = {1},
	issn = {1662-5218},
	shorttitle = {What is intrinsic motivation?},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.12.006.2007/abstract},
	doi = {10.3389/neuro.12.006.2007},
	urldate = {2020-12-02},
	journal = {Frontiers in Neurorobotics},
	author = {Oudeyer, Pierre-Yves},
	year = {2007},
	file = {Full Text:C\:\\Users\\user\\Zotero\\storage\\R84RGPJK\\Oudeyer - 2007 - What is intrinsic motivation A typology of comput.pdf:application/pdf},
}

@article{nair_visual_2018,
	title = {Visual {Reinforcement} {Learning} with {Imagined} {Goals}},
	url = {http://arxiv.org/abs/1807.04742},
	abstract = {For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.},
	urldate = {2020-11-18},
	journal = {arXiv:1807.04742 [cs, stat]},
	author = {Nair, Ashvin and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
	month = dec,
	year = {2018},
	note = {arXiv: 1807.04742},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\J6DJ4GAC\\1807.html:text/html},
}

@inproceedings{eysenbach_diversity_2019,
	title = {Diversity is {All} {You} {Need}: {Learning} {Skills} without a {Reward} {Function}},
	url = {https://openreview.net/forum?id=SJx63jRqFm},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	year = {2019},
}

@article{stadie_incentivizing_2015,
	title = {Incentivizing exploration in reinforcement learning with deep predictive models},
	journal = {arXiv preprint arXiv:1507.00814},
	author = {Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
	year = {2015},
}

@article{held_automatic_2017,
	title = {Automatic {Goal} {Generation} for {Reinforcement} {Learning} {Agents}},
	url = {http://arxiv.org/abs/1705.06366},
	abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
	urldate = {2018-01-31},
	author = {Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06366},
}

@incollection{andersen_active_2017,
	title = {Active {Exploration} for {Learning} {Symbolic} {Representations}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Andersen, Garrett and Konidaris, George},
	editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
	year = {2017},
	pages = {5016--5026},
}

@incollection{houthooft_vime_2016,
	title = {{VIME}: {Variational} {Information} {Maximizing} {Exploration}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Houthooft, Rein and Chen, Xi and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	editor = {Lee, D D and Sugiyama, M and Luxburg, U V and Guyon, I and Garnett, R},
	year = {2016},
	pages = {1109--1117},
}

@article{achiam_surprise-based_2017,
	title = {Surprise-based intrinsic motivation for deep reinforcement learning},
	journal = {arXiv preprint arXiv:1703.01732},
	author = {Achiam, Joshua and Sastry, Shankar},
	year = {2017},
}

@inproceedings{jaques_social_2019,
	title = {Social {Influence} as {Intrinsic} {Motivation} for {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=B1lG42C9Km},
	abstract = {We derive a new intrinsic social motivation for multi-agent reinforcement learning (MARL), in which agents are rewarded for having causal influence over another agent's actions. Causal influence is assessed using counterfactual reasoning. The reward does not depend on observing another agent's reward function, and is thus a more realistic approach to MARL than taken in previous work. We show that the causal influence reward is related to maximizing the mutual information between agents' actions. We test the approach in challenging social dilemma environments, where it consistently leads to enhanced cooperation between agents and higher collective reward. Moreover, we find that rewarding influence can lead agents to develop emergent communication protocols. We therefore employ influence to train agents to use an explicit communication channel, and find that it leads to more effective communication and higher collective reward. Finally, we show that influence can be computed by equipping each agent with an internal model that predicts the actions of other agents. This allows the social influence reward to be computed without the use of a centralised controller, and as such represents a significantly more general and scalable inductive bias for MARL with independent agents.},
	booktitle = {Proceedings of the 35 th {International} {Conference} on {Machine} {Learning}, {Stockholm}, {Sweden}},
	author = {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro A. and Strouse, DJ and Leibo, Joel Z. and de Freitas, Nando},
	year = {2019},
	note = {arXiv: 1810.08647},
}

@inproceedings{bellemare_unifying_2016,
	title = {Unifying count-based exploration and intrinsic motivation},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	year = {2016},
	pages = {1471--1479},
	file = {Bellemare et al. - 2016 - Unifying count-based exploration and intrinsic mot.pdf:C\:\\Users\\user\\Zotero\\storage\\XV8QXLQ6\\Bellemare et al. - 2016 - Unifying count-based exploration and intrinsic mot.pdf:application/pdf},
}

@article{pong_skew-fit_2020,
	title = {Skew-{Fit}: {State}-{Covering} {Self}-{Supervised} {Reinforcement} {Learning}},
	shorttitle = {Skew-{Fit}},
	url = {http://arxiv.org/abs/1903.03698},
	abstract = {Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. We prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks. Moreover, we demonstrate that {\textbackslash}METHOD enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.},
	urldate = {2020-05-15},
	journal = {arXiv:1903.03698 [cs, stat]},
	author = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
	year = {2020},
	note = {arXiv: 1903.03698},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\4VRSFGCG\\1903.html:text/html},
}

@article{cully_robots_2015,
	title = {Robots that can adapt like animals},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14422},
	doi = {10.1038/nature14422},
	abstract = {An intelligent trial-and-error learning algorithm is presented that allows robots to adapt in minutes to compensate for a wide variety of types of damage.},
	language = {en},
	number = {7553},
	urldate = {2020-11-18},
	journal = {Nature},
	author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	pages = {503--507},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\K6ZEGTCB\\nature14422.html:text/html},
}

@article{osband_optimistic_2017,
	title = {On {Optimistic} versus {Randomized} {Exploration} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1706.04241},
	abstract = {We discuss the relative merits of optimistic and randomized approaches to exploration in reinforcement learning. Optimistic approaches presented in the literature apply an optimistic boost to the value estimate at each state-action pair and select actions that are greedy with respect to the resulting optimistic value function. Randomized approaches sample from among statistically plausible value functions and select actions that are greedy with respect to the random sample. Prior computational experience suggests that randomized approaches can lead to far more statistically efficient learning. We present two simple analytic examples that elucidate why this is the case. In principle, there should be optimistic approaches that fare well relative to randomized approaches, but that would require intractable computation. Optimistic approaches that have been proposed in the literature sacrifice statistical efficiency for the sake of computational efficiency. Randomized approaches, on the other hand, may enable simultaneous statistical and computational efficiency.},
	urldate = {2018-01-30},
	author = {Osband, Ian and Van Roy, Benjamin},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04241},
}

@inproceedings{alet_meta-learning_2020,
	title = {Meta-learning curiosity algorithms},
	url = {https://openreview.net/forum?id=BygdyxHFDS},
	abstract = {We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high...},
	urldate = {2020-04-21},
	author = {Alet, Ferran and Schneider, Martin F. and Lozano-Perez, Tomas and Kaelbling, Leslie Pack},
	year = {2020},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\XJXY7KNY\\forum.html:text/html},
}

@inproceedings{rahaman_learning_2019,
	title = {Learning the {Arrow} of {Time} for {Problems} in {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=rylJkpEtwS},
	abstract = {We learn the arrow of time for MDPs and use it to measure reachability, detect side-effects and obtain a curiosity reward signal.},
	language = {en},
	urldate = {2020-11-18},
	author = {Rahaman, Nasim and Wolf, Steffen and Goyal, Anirudh and Remme, Roman and Bengio, Yoshua},
	month = sep,
	year = {2019},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\FSMI4M2D\\forum.html:text/html},
}

@inproceedings{chitnis_intrinsic_2020,
	title = {Intrinsic {Motivation} for {Encouraging} {Synergistic} {Behavior}},
	url = {https://openreview.net/forum?id=SJleNCNtDH},
	abstract = {We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a...},
	urldate = {2020-04-22},
	author = {Chitnis, Rohan and Tulsiani, Shubham and Gupta, Saurabh and Gupta, Abhinav},
	year = {2020},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\6VNCE5NE\\forum.html:text/html},
}

@article{sukhbaatar_intrinsic_2017,
	title = {Intrinsic {Motivation} and {Automatic} {Curricula} via {Asymmetric} {Self}-{Play}},
	url = {http://arxiv.org/abs/1703.05407},
	abstract = {We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.},
	urldate = {2018-02-13},
	author = {Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05407},
}

@article{kulkarni_hierarchical_2016,
	title = {Hierarchical deep reinforcement learning: {Integrating} temporal abstraction and intrinsic motivation},
	journal = {arXiv preprint arXiv:1604.06057},
	author = {Kulkarni, Tejas D and Narasimhan, Karthik R and Saeedi, Ardavan and Tenenbaum, Joshua B},
	year = {2016},
}

@inproceedings{fu_ex2_2017,
	title = {Ex2: {Exploration} with exemplar models for deep reinforcement learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Fu, Justin and Co-Reyes, John and Levine, Sergey},
	year = {2017},
	pages = {2574--2584},
}

@article{singh_intrinsically_2010,
	title = {Intrinsically motivated reinforcement learning: {An} evolutionary perspective},
	volume = {2},
	number = {2},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg, Jonathan},
	year = {2010},
	note = {Publisher: IEEE},
	pages = {70--82},
}

@incollection{salge_empowermentintroduction_2014,
	title = {Empowerment–{An} {Introduction}},
	url = {http://link.springer.com/10.1007/978-3-642-53734-9_4},
	urldate = {2018-01-31},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Salge, Christoph and Glackin, Cornelius and Polani, Daniel},
	year = {2014},
	doi = {10.1007/978-3-642-53734-9_4},
	pages = {67--114},
}

@inproceedings{baker_emergent_2020,
	title = {Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}},
	url = {https://openreview.net/forum?id=SkxpxJBKwS},
	abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
	urldate = {2020-05-18},
	author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
	year = {2020},
	note = {tex.ids: Baker2019
arXiv: 1909.07528},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\VHQTIFLG\\forum.html:text/html},
}

@article{doncieux_dream_2020,
	title = {{DREAM} {Architecture}: a {Developmental} {Approach} to {Open}-{Ended} {Learning} in {Robotics}},
	shorttitle = {{DREAM} {Architecture}},
	url = {http://arxiv.org/abs/2005.06223},
	abstract = {Robots are still limited to controlled conditions, that the robot designer knows with enough details to endow the robot with the appropriate models or behaviors. Learning algorithms add some flexibility with the ability to discover the appropriate behavior given either some demonstrations or a reward to guide its exploration with a reinforcement learning algorithm. Reinforcement learning algorithms rely on the definition of state and action spaces that define reachable behaviors. Their adaptation capability critically depends on the representations of these spaces: small and discrete spaces result in fast learning while large and continuous spaces are challenging and either require a long training period or prevent the robot from converging to an appropriate behavior. Beside the operational cycle of policy execution and the learning cycle, which works at a slower time scale to acquire new policies, we introduce the redescription cycle, a third cycle working at an even slower time scale to generate or adapt the required representations to the robot, its environment and the task. We introduce the challenges raised by this cycle and we present DREAM (Deferred Restructuring of Experience in Autonomous Machines), a developmental cognitive architecture to bootstrap this redescription process stage by stage, build new state representations with appropriate motivations, and transfer the acquired knowledge across domains or tasks or even across robots. We describe results obtained so far with this approach and end up with a discussion of the questions it raises in Neuroscience.},
	urldate = {2020-06-14},
	journal = {arXiv:2005.06223 [cs]},
	author = {Doncieux, Stephane and Bredeche, Nicolas and Goff, Léni Le and Girard, Benoît and Coninx, Alexandre and Sigaud, Olivier and Khamassi, Mehdi and Díaz-Rodríguez, Natalia and Filliat, David and Hospedales, Timothy and Eiben, A. and Duro, Richard},
	month = may,
	year = {2020},
	note = {arXiv: 2005.06223},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Robotics, Cognitive architecture, Developmental learning, Intrinsic motivation, Open-ended, Transfer learning},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\JADR8JTR\\2005.html:text/html},
}

@inproceedings{pathak_curiosity-driven_2017,
	title = {Curiosity-driven exploration by self-supervised prediction},
	volume = {2017},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
	year = {2017},
}

@article{blaes_control_2019,
	title = {Control {What} {You} {Can}: {Intrinsically} {Motivated} {Task}-{Planning} {Agent}},
	volume = {32},
	shorttitle = {Control {What} {You} {Can}},
	url = {https://papers.nips.cc/paper/2019/hash/b6f97e6f0fd175613910d613d574d0cb-Abstract.html},
	language = {en},
	urldate = {2020-11-18},
	journal = {Advances in Neural Information Processing Systems},
	author = {Blaes, Sebastian and Vlastelica Pogančić, Marin and Zhu, Jiajie and Martius, Georg},
	year = {2019},
	pages = {12541--12552},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\CYVUD49R\\b6f97e6f0fd175613910d613d574d0cb-Abstract.html:text/html},
}

@article{graves_automated_2017,
	title = {Automated curriculum learning for neural networks},
	journal = {arXiv preprint arXiv:1704.03003},
	author = {Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
	year = {2017},
}

@incollection{tang_exploration_2017,
	title = {\#{Exploration}: {A} {Study} of {Count}-{Based} {Exploration} for {Deep} {Reinforcement} {Learning}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
	editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
	year = {2017},
	pages = {2750--2759},
}

@inproceedings{osband_deep_2016,
	title = {Deep exploration via bootstrapped {DQN}},
	abstract = {fficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games},
	booktitle = {Advances in neural information processing systems},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	year = {2016},
	pages = {4026--4034},
}

@inproceedings{cohn_active_1995,
	title = {Active {Learning} with {Statistical} {Models}},
	volume = {7},
	url = {https://proceedings.neurips.cc/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Cohn, David and Ghahramani, Zoubin and Jordan, Michael},
	editor = {Tesauro, G. and Touretzky, D. and Leen, T.},
	year = {1995},
}

@inproceedings{thrun_lifelong_1994,
	title = {A lifelong learning perspective for mobile robot control},
	volume = {1},
	doi = {10.1109/IROS.1994.407413},
	abstract = {Designing robots that learn by themselves to perform complex real-world tasks is a still-open challenge for the field of robotics and artificial intelligence. In this paper the author presents the robot learning problem as a lifelong problem, in which a robot faces a collection of tasks over its entire lifetime. Such a scenario provides the opportunity to gather general-purpose knowledge that transfers across tasks. The author illustrates a particular leaning mechanism, explanation-based neural network learning, that transfers knowledge between related tasks via neural network action models. The learning approach is illustrated using a mobile robot, equipped with visual, ultrasonic and laser sensors. In less than 10 minutes operation time, the robot is able to learn to navigate to a marked target object in a natural office environment.{\textless}{\textgreater}},
	booktitle = {Proceedings of {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS}'94)},
	author = {Thrun, S.},
	month = sep,
	year = {1994},
	keywords = {Computational complexity, Hardware, Intelligent robots, Learning, Manipulator dynamics, Mobile robots, Robot control, Robot kinematics, Robot sensing systems, Sensor phenomena and characterization},
	pages = {23--30 vol.1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\user\\Zotero\\storage\\QKUAD5W3\\407413.html:text/html},
}

@article{averbeck_theory_2015,
	title = {Theory of {Choice} in {Bandit}, {Information} {Sampling} and {Foraging} {Tasks}},
	volume = {11},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004164},
	doi = {10.1371/journal.pcbi.1004164},
	abstract = {Decision making has been studied with a wide array of tasks. Here we examine the theoretical structure of bandit, information sampling and foraging tasks. These tasks move beyond tasks where the choice in the current trial does not affect future expected rewards. We have modeled these tasks using Markov decision processes (MDPs). MDPs provide a general framework for modeling tasks in which decisions affect the information on which future choices will be made. Under the assumption that agents are maximizing expected rewards, MDPs provide normative solutions. We find that all three classes of tasks pose choices among actions which trade-off immediate and future expected rewards. The tasks drive these trade-offs in unique ways, however. For bandit and information sampling tasks, increasing uncertainty or the time horizon shifts value to actions that pay-off in the future. Correspondingly, decreasing uncertainty increases the relative value of actions that pay-off immediately. For foraging tasks the time-horizon plays the dominant role, as choices do not affect future uncertainty in these tasks.},
	language = {en},
	number = {3},
	urldate = {2021-11-03},
	journal = {PLOS Computational Biology},
	author = {Averbeck, Bruno B.},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Decision making, Foraging, Learning, Markov models, Markov processes, Polynomials, Probability distribution},
	pages = {e1004164},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\A3BTA2WD\\Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling a.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\A9XBI3I8\\article.html:text/html},
}

@article{colas_gep-pg_2018,
	title = {{GEP}-{PG}: {Decoupling} {Exploration} and {Exploitation} in {Deep} {Reinforcement} {Learning} {Algorithms}},
	shorttitle = {{GEP}-{PG}},
	url = {http://arxiv.org/abs/1802.05054},
	abstract = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments. Supplementary videos and discussion can be found at http://frama.link/gep\_pg, the code at http://github.com/flowersteam/geppg.},
	urldate = {2021-11-03},
	journal = {arXiv:1802.05054 [cs]},
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	month = sep,
	year = {2018},
	note = {arXiv: 1802.05054},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: accepted at ICML 2018, 14 pages},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	number = {6245},
	urldate = {2021-11-03},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {255--260},
}

@article{cully_autonomous_2019,
	title = {Autonomous skill discovery with {Quality}-{Diversity} and {Unsupervised} {Descriptors}},
	url = {http://arxiv.org/abs/1905.11874},
	doi = {10.1145/3321707.3321804},
	abstract = {Quality-Diversity optimization is a new family of optimization algorithms that, instead of searching for a single optimal solution to solving a task, searches for a large collection of solutions that all solve the task in a different way. This approach is particularly promising for learning behavioral repertoires in robotics, as such a diversity of behaviors enables robots to be more versatile and resilient. However, these algorithms require the user to manually define behavioral descriptors, which is used to determine whether two solutions are different or similar. The choice of a behavioral descriptor is crucial, as it completely changes the solution types that the algorithm derives. In this paper, we introduce a new method to automatically define this descriptor by combining Quality-Diversity algorithms with unsupervised dimensionality reduction algorithms. This approach enables robots to autonomously discover the range of their capabilities while interacting with their environment. The results from two experimental scenarios demonstrate that robot can autonomously discover a large range of possible behaviors, without any prior knowledge about their morphology and environment. Furthermore, these behaviors are deemed to be similar to handcrafted solutions that uses domain knowledge and significantly more diverse than when using existing unsupervised methods.},
	urldate = {2020-11-18},
	journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
	author = {Cully, Antoine},
	month = jul,
	year = {2019},
	note = {arXiv: 1905.11874},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	pages = {81--89},
	file = {arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\9DRXLUTD\\1905.html:text/html},
}

@article{etcheverry_hierarchically_2021,
	title = {Hierarchically {Organized} {Latent} {Modules} for {Exploratory} {Search} in {Morphogenetic} {Systems}},
	url = {http://arxiv.org/abs/2007.01195},
	abstract = {Self-organization of complex morphological patterns from local interactions is a fascinating phenomenon in many natural and artificial systems. In the artificial world, typical examples of such morphogenetic systems are cellular automata. Yet, their mechanisms are often very hard to grasp and so far scientific discoveries of novel patterns have primarily been relying on manual tuning and ad hoc exploratory search. The problem of automated diversity-driven discovery in these systems was recently introduced [26, 62], highlighting that two key ingredients are autonomous exploration and unsupervised representation learning to describe "relevant" degrees of variations in the patterns. In this paper, we motivate the need for what we call Meta-diversity search, arguing that there is not a unique ground truth interesting diversity as it strongly depends on the final observer and its motives. Using a continuous game-of-life system for experiments, we provide empirical evidences that relying on monolithic architectures for the behavioral embedding design tends to bias the final discoveries (both for hand-defined and unsupervisedly-learned features) which are unlikely to be aligned with the interest of a final end-user. To address these issues, we introduce a novel dynamic and modular architecture that enables unsupervised learning of a hierarchy of diverse representations. Combined with intrinsically motivated goal exploration algorithms, we show that this system forms a discovery assistant that can efficiently adapt its diversity search towards preferences of a user using only a very small amount of user feedback.},
	urldate = {2021-10-31},
	journal = {arXiv:2007.01195 [nlin, stat]},
	author = {Etcheverry, Mayalen and Moulin-Frier, Clement and Oudeyer, Pierre-Yves},
	month = sep,
	year = {2021},
	note = {arXiv: 2007.01195},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Nonlinear Sciences - Cellular Automata and Lattice Gases, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\7PHZIMRW\\Etcheverry et al. - 2021 - Hierarchically Organized Latent Modules for Explor.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\QSAPWDVG\\2007.html:text/html},
}
